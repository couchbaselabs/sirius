diff --git a/README.md b/README.md
index 67b2523..b2fed07 100644
--- a/README.md
+++ b/README.md
@@ -3,7 +3,7 @@
 This Readme file provides instructions for building and running a Go application named **Sirius**. A distributed testing framework designed for Couchbase,
 serving as a REST-based loading service for system, functional, performance and volume testing.
 
-Doc loader different capabilities are described using  [**Rest Endpoints**](task-config.generated.md).
+Doc loader different capabilities are described using  [**Rest Endpoints**](t-config.generated.md).
 
 [Sirius APIs Demo](https://documenter.getpostman.com/view/25450208/2s93sdXWeB)
 ## Requirements
@@ -79,7 +79,7 @@ make down
 
 ## Cleaning
 
-To clean up the task metadata and results directories, use the following command:
+To clean up the t metadata and results directories, use the following command:
 
 ```shell
 make clean
@@ -99,7 +99,7 @@ make clean_deploy
 
 ## Conclusion
 
-Sirius is a simple but powerful Go application for loading Data into a server and storing task metadata and results. By
+Sirius is a simple but powerful Go application for loading Data into a server and storing t metadata and results. By
 following the instructions in this Readme, you can quickly build and run the application locally, or deploy it to a
 Docker environment for production use.
 
diff --git a/cmd/api/handler.go b/cmd/api/handler.go
index bb63813..c6dfa79 100644
--- a/cmd/api/handler.go
+++ b/cmd/api/handler.go
@@ -4,10 +4,8 @@ import (
 	"fmt"
 	"github.com/couchbaselabs/sirius/internal/task_result"
 	"github.com/couchbaselabs/sirius/internal/tasks"
-	"github.com/couchbaselabs/sirius/internal/tasks/bulk_loading_cb"
-	"github.com/couchbaselabs/sirius/internal/tasks/bulk_query_cb"
-	"github.com/couchbaselabs/sirius/internal/tasks/key_based_loading_cb"
-	"github.com/couchbaselabs/sirius/internal/tasks/util_cb"
+	"github.com/couchbaselabs/sirius/internal/tasks/bulk_loading"
+	"github.com/couchbaselabs/sirius/internal/tasks/db_util"
 	"github.com/couchbaselabs/sirius/internal/tasks/util_sirius"
 	"log"
 	"net/http"
@@ -49,7 +47,7 @@ func (app *Config) taskResult(w http.ResponseWriter, r *http.Request) {
 
 // insertTask is used to bulk loading documents into buckets
 func (app *Config) insertTask(w http.ResponseWriter, r *http.Request) {
-	task := &bulk_loading_cb.InsertTask{}
+	task := &bulk_loading.GenericLoadingTask{}
 	if err := app.readJSON(w, r, task); err != nil {
 		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
 		return
@@ -58,6 +56,7 @@ func (app *Config) insertTask(w http.ResponseWriter, r *http.Request) {
 		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
 		return
 	}
+	task.Operation = tasks.InsertOperation
 	log.Print(task, tasks.InsertOperation)
 	err := app.serverRequests.AddTask(task.IdentifierToken, tasks.InsertOperation, task)
 	if err != nil {
@@ -88,169 +87,169 @@ func (app *Config) insertTask(w http.ResponseWriter, r *http.Request) {
 	_ = app.writeJSON(w, http.StatusOK, resPayload)
 }
 
-// deleteTask is used to delete documents in bulk of a bucket.
-func (app *Config) deleteTask(w http.ResponseWriter, r *http.Request) {
-	task := &bulk_loading_cb.DeleteTask{}
-	if err := app.readJSON(w, r, task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := checkIdentifierToken(task.IdentifierToken); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	log.Print(task, tasks.DeleteOperation)
-	err := app.serverRequests.AddTask(task.IdentifierToken, tasks.DeleteOperation, task)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	req, err := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	seedResult, err := task.Config(req, false)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := app.taskManager.AddTask(task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-	}
-	respPayload := util_sirius.TaskResponse{
-		Seed: fmt.Sprintf("%d", seedResult),
-	}
-	resPayload := jsonResponse{
-		Error:   false,
-		Message: "Successfully started requested doc loading",
-		Data:    respPayload,
-	}
-	_ = app.writeJSON(w, http.StatusOK, resPayload)
-}
-
-// upsertTask is used to bulk loading updated documents into bucket.
-func (app *Config) upsertTask(w http.ResponseWriter, r *http.Request) {
-	task := &bulk_loading_cb.UpsertTask{}
-	if err := app.readJSON(w, r, task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := checkIdentifierToken(task.IdentifierToken); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	log.Print(task, tasks.UpsertOperation)
-	err := app.serverRequests.AddTask(task.IdentifierToken, tasks.UpsertOperation, task)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	req, err := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	resultSeed, err := task.Config(req, false)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := app.taskManager.AddTask(task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-	}
-	respPayload := util_sirius.TaskResponse{
-		Seed: fmt.Sprintf("%d", resultSeed),
-	}
-	resPayload := jsonResponse{
-		Error:   false,
-		Message: "Successfully started requested doc loading",
-		Data:    respPayload,
-	}
-	_ = app.writeJSON(w, http.StatusOK, resPayload)
-}
-
-// touchTask is used to update the ttl of documents in bulk.
-func (app *Config) touchTask(w http.ResponseWriter, r *http.Request) {
-	task := &bulk_loading_cb.TouchTask{}
-	if err := app.readJSON(w, r, task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := checkIdentifierToken(task.IdentifierToken); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	log.Print(task, bulk_loading_cb.TouchTask{})
-	err := app.serverRequests.AddTask(task.IdentifierToken, tasks.UpsertOperation, task)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	req, err := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	resultSeed, err := task.Config(req, false)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := app.taskManager.AddTask(task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-	}
-	respPayload := util_sirius.TaskResponse{
-		Seed: fmt.Sprintf("%d", resultSeed),
-	}
-	resPayload := jsonResponse{
-		Error:   false,
-		Message: "Successfully started requested doc loading",
-		Data:    respPayload,
-	}
-	_ = app.writeJSON(w, http.StatusOK, resPayload)
-}
-
-// validateTask is validating the cluster's current state.
-func (app *Config) validateTask(w http.ResponseWriter, r *http.Request) {
-	task := &bulk_loading_cb.ValidateTask{}
-	if err := app.readJSON(w, r, task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := checkIdentifierToken(task.IdentifierToken); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	log.Print(task, tasks.ValidateOperation)
-	err := app.serverRequests.AddTask(task.IdentifierToken, tasks.ValidateOperation, task)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	req, err := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	seedResult, err := task.Config(req, false)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := app.taskManager.AddTask(task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-	}
-	respPayload := util_sirius.TaskResponse{
-		Seed: fmt.Sprintf("%d", seedResult),
-	}
-	resPayload := jsonResponse{
-		Error:   false,
-		Message: "Successfully started requested doc loading",
-		Data:    respPayload,
-	}
-	_ = app.writeJSON(w, http.StatusOK, resPayload)
-}
+//// deleteTask is used to delete documents in bulk of a bucket.
+//func (app *Config) deleteTask(w http.ResponseWriter, r *http.Request) {
+//	task := &bulk_loading.DeleteTask{}
+//	if err_sirius := app.readJSON(w, r, task); err_sirius != nil {
+//		_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//		return
+//	}
+//	if err_sirius := checkIdentifierToken(task.IdentifierToken); err_sirius != nil {
+//		_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//		return
+//	}
+//	log.Print(task, tasks.DeleteOperation)
+//	err_sirius := app.serverRequests.AddTask(task.IdentifierToken, tasks.DeleteOperation, task)
+//	if err_sirius != nil {
+//		_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//		return
+//	}
+//	req, err_sirius := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
+//	if err_sirius != nil {
+//		_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//		return
+//	}
+//	seedResult, err_sirius := task.Config(req, false)
+//	if err_sirius != nil {
+//		_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//		return
+//	}
+//	if err_sirius := app.taskManager.AddTask(task); err_sirius != nil {
+//		_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//	}
+//	respPayload := util_sirius.TaskResponse{
+//		Seed: fmt.Sprintf("%d", seedResult),
+//	}
+//	resPayload := jsonResponse{
+//		Error:   false,
+//		Message: "Successfully started requested doc loading",
+//		Data:    respPayload,
+//	}
+//	_ = app.writeJSON(w, http.StatusOK, resPayload)
+//}
+//
+//// upsertTask is used to bulk loading updated documents into bucket.
+//func (app *Config) upsertTask(w http.ResponseWriter, r *http.Request) {
+//	task := &bulk_loading.UpsertTask{}
+//	if err_sirius := app.readJSON(w, r, task); err_sirius != nil {
+//		_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//		return
+//	}
+//	if err_sirius := checkIdentifierToken(task.IdentifierToken); err_sirius != nil {
+//		_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//		return
+//	}
+//	log.Print(task, tasks.UpsertOperation)
+//	err_sirius := app.serverRequests.AddTask(task.IdentifierToken, tasks.UpsertOperation, task)
+//	if err_sirius != nil {
+//		_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//		return
+//	}
+//	req, err_sirius := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
+//	if err_sirius != nil {
+//		_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//		return
+//	}
+//	resultSeed, err_sirius := task.Config(req, false)
+//	if err_sirius != nil {
+//		_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//		return
+//	}
+//	if err_sirius := app.taskManager.AddTask(task); err_sirius != nil {
+//		_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//	}
+//	respPayload := util_sirius.TaskResponse{
+//		Seed: fmt.Sprintf("%d", resultSeed),
+//	}
+//	resPayload := jsonResponse{
+//		Error:   false,
+//		Message: "Successfully started requested doc loading",
+//		Data:    respPayload,
+//	}
+//	_ = app.writeJSON(w, http.StatusOK, resPayload)
+//}
+//
+//// touchTask is used to update the ttl of documents in bulk.
+//func (app *Config) touchTask(w http.ResponseWriter, r *http.Request) {
+//	task := &bulk_loading.TouchTask{}
+//	if err_sirius := app.readJSON(w, r, task); err_sirius != nil {
+//		_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//		return
+//	}
+//	if err_sirius := checkIdentifierToken(task.IdentifierToken); err_sirius != nil {
+//		_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//		return
+//	}
+//	log.Print(task, bulk_loading.TouchTask{})
+//	err_sirius := app.serverRequests.AddTask(task.IdentifierToken, tasks.UpsertOperation, task)
+//	if err_sirius != nil {
+//		_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//		return
+//	}
+//	req, err_sirius := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
+//	if err_sirius != nil {
+//		_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//		return
+//	}
+//	resultSeed, err_sirius := task.Config(req, false)
+//	if err_sirius != nil {
+//		_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//		return
+//	}
+//	if err_sirius := app.taskManager.AddTask(task); err_sirius != nil {
+//		_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//	}
+//	respPayload := util_sirius.TaskResponse{
+//		Seed: fmt.Sprintf("%d", resultSeed),
+//	}
+//	resPayload := jsonResponse{
+//		Error:   false,
+//		Message: "Successfully started requested doc loading",
+//		Data:    respPayload,
+//	}
+//	_ = app.writeJSON(w, http.StatusOK, resPayload)
+//}
+//
+//// validateTask is validating the cluster's current state.
+//func (app *Config) validateTask(w http.ResponseWriter, r *http.Request) {
+//	task := &bulk_loading.ValidateTask{}
+//	if err_sirius := app.readJSON(w, r, task); err_sirius != nil {
+//		_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//		return
+//	}
+//	if err_sirius := checkIdentifierToken(task.IdentifierToken); err_sirius != nil {
+//		_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//		return
+//	}
+//	log.Print(task, tasks.ValidateOperation)
+//	err_sirius := app.serverRequests.AddTask(task.IdentifierToken, tasks.ValidateOperation, task)
+//	if err_sirius != nil {
+//		_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//		return
+//	}
+//	req, err_sirius := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
+//	if err_sirius != nil {
+//		_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//		return
+//	}
+//	seedResult, err_sirius := task.Config(req, false)
+//	if err_sirius != nil {
+//		_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//		return
+//	}
+//	if err_sirius := app.taskManager.AddTask(task); err_sirius != nil {
+//		_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//	}
+//	respPayload := util_sirius.TaskResponse{
+//		Seed: fmt.Sprintf("%d", seedResult),
+//	}
+//	resPayload := jsonResponse{
+//		Error:   false,
+//		Message: "Successfully started requested doc loading",
+//		Data:    respPayload,
+//	}
+//	_ = app.writeJSON(w, http.StatusOK, resPayload)
+//}
 
 // clearRequestFromServer clears a test's request from the server.
 func (app *Config) clearRequestFromServer(w http.ResponseWriter, r *http.Request) {
@@ -277,850 +276,874 @@ func (app *Config) clearRequestFromServer(w http.ResponseWriter, r *http.Request
 	_ = app.writeJSON(w, http.StatusOK, resPayload)
 }
 
-// readTask is validating the cluster's current state.
-func (app *Config) readTask(w http.ResponseWriter, r *http.Request) {
-	task := &bulk_loading_cb.ReadTask{}
-	if err := app.readJSON(w, r, task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := checkIdentifierToken(task.IdentifierToken); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	log.Print(task, "read")
-	err := app.serverRequests.AddTask(task.IdentifierToken, tasks.ReadOperation, task)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	req, err := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	seedResult, err := task.Config(req, false)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := app.taskManager.AddTask(task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-	}
-	respPayload := util_sirius.TaskResponse{
-		Seed: fmt.Sprintf("%d", seedResult),
-	}
-	resPayload := jsonResponse{
-		Error:   false,
-		Message: "Successfully started requested doc loading",
-		Data:    respPayload,
-	}
-	_ = app.writeJSON(w, http.StatusOK, resPayload)
-}
+// // readTask is validating the cluster's current state.
+//
+//	func (app *Config) readTask(w http.ResponseWriter, r *http.Request) {
+//		task := &bulk_loading.ReadTask{}
+//		if err_sirius := app.readJSON(w, r, task); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		if err_sirius := checkIdentifierToken(task.IdentifierToken); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		log.Print(task, "read")
+//		err_sirius := app.serverRequests.AddTask(task.IdentifierToken, tasks.ReadOperation, task)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		req, err_sirius := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		seedResult, err_sirius := task.Config(req, false)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		if err_sirius := app.taskManager.AddTask(task); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//		}
+//		respPayload := util_sirius.TaskResponse{
+//			Seed: fmt.Sprintf("%d", seedResult),
+//		}
+//		resPayload := jsonResponse{
+//			Error:   false,
+//			Message: "Successfully started requested doc loading",
+//			Data:    respPayload,
+//		}
+//		_ = app.writeJSON(w, http.StatusOK, resPayload)
+//	}
+//
+// // singleInsertTask is used to insert document in a collection
+//
+//	func (app *Config) singleInsertTask(w http.ResponseWriter, r *http.Request) {
+//		task := &key_based_loading_cb.SingleInsertTask{}
+//		if err_sirius := app.readJSON(w, r, task); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		if err_sirius := checkIdentifierToken(task.IdentifierToken); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		log.Print(task, tasks.SingleInsertOperation)
+//		err_sirius := app.serverRequests.AddTask(task.IdentifierToken, tasks.SingleInsertOperation, task)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		req, err_sirius := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		seedResult, err_sirius := task.Config(req, false)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		if err_sirius := app.taskManager.AddTask(task); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//		}
+//		respPayload := util_sirius.TaskResponse{
+//			Seed: fmt.Sprintf("%d", seedResult),
+//		}
+//		resPayload := jsonResponse{
+//			Error:   false,
+//			Message: "Successfully started requested doc loading",
+//			Data:    respPayload,
+//		}
+//		_ = app.writeJSON(w, http.StatusOK, resPayload)
+//	}
+//
+// // singleDeleteTask is used to delete documents on a collection
+//
+//	func (app *Config) singleDeleteTask(w http.ResponseWriter, r *http.Request) {
+//		task := &key_based_loading_cb.SingleDeleteTask{}
+//		if err_sirius := app.readJSON(w, r, task); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		if err_sirius := checkIdentifierToken(task.IdentifierToken); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		log.Print(task, tasks.SingleDeleteOperation)
+//		err_sirius := app.serverRequests.AddTask(task.IdentifierToken, tasks.SingleDeleteOperation, task)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		req, err_sirius := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		seedResult, err_sirius := task.Config(req, false)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		if err_sirius := app.taskManager.AddTask(task); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//		}
+//		respPayload := util_sirius.TaskResponse{
+//			Seed: fmt.Sprintf("%d", seedResult),
+//		}
+//		resPayload := jsonResponse{
+//			Error:   false,
+//			Message: "Successfully started requested doc loading",
+//			Data:    respPayload,
+//		}
+//		_ = app.writeJSON(w, http.StatusOK, resPayload)
+//	}
+//
+// // singleUpsertTask is used to update the existing document in a collection
+//
+//	func (app *Config) singleUpsertTask(w http.ResponseWriter, r *http.Request) {
+//		task := &key_based_loading_cb.SingleUpsertTask{}
+//		if err_sirius := app.readJSON(w, r, task); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		if err_sirius := checkIdentifierToken(task.IdentifierToken); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		log.Print(task, tasks.SingleUpsertOperation)
+//		err_sirius := app.serverRequests.AddTask(task.IdentifierToken, tasks.SingleUpsertOperation, task)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		req, err_sirius := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		seedResult, err_sirius := task.Config(req, false)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		if err_sirius := app.taskManager.AddTask(task); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//		}
+//		respPayload := util_sirius.TaskResponse{
+//			Seed: fmt.Sprintf("%d", seedResult),
+//		}
+//		resPayload := jsonResponse{
+//			Error:   false,
+//			Message: "Successfully started requested doc loading",
+//			Data:    respPayload,
+//		}
+//		_ = app.writeJSON(w, http.StatusOK, resPayload)
+//	}
+//
+// // singleReadTask is used read documents and verify from a collection.
+//
+//	func (app *Config) singleReadTask(w http.ResponseWriter, r *http.Request) {
+//		task := &key_based_loading_cb.SingleReadTask{}
+//		if err_sirius := app.readJSON(w, r, task); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		if err_sirius := checkIdentifierToken(task.IdentifierToken); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		log.Print(task, tasks.SingleReadOperation)
+//		err_sirius := app.serverRequests.AddTask(task.IdentifierToken, tasks.SingleReadOperation, task)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		req, err_sirius := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		seedResult, err_sirius := task.Config(req, false)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		if err_sirius := app.taskManager.AddTask(task); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//		}
+//		respPayload := util_sirius.TaskResponse{
+//			Seed: fmt.Sprintf("%d", seedResult),
+//		}
+//		resPayload := jsonResponse{
+//			Error:   false,
+//			Message: "Successfully started requested doc loading",
+//			Data:    respPayload,
+//		}
+//		_ = app.writeJSON(w, http.StatusOK, resPayload)
+//	}
+//
+// // singleTouchTask is used to update expiry of documents in a collection
+//
+//	func (app *Config) singleTouchTask(w http.ResponseWriter, r *http.Request) {
+//		task := &key_based_loading_cb.SingleTouchTask{}
+//		if err_sirius := app.readJSON(w, r, task); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		if err_sirius := checkIdentifierToken(task.IdentifierToken); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		log.Print(task, tasks.SingleTouchOperation)
+//		err_sirius := app.serverRequests.AddTask(task.IdentifierToken, tasks.SingleTouchOperation, task)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		req, err_sirius := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		seedResult, err_sirius := task.Config(req, false)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		if err_sirius := app.taskManager.AddTask(task); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//		}
+//		respPayload := util_sirius.TaskResponse{
+//			Seed: fmt.Sprintf("%d", seedResult),
+//		}
+//		resPayload := jsonResponse{
+//			Error:   false,
+//			Message: "Successfully started requested doc loading",
+//			Data:    respPayload,
+//		}
+//		_ = app.writeJSON(w, http.StatusOK, resPayload)
+//	}
+//
+// // singleReplaceTask is used replace content of document on a collection
+//
+//	func (app *Config) singleReplaceTask(w http.ResponseWriter, r *http.Request) {
+//		task := &key_based_loading_cb.SingleReplaceTask{}
+//		if err_sirius := app.readJSON(w, r, task); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		if err_sirius := checkIdentifierToken(task.IdentifierToken); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		log.Print(task, tasks.SingleReplaceOperation)
+//		err_sirius := app.serverRequests.AddTask(task.IdentifierToken, tasks.SingleReplaceOperation, task)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		req, err_sirius := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		seedResult, err_sirius := task.Config(req, false)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		if err_sirius := app.taskManager.AddTask(task); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//		}
+//		respPayload := util_sirius.TaskResponse{
+//			Seed: fmt.Sprintf("%d", seedResult),
+//		}
+//		resPayload := jsonResponse{
+//			Error:   false,
+//			Message: "Successfully started requested doc loading",
+//			Data:    respPayload,
+//		}
+//		_ = app.writeJSON(w, http.StatusOK, resPayload)
+//	}
+//
+// // runQueryTask runs the query workload for a duration of time
+//
+//	func (app *Config) runQueryTask(w http.ResponseWriter, r *http.Request) {
+//		task := &bulk_query_cb.QueryTask{}
+//		if err_sirius := app.readJSON(w, r, task); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		if err_sirius := checkIdentifierToken(task.IdentifierToken); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		log.Print(task, tasks.QueryOperation)
+//		err_sirius := app.serverRequests.AddTask(task.IdentifierToken, tasks.QueryOperation, task)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		req, err_sirius := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		seedResult, err_sirius := task.Config(req, false)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		if err_sirius := app.taskManager.AddTask(task); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//		}
+//		respPayload := util_sirius.TaskResponse{
+//			Seed: fmt.Sprintf("%d", seedResult),
+//		}
+//		resPayload := jsonResponse{
+//			Error:   false,
+//			Message: "Successfully started requested query running",
+//			Data:    respPayload,
+//		}
+//		_ = app.writeJSON(w, http.StatusOK, resPayload)
+//	}
+//
+// // RetryExceptionTask runs the query workload for a duration of time
+//
+//	func (app *Config) RetryExceptionTask(w http.ResponseWriter, r *http.Request) {
+//		task := &bulk_loading.RetryExceptions{}
+//		if err_sirius := app.readJSON(w, r, task); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		if err_sirius := checkIdentifierToken(task.IdentifierToken); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		log.Print(task, tasks.RetryExceptionOperation)
+//		req, err_sirius := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		seedResult, err_sirius := task.Config(req, false)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		if err_sirius := app.taskManager.AddTask(task); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//		}
+//		respPayload := util_sirius.TaskResponse{
+//			Seed: fmt.Sprintf("%d", seedResult),
+//		}
+//		resPayload := jsonResponse{
+//			Error:   false,
+//			Message: "Successfully started requested query running",
+//			Data:    respPayload,
+//		}
+//		_ = app.writeJSON(w, http.StatusOK, resPayload)
+//	}
+//
+// // SubDocInsertTask is used to load bulk sub documents into buckets
+//
+//	func (app *Config) SubDocInsertTask(w http.ResponseWriter, r *http.Request) {
+//		task := &bulk_loading.SubDocInsert{}
+//		if err_sirius := app.readJSON(w, r, task); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		if err_sirius := checkIdentifierToken(task.IdentifierToken); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		log.Print(task, tasks.SubDocInsertOperation)
+//		err_sirius := app.serverRequests.AddTask(task.IdentifierToken, tasks.SubDocInsertOperation, task)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		req, err_sirius := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		resultSeed, err_sirius := task.Config(req, false)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		if err_sirius := app.taskManager.AddTask(task); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//		}
+//		respPayload := util_sirius.TaskResponse{
+//			Seed: fmt.Sprintf("%d", resultSeed),
+//		}
+//		resPayload := jsonResponse{
+//			Error:   false,
+//			Message: "Successfully started requested doc loading",
+//			Data:    respPayload,
+//		}
+//		_ = app.writeJSON(w, http.StatusOK, resPayload)
+//	}
+//
+// // SubDocUpsertTask is used to bulk updating sub documents into buckets
+//
+//	func (app *Config) SubDocUpsertTask(w http.ResponseWriter, r *http.Request) {
+//		task := &bulk_loading.SubDocUpsert{}
+//		if err_sirius := app.readJSON(w, r, task); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		if err_sirius := checkIdentifierToken(task.IdentifierToken); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		log.Print(task, tasks.SubDocUpsertOperation)
+//		err_sirius := app.serverRequests.AddTask(task.IdentifierToken, tasks.SubDocUpsertOperation, task)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		req, err_sirius := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		resultSeed, err_sirius := task.Config(req, false)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		if err_sirius := app.taskManager.AddTask(task); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//		}
+//		respPayload := util_sirius.TaskResponse{
+//			Seed: fmt.Sprintf("%d", resultSeed),
+//		}
+//		resPayload := jsonResponse{
+//			Error:   false,
+//			Message: "Successfully started requested doc loading",
+//			Data:    respPayload,
+//		}
+//		_ = app.writeJSON(w, http.StatusOK, resPayload)
+//	}
+//
+// // SubDocDeleteTask is used to bulk updating sub documents into buckets
+//
+//	func (app *Config) SubDocDeleteTask(w http.ResponseWriter, r *http.Request) {
+//		task := &bulk_loading.SubDocDelete{}
+//		if err_sirius := app.readJSON(w, r, task); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		if err_sirius := checkIdentifierToken(task.IdentifierToken); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		log.Print(task, tasks.SubDocDeleteOperation)
+//		err_sirius := app.serverRequests.AddTask(task.IdentifierToken, tasks.SubDocDeleteOperation, task)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		req, err_sirius := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		resultSeed, err_sirius := task.Config(req, false)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		if err_sirius := app.taskManager.AddTask(task); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//		}
+//		respPayload := util_sirius.TaskResponse{
+//			Seed: fmt.Sprintf("%d", resultSeed),
+//		}
+//		resPayload := jsonResponse{
+//			Error:   false,
+//			Message: "Successfully started requested doc loading",
+//			Data:    respPayload,
+//		}
+//		_ = app.writeJSON(w, http.StatusOK, resPayload)
+//	}
+//
+// // SubDocReadTask is used to bulk updating sub documents into buckets
+//
+//	func (app *Config) SubDocReadTask(w http.ResponseWriter, r *http.Request) {
+//		task := &bulk_loading.SubDocRead{}
+//		if err_sirius := app.readJSON(w, r, task); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		if err_sirius := checkIdentifierToken(task.IdentifierToken); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		log.Print(task, tasks.SubDocReadOperation)
+//		err_sirius := app.serverRequests.AddTask(task.IdentifierToken, tasks.SubDocReadOperation, task)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		req, err_sirius := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		resultSeed, err_sirius := task.Config(req, false)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		if err_sirius := app.taskManager.AddTask(task); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//		}
+//		respPayload := util_sirius.TaskResponse{
+//			Seed: fmt.Sprintf("%d", resultSeed),
+//		}
+//		resPayload := jsonResponse{
+//			Error:   false,
+//			Message: "Successfully started requested doc loading",
+//			Data:    respPayload,
+//		}
+//		_ = app.writeJSON(w, http.StatusOK, resPayload)
+//	}
+//
+// // SubDocReplaceTask is used to bulk updating sub documents into buckets
+//
+//	func (app *Config) SubDocReplaceTask(w http.ResponseWriter, r *http.Request) {
+//		task := &bulk_loading.SubDocReplace{}
+//		if err_sirius := app.readJSON(w, r, task); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		if err_sirius := checkIdentifierToken(task.IdentifierToken); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		log.Print(task, tasks.SubDocReplaceOperation)
+//		err_sirius := app.serverRequests.AddTask(task.IdentifierToken, tasks.SubDocReplaceOperation, task)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		req, err_sirius := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		resultSeed, err_sirius := task.Config(req, false)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		if err_sirius := app.taskManager.AddTask(task); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//		}
+//		respPayload := util_sirius.TaskResponse{
+//			Seed: fmt.Sprintf("%d", resultSeed),
+//		}
+//		resPayload := jsonResponse{
+//			Error:   false,
+//			Message: "Successfully started requested doc loading",
+//			Data:    respPayload,
+//		}
+//		_ = app.writeJSON(w, http.StatusOK, resPayload)
+//	}
+//
+// // SingleSubDocInsert is used to insert user's input value in sub docs
+//
+//	func (app *Config) SingleSubDocInsert(w http.ResponseWriter, r *http.Request) {
+//		task := &key_based_loading_cb.SingleSubDocInsert{}
+//		if err_sirius := app.readJSON(w, r, task); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		if err_sirius := checkIdentifierToken(task.IdentifierToken); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		log.Print(task, tasks.SingleSubDocInsertOperation)
+//		err_sirius := app.serverRequests.AddTask(task.IdentifierToken, tasks.SingleSubDocInsertOperation, task)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		req, err_sirius := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		resultSeed, err_sirius := task.Config(req, false)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		if err_sirius := app.taskManager.AddTask(task); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//		}
+//		respPayload := util_sirius.TaskResponse{
+//			Seed: fmt.Sprintf("%d", resultSeed),
+//		}
+//		resPayload := jsonResponse{
+//			Error:   false,
+//			Message: "Successfully started requested doc loading",
+//			Data:    respPayload,
+//		}
+//		_ = app.writeJSON(w, http.StatusOK, resPayload)
+//	}
+//
+// // SingleSubDocUpsert is used to update user's input value in sub docs
+//
+//	func (app *Config) SingleSubDocUpsert(w http.ResponseWriter, r *http.Request) {
+//		task := &key_based_loading_cb.SingleSubDocUpsert{}
+//		if err_sirius := app.readJSON(w, r, task); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		if err_sirius := checkIdentifierToken(task.IdentifierToken); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		log.Print(task, tasks.SingleSubDocUpsertOperation)
+//		err_sirius := app.serverRequests.AddTask(task.IdentifierToken, tasks.SingleSubDocUpsertOperation, task)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		req, err_sirius := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		resultSeed, err_sirius := task.Config(req, false)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		if err_sirius := app.taskManager.AddTask(task); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//		}
+//		respPayload := util_sirius.TaskResponse{
+//			Seed: fmt.Sprintf("%d", resultSeed),
+//		}
+//		resPayload := jsonResponse{
+//			Error:   false,
+//			Message: "Successfully started requested doc loading",
+//			Data:    respPayload,
+//		}
+//		_ = app.writeJSON(w, http.StatusOK, resPayload)
+//	}
+//
+// // SingleSubDocReplace is used to replace user's input value in sub docs
+//
+//	func (app *Config) SingleSubDocReplace(w http.ResponseWriter, r *http.Request) {
+//		task := &key_based_loading_cb.SingleSubDocReplace{}
+//		if err_sirius := app.readJSON(w, r, task); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		if err_sirius := checkIdentifierToken(task.IdentifierToken); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		log.Print(task, tasks.SingleSubDocReplaceOperation)
+//		err_sirius := app.serverRequests.AddTask(task.IdentifierToken, tasks.SingleSubDocReplaceOperation, task)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		req, err_sirius := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		resultSeed, err_sirius := task.Config(req, false)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		if err_sirius := app.taskManager.AddTask(task); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//		}
+//		respPayload := util_sirius.TaskResponse{
+//			Seed: fmt.Sprintf("%d", resultSeed),
+//		}
+//		resPayload := jsonResponse{
+//			Error:   false,
+//			Message: "Successfully started requested doc loading",
+//			Data:    respPayload,
+//		}
+//		_ = app.writeJSON(w, http.StatusOK, resPayload)
+//	}
+//
+// // SingleSubDocDelete is used delete user's sub document
+//
+//	func (app *Config) SingleSubDocDelete(w http.ResponseWriter, r *http.Request) {
+//		task := &key_based_loading_cb.SingleSubDocDelete{}
+//		if err_sirius := app.readJSON(w, r, task); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		if err_sirius := checkIdentifierToken(task.IdentifierToken); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		log.Print(task, tasks.SingleSubDocDeleteOperation)
+//		err_sirius := app.serverRequests.AddTask(task.IdentifierToken, tasks.SingleSubDocDeleteOperation, task)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		req, err_sirius := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		resultSeed, err_sirius := task.Config(req, false)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		if err_sirius := app.taskManager.AddTask(task); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//		}
+//		respPayload := util_sirius.TaskResponse{
+//			Seed: fmt.Sprintf("%d", resultSeed),
+//		}
+//		resPayload := jsonResponse{
+//			Error:   false,
+//			Message: "Successfully started requested doc loading",
+//			Data:    respPayload,
+//		}
+//		_ = app.writeJSON(w, http.StatusOK, resPayload)
+//	}
+//
+// // SingleSubDocRead is used to read user's sub document
+//
+//	func (app *Config) SingleSubDocRead(w http.ResponseWriter, r *http.Request) {
+//		task := &key_based_loading_cb.SingleSubDocRead{}
+//		if err_sirius := app.readJSON(w, r, task); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		if err_sirius := checkIdentifierToken(task.IdentifierToken); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		log.Print(task, tasks.SingleSubDocReadOperation)
+//		err_sirius := app.serverRequests.AddTask(task.IdentifierToken, tasks.SingleSubDocReadOperation, task)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		req, err_sirius := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		resultSeed, err_sirius := task.Config(req, false)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		if err_sirius := app.taskManager.AddTask(task); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//		}
+//		respPayload := util_sirius.TaskResponse{
+//			Seed: fmt.Sprintf("%d", resultSeed),
+//		}
+//		resPayload := jsonResponse{
+//			Error:   false,
+//			Message: "Successfully started requested doc loading",
+//			Data:    respPayload,
+//		}
+//		_ = app.writeJSON(w, http.StatusOK, resPayload)
+//	}
+//
+// // SingleDocValidate is used to read user's sub document
+//
+//	func (app *Config) SingleDocValidate(w http.ResponseWriter, r *http.Request) {
+//		task := &key_based_loading_cb.SingleValidate{}
+//		if err_sirius := app.readJSON(w, r, task); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		if err_sirius := checkIdentifierToken(task.IdentifierToken); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		log.Print(task, tasks.SingleDocValidateOperation)
+//		err_sirius := app.serverRequests.AddTask(task.IdentifierToken, tasks.SingleDocValidateOperation, task)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		req, err_sirius := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		resultSeed, err_sirius := task.Config(req, false)
+//		if err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//			return
+//		}
+//		if err_sirius := app.taskManager.AddTask(task); err_sirius != nil {
+//			_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
+//		}
+//		respPayload := util_sirius.TaskResponse{
+//			Seed: fmt.Sprintf("%d", resultSeed),
+//		}
+//		resPayload := jsonResponse{
+//			Error:   false,
+//			Message: "Successfully started requested doc loading",
+//			Data:    respPayload,
+//		}
+//		_ = app.writeJSON(w, http.StatusOK, resPayload)
+//	}
+//
 
-// singleInsertTask is used to insert document in a collection
-func (app *Config) singleInsertTask(w http.ResponseWriter, r *http.Request) {
-	task := &key_based_loading_cb.SingleInsertTask{}
-	if err := app.readJSON(w, r, task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := checkIdentifierToken(task.IdentifierToken); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	log.Print(task, tasks.SingleInsertOperation)
-	err := app.serverRequests.AddTask(task.IdentifierToken, tasks.SingleInsertOperation, task)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	req, err := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	seedResult, err := task.Config(req, false)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := app.taskManager.AddTask(task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-	}
-	respPayload := util_sirius.TaskResponse{
-		Seed: fmt.Sprintf("%d", seedResult),
-	}
-	resPayload := jsonResponse{
-		Error:   false,
-		Message: "Successfully started requested doc loading",
-		Data:    respPayload,
-	}
-	_ = app.writeJSON(w, http.StatusOK, resPayload)
-}
+// WarmUpBucket establish a connection to bucket
+func (app *Config) WarmUpBucket(w http.ResponseWriter, r *http.Request) {
 
-// singleDeleteTask is used to delete documents on a collection
-func (app *Config) singleDeleteTask(w http.ResponseWriter, r *http.Request) {
-	task := &key_based_loading_cb.SingleDeleteTask{}
-	if err := app.readJSON(w, r, task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := checkIdentifierToken(task.IdentifierToken); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	log.Print(task, tasks.SingleDeleteOperation)
-	err := app.serverRequests.AddTask(task.IdentifierToken, tasks.SingleDeleteOperation, task)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	req, err := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	seedResult, err := task.Config(req, false)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
+	task := &db_util.BucketWarmUpTask{}
+	if err_sirius := app.readJSON(w, r, task); err_sirius != nil {
+		_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
 		return
 	}
-	if err := app.taskManager.AddTask(task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-	}
-	respPayload := util_sirius.TaskResponse{
-		Seed: fmt.Sprintf("%d", seedResult),
-	}
-	resPayload := jsonResponse{
-		Error:   false,
-		Message: "Successfully started requested doc loading",
-		Data:    respPayload,
-	}
-	_ = app.writeJSON(w, http.StatusOK, resPayload)
-}
+	task.Operation = tasks.BucketWarmUpOperation
 
-// singleUpsertTask is used to update the existing document in a collection
-func (app *Config) singleUpsertTask(w http.ResponseWriter, r *http.Request) {
-	task := &key_based_loading_cb.SingleUpsertTask{}
-	if err := app.readJSON(w, r, task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
+	if err_sirius := checkIdentifierToken(task.IdentifierToken); err_sirius != nil {
+		_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
 		return
 	}
-	if err := checkIdentifierToken(task.IdentifierToken); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	log.Print(task, tasks.SingleUpsertOperation)
-	err := app.serverRequests.AddTask(task.IdentifierToken, tasks.SingleUpsertOperation, task)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
+	log.Print(task, tasks.BucketWarmUpOperation)
+	err_sirius := app.serverRequests.AddTask(task.IdentifierToken, tasks.BucketWarmUpOperation, task)
+	if err_sirius != nil {
+		_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
 		return
 	}
-	req, err := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
+	req, err_sirius := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
+	if err_sirius != nil {
+		_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
 		return
 	}
-	seedResult, err := task.Config(req, false)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
+	resultSeed, err_sirius := task.Config(req, false)
+	if err_sirius != nil {
+		_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
 		return
 	}
-	if err := app.taskManager.AddTask(task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-	}
-	respPayload := util_sirius.TaskResponse{
-		Seed: fmt.Sprintf("%d", seedResult),
-	}
-	resPayload := jsonResponse{
-		Error:   false,
-		Message: "Successfully started requested doc loading",
-		Data:    respPayload,
-	}
-	_ = app.writeJSON(w, http.StatusOK, resPayload)
-}
-
-// singleReadTask is used read documents and verify from a collection.
-func (app *Config) singleReadTask(w http.ResponseWriter, r *http.Request) {
-	task := &key_based_loading_cb.SingleReadTask{}
-	if err := app.readJSON(w, r, task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := checkIdentifierToken(task.IdentifierToken); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	log.Print(task, tasks.SingleReadOperation)
-	err := app.serverRequests.AddTask(task.IdentifierToken, tasks.SingleReadOperation, task)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	req, err := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	seedResult, err := task.Config(req, false)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := app.taskManager.AddTask(task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-	}
-	respPayload := util_sirius.TaskResponse{
-		Seed: fmt.Sprintf("%d", seedResult),
-	}
-	resPayload := jsonResponse{
-		Error:   false,
-		Message: "Successfully started requested doc loading",
-		Data:    respPayload,
-	}
-	_ = app.writeJSON(w, http.StatusOK, resPayload)
-}
-
-// singleTouchTask is used to update expiry of documents in a collection
-func (app *Config) singleTouchTask(w http.ResponseWriter, r *http.Request) {
-	task := &key_based_loading_cb.SingleTouchTask{}
-	if err := app.readJSON(w, r, task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := checkIdentifierToken(task.IdentifierToken); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	log.Print(task, tasks.SingleTouchOperation)
-	err := app.serverRequests.AddTask(task.IdentifierToken, tasks.SingleTouchOperation, task)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	req, err := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	seedResult, err := task.Config(req, false)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := app.taskManager.AddTask(task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-	}
-	respPayload := util_sirius.TaskResponse{
-		Seed: fmt.Sprintf("%d", seedResult),
-	}
-	resPayload := jsonResponse{
-		Error:   false,
-		Message: "Successfully started requested doc loading",
-		Data:    respPayload,
-	}
-	_ = app.writeJSON(w, http.StatusOK, resPayload)
-}
-
-// singleReplaceTask is used replace content of document on a collection
-func (app *Config) singleReplaceTask(w http.ResponseWriter, r *http.Request) {
-	task := &key_based_loading_cb.SingleReplaceTask{}
-	if err := app.readJSON(w, r, task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := checkIdentifierToken(task.IdentifierToken); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	log.Print(task, tasks.SingleReplaceOperation)
-	err := app.serverRequests.AddTask(task.IdentifierToken, tasks.SingleReplaceOperation, task)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	req, err := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	seedResult, err := task.Config(req, false)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := app.taskManager.AddTask(task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-	}
-	respPayload := util_sirius.TaskResponse{
-		Seed: fmt.Sprintf("%d", seedResult),
-	}
-	resPayload := jsonResponse{
-		Error:   false,
-		Message: "Successfully started requested doc loading",
-		Data:    respPayload,
-	}
-	_ = app.writeJSON(w, http.StatusOK, resPayload)
-}
-
-// runQueryTask runs the query workload for a duration of time
-func (app *Config) runQueryTask(w http.ResponseWriter, r *http.Request) {
-	task := &bulk_query_cb.QueryTask{}
-	if err := app.readJSON(w, r, task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := checkIdentifierToken(task.IdentifierToken); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	log.Print(task, tasks.QueryOperation)
-	err := app.serverRequests.AddTask(task.IdentifierToken, tasks.QueryOperation, task)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	req, err := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	seedResult, err := task.Config(req, false)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := app.taskManager.AddTask(task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-	}
-	respPayload := util_sirius.TaskResponse{
-		Seed: fmt.Sprintf("%d", seedResult),
-	}
-	resPayload := jsonResponse{
-		Error:   false,
-		Message: "Successfully started requested query running",
-		Data:    respPayload,
-	}
-	_ = app.writeJSON(w, http.StatusOK, resPayload)
-}
-
-// RetryExceptionTask runs the query workload for a duration of time
-func (app *Config) RetryExceptionTask(w http.ResponseWriter, r *http.Request) {
-	task := &bulk_loading_cb.RetryExceptions{}
-	if err := app.readJSON(w, r, task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := checkIdentifierToken(task.IdentifierToken); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	log.Print(task, tasks.RetryExceptionOperation)
-	req, err := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	seedResult, err := task.Config(req, false)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := app.taskManager.AddTask(task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-	}
-	respPayload := util_sirius.TaskResponse{
-		Seed: fmt.Sprintf("%d", seedResult),
-	}
-	resPayload := jsonResponse{
-		Error:   false,
-		Message: "Successfully started requested query running",
-		Data:    respPayload,
-	}
-	_ = app.writeJSON(w, http.StatusOK, resPayload)
-}
-
-// SubDocInsertTask is used to load bulk sub documents into buckets
-func (app *Config) SubDocInsertTask(w http.ResponseWriter, r *http.Request) {
-	task := &bulk_loading_cb.SubDocInsert{}
-	if err := app.readJSON(w, r, task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := checkIdentifierToken(task.IdentifierToken); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	log.Print(task, tasks.SubDocInsertOperation)
-	err := app.serverRequests.AddTask(task.IdentifierToken, tasks.SubDocInsertOperation, task)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	req, err := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	resultSeed, err := task.Config(req, false)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := app.taskManager.AddTask(task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-	}
-	respPayload := util_sirius.TaskResponse{
-		Seed: fmt.Sprintf("%d", resultSeed),
-	}
-	resPayload := jsonResponse{
-		Error:   false,
-		Message: "Successfully started requested doc loading",
-		Data:    respPayload,
-	}
-	_ = app.writeJSON(w, http.StatusOK, resPayload)
-}
-
-// SubDocUpsertTask is used to bulk updating sub documents into buckets
-func (app *Config) SubDocUpsertTask(w http.ResponseWriter, r *http.Request) {
-	task := &bulk_loading_cb.SubDocUpsert{}
-	if err := app.readJSON(w, r, task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := checkIdentifierToken(task.IdentifierToken); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	log.Print(task, tasks.SubDocUpsertOperation)
-	err := app.serverRequests.AddTask(task.IdentifierToken, tasks.SubDocUpsertOperation, task)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	req, err := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	resultSeed, err := task.Config(req, false)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := app.taskManager.AddTask(task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-	}
-	respPayload := util_sirius.TaskResponse{
-		Seed: fmt.Sprintf("%d", resultSeed),
-	}
-	resPayload := jsonResponse{
-		Error:   false,
-		Message: "Successfully started requested doc loading",
-		Data:    respPayload,
-	}
-	_ = app.writeJSON(w, http.StatusOK, resPayload)
-}
-
-// SubDocDeleteTask is used to bulk updating sub documents into buckets
-func (app *Config) SubDocDeleteTask(w http.ResponseWriter, r *http.Request) {
-	task := &bulk_loading_cb.SubDocDelete{}
-	if err := app.readJSON(w, r, task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := checkIdentifierToken(task.IdentifierToken); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	log.Print(task, tasks.SubDocDeleteOperation)
-	err := app.serverRequests.AddTask(task.IdentifierToken, tasks.SubDocDeleteOperation, task)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	req, err := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	resultSeed, err := task.Config(req, false)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := app.taskManager.AddTask(task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-	}
-	respPayload := util_sirius.TaskResponse{
-		Seed: fmt.Sprintf("%d", resultSeed),
-	}
-	resPayload := jsonResponse{
-		Error:   false,
-		Message: "Successfully started requested doc loading",
-		Data:    respPayload,
-	}
-	_ = app.writeJSON(w, http.StatusOK, resPayload)
-}
-
-// SubDocReadTask is used to bulk updating sub documents into buckets
-func (app *Config) SubDocReadTask(w http.ResponseWriter, r *http.Request) {
-	task := &bulk_loading_cb.SubDocRead{}
-	if err := app.readJSON(w, r, task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := checkIdentifierToken(task.IdentifierToken); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	log.Print(task, tasks.SubDocReadOperation)
-	err := app.serverRequests.AddTask(task.IdentifierToken, tasks.SubDocReadOperation, task)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	req, err := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	resultSeed, err := task.Config(req, false)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := app.taskManager.AddTask(task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-	}
-	respPayload := util_sirius.TaskResponse{
-		Seed: fmt.Sprintf("%d", resultSeed),
-	}
-	resPayload := jsonResponse{
-		Error:   false,
-		Message: "Successfully started requested doc loading",
-		Data:    respPayload,
-	}
-	_ = app.writeJSON(w, http.StatusOK, resPayload)
-}
-
-// SubDocReplaceTask is used to bulk updating sub documents into buckets
-func (app *Config) SubDocReplaceTask(w http.ResponseWriter, r *http.Request) {
-	task := &bulk_loading_cb.SubDocReplace{}
-	if err := app.readJSON(w, r, task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := checkIdentifierToken(task.IdentifierToken); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	log.Print(task, tasks.SubDocReplaceOperation)
-	err := app.serverRequests.AddTask(task.IdentifierToken, tasks.SubDocReplaceOperation, task)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	req, err := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	resultSeed, err := task.Config(req, false)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := app.taskManager.AddTask(task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-	}
-	respPayload := util_sirius.TaskResponse{
-		Seed: fmt.Sprintf("%d", resultSeed),
-	}
-	resPayload := jsonResponse{
-		Error:   false,
-		Message: "Successfully started requested doc loading",
-		Data:    respPayload,
-	}
-	_ = app.writeJSON(w, http.StatusOK, resPayload)
-}
-
-// SingleSubDocInsert is used to insert user's input value in sub docs
-func (app *Config) SingleSubDocInsert(w http.ResponseWriter, r *http.Request) {
-	task := &key_based_loading_cb.SingleSubDocInsert{}
-	if err := app.readJSON(w, r, task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := checkIdentifierToken(task.IdentifierToken); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	log.Print(task, tasks.SingleSubDocInsertOperation)
-	err := app.serverRequests.AddTask(task.IdentifierToken, tasks.SingleSubDocInsertOperation, task)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	req, err := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	resultSeed, err := task.Config(req, false)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := app.taskManager.AddTask(task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-	}
-	respPayload := util_sirius.TaskResponse{
-		Seed: fmt.Sprintf("%d", resultSeed),
-	}
-	resPayload := jsonResponse{
-		Error:   false,
-		Message: "Successfully started requested doc loading",
-		Data:    respPayload,
-	}
-	_ = app.writeJSON(w, http.StatusOK, resPayload)
-}
-
-// SingleSubDocUpsert is used to update user's input value in sub docs
-func (app *Config) SingleSubDocUpsert(w http.ResponseWriter, r *http.Request) {
-	task := &key_based_loading_cb.SingleSubDocUpsert{}
-	if err := app.readJSON(w, r, task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := checkIdentifierToken(task.IdentifierToken); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	log.Print(task, tasks.SingleSubDocUpsertOperation)
-	err := app.serverRequests.AddTask(task.IdentifierToken, tasks.SingleSubDocUpsertOperation, task)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	req, err := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	resultSeed, err := task.Config(req, false)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := app.taskManager.AddTask(task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-	}
-	respPayload := util_sirius.TaskResponse{
-		Seed: fmt.Sprintf("%d", resultSeed),
-	}
-	resPayload := jsonResponse{
-		Error:   false,
-		Message: "Successfully started requested doc loading",
-		Data:    respPayload,
-	}
-	_ = app.writeJSON(w, http.StatusOK, resPayload)
-}
-
-// SingleSubDocReplace is used to replace user's input value in sub docs
-func (app *Config) SingleSubDocReplace(w http.ResponseWriter, r *http.Request) {
-	task := &key_based_loading_cb.SingleSubDocReplace{}
-	if err := app.readJSON(w, r, task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := checkIdentifierToken(task.IdentifierToken); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	log.Print(task, tasks.SingleSubDocReplaceOperation)
-	err := app.serverRequests.AddTask(task.IdentifierToken, tasks.SingleSubDocReplaceOperation, task)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	req, err := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	resultSeed, err := task.Config(req, false)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := app.taskManager.AddTask(task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-	}
-	respPayload := util_sirius.TaskResponse{
-		Seed: fmt.Sprintf("%d", resultSeed),
-	}
-	resPayload := jsonResponse{
-		Error:   false,
-		Message: "Successfully started requested doc loading",
-		Data:    respPayload,
-	}
-	_ = app.writeJSON(w, http.StatusOK, resPayload)
-}
-
-// SingleSubDocDelete is used delete user's sub document
-func (app *Config) SingleSubDocDelete(w http.ResponseWriter, r *http.Request) {
-	task := &key_based_loading_cb.SingleSubDocDelete{}
-	if err := app.readJSON(w, r, task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := checkIdentifierToken(task.IdentifierToken); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	log.Print(task, tasks.SingleSubDocDeleteOperation)
-	err := app.serverRequests.AddTask(task.IdentifierToken, tasks.SingleSubDocDeleteOperation, task)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	req, err := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	resultSeed, err := task.Config(req, false)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := app.taskManager.AddTask(task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-	}
-	respPayload := util_sirius.TaskResponse{
-		Seed: fmt.Sprintf("%d", resultSeed),
-	}
-	resPayload := jsonResponse{
-		Error:   false,
-		Message: "Successfully started requested doc loading",
-		Data:    respPayload,
-	}
-	_ = app.writeJSON(w, http.StatusOK, resPayload)
-}
-
-// SingleSubDocRead is used to read user's sub document
-func (app *Config) SingleSubDocRead(w http.ResponseWriter, r *http.Request) {
-	task := &key_based_loading_cb.SingleSubDocRead{}
-	if err := app.readJSON(w, r, task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := checkIdentifierToken(task.IdentifierToken); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	log.Print(task, tasks.SingleSubDocReadOperation)
-	err := app.serverRequests.AddTask(task.IdentifierToken, tasks.SingleSubDocReadOperation, task)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	req, err := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	resultSeed, err := task.Config(req, false)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := app.taskManager.AddTask(task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-	}
-	respPayload := util_sirius.TaskResponse{
-		Seed: fmt.Sprintf("%d", resultSeed),
-	}
-	resPayload := jsonResponse{
-		Error:   false,
-		Message: "Successfully started requested doc loading",
-		Data:    respPayload,
-	}
-	_ = app.writeJSON(w, http.StatusOK, resPayload)
-}
-
-// SingleDocValidate is used to read user's sub document
-func (app *Config) SingleDocValidate(w http.ResponseWriter, r *http.Request) {
-	task := &key_based_loading_cb.SingleValidate{}
-	if err := app.readJSON(w, r, task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := checkIdentifierToken(task.IdentifierToken); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	log.Print(task, tasks.SingleDocValidateOperation)
-	err := app.serverRequests.AddTask(task.IdentifierToken, tasks.SingleDocValidateOperation, task)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	req, err := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	resultSeed, err := task.Config(req, false)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := app.taskManager.AddTask(task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-	}
-	respPayload := util_sirius.TaskResponse{
-		Seed: fmt.Sprintf("%d", resultSeed),
-	}
-	resPayload := jsonResponse{
-		Error:   false,
-		Message: "Successfully started requested doc loading",
-		Data:    respPayload,
-	}
-	_ = app.writeJSON(w, http.StatusOK, resPayload)
-}
-
-// WarmUpBucket establish a connection to bucket
-func (app *Config) WarmUpBucket(w http.ResponseWriter, r *http.Request) {
-	task := &util_cb.BucketWarmUpTask{}
-	if err := app.readJSON(w, r, task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := checkIdentifierToken(task.IdentifierToken); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	log.Print(task, tasks.BucketWarmUpOperation)
-	err := app.serverRequests.AddTask(task.IdentifierToken, tasks.BucketWarmUpOperation, task)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	req, err := app.serverRequests.GetRequestOfIdentifier(task.IdentifierToken)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	resultSeed, err := task.Config(req, false)
-	if err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
-		return
-	}
-	if err := app.taskManager.AddTask(task); err != nil {
-		_ = app.errorJSON(w, err, http.StatusUnprocessableEntity)
+	if err_sirius := app.taskManager.AddTask(task); err_sirius != nil {
+		_ = app.errorJSON(w, err_sirius, http.StatusUnprocessableEntity)
 	}
 	respPayload := util_sirius.TaskResponse{
 		Seed: fmt.Sprintf("%d", resultSeed),
diff --git a/cmd/api/helpers.go b/cmd/api/helpers.go
index f0a4e39..ae39a67 100644
--- a/cmd/api/helpers.go
+++ b/cmd/api/helpers.go
@@ -10,10 +10,7 @@ import (
 	"github.com/couchbaselabs/sirius/internal/task_result"
 	"github.com/couchbaselabs/sirius/internal/task_state"
 	"github.com/couchbaselabs/sirius/internal/tasks"
-	"github.com/couchbaselabs/sirius/internal/tasks/bulk_loading_cb"
-	"github.com/couchbaselabs/sirius/internal/tasks/bulk_query_cb"
-	"github.com/couchbaselabs/sirius/internal/tasks/key_based_loading_cb"
-	"github.com/couchbaselabs/sirius/internal/tasks/util_cb"
+	"github.com/couchbaselabs/sirius/internal/tasks/bulk_loading"
 	"github.com/couchbaselabs/sirius/internal/tasks/util_sirius"
 	"github.com/couchbaselabs/sirius/internal/template"
 	"net/http"
@@ -89,40 +86,44 @@ func registerInterfaces() {
 	gob.Register(&template.Hotel{})
 	gob.Register(&template.SmallTemplate{})
 	gob.Register(&server_requests.ServerRequests{})
-	gob.Register(&bulk_loading_cb.InsertTask{})
-	gob.Register(&bulk_loading_cb.UpsertTask{})
+	gob.Register(&bulk_loading.GenericLoadingTask{})
+	//gob.Register(&bulk_loading.UpsertTask{})
 	gob.Register(&util_sirius.TaskResult{})
-	gob.Register(&bulk_loading_cb.DeleteTask{})
-	gob.Register(&bulk_loading_cb.TouchTask{})
-	gob.Register(&bulk_loading_cb.ValidateTask{})
+	//gob.Register(&bulk_loading.DeleteTask{})
+	//gob.Register(&bulk_loading.TouchTask{})
+	//gob.Register(&bulk_loading.ValidateTask{})
 	gob.Register(&task_result.TaskResult{})
 	gob.Register(&task_state.TaskState{})
-	gob.Register(&bulk_loading_cb.ReadTask{})
-	gob.Register(&key_based_loading_cb.SingleInsertTask{})
-	gob.Register(&key_based_loading_cb.SingleDeleteTask{})
-	gob.Register(&key_based_loading_cb.SingleUpsertTask{})
-	gob.Register(&key_based_loading_cb.SingleReadTask{})
-	gob.Register(&key_based_loading_cb.SingleTouchTask{})
-	gob.Register(&key_based_loading_cb.SingleReplaceTask{})
-	gob.Register(&bulk_query_cb.QueryTask{})
+	//gob.Register(&bulk_loading.ReadTask{})
+	//gob.Register(&key_based_loading_cb.SingleInsertTask{})
+	//gob.Register(&key_based_loading_cb.SingleDeleteTask{})
+	//gob.Register(&key_based_loading_cb.SingleUpsertTask{})
+	//gob.Register(&key_based_loading_cb.SingleReadTask{})
+	//gob.Register(&key_based_loading_cb.SingleTouchTask{})
+	//gob.Register(&key_based_loading_cb.SingleReplaceTask{})
+	//gob.Register(&bulk_query_cb.QueryTask{})
 	gob.Register(&meta_data.MetaData{})
 	gob.Register(&meta_data.CollectionMetaData{})
-	gob.Register(&bulk_loading_cb.RetryExceptions{})
-	gob.Register(&bulk_loading_cb.SubDocInsert{})
-	gob.Register(&bulk_loading_cb.SubDocUpsert{})
-	gob.Register(&bulk_loading_cb.SubDocDelete{})
-	gob.Register(&bulk_loading_cb.SubDocRead{})
-	gob.Register(&bulk_loading_cb.SubDocReplace{})
-	gob.Register(&key_based_loading_cb.SingleSubDocInsert{})
-	gob.Register(&key_based_loading_cb.SingleSubDocUpsert{})
-	gob.Register(&key_based_loading_cb.SingleSubDocReplace{})
-	gob.Register(&key_based_loading_cb.SingleSubDocDelete{})
-	gob.Register(&key_based_loading_cb.SingleSubDocRead{})
-	gob.Register(&key_based_loading_cb.SingleValidate{})
-	gob.Register(&util_cb.BucketWarmUpTask{})
+	//gob.Register(&bulk_loading.RetryExceptions{})
+	//gob.Register(&bulk_loading.SubDocInsert{})
+	//gob.Register(&bulk_loading.SubDocUpsert{})
+	//gob.Register(&bulk_loading.SubDocDelete{})
+	//gob.Register(&bulk_loading.SubDocRead{})
+	//gob.Register(&bulk_loading.SubDocReplace{})
+	//gob.Register(&key_based_loading_cb.SingleSubDocInsert{})
+	//gob.Register(&key_based_loading_cb.SingleSubDocUpsert{})
+	//gob.Register(&key_based_loading_cb.SingleSubDocReplace{})
+	//gob.Register(&key_based_loading_cb.SingleSubDocDelete{})
+	//gob.Register(&key_based_loading_cb.SingleSubDocRead{})
+	//gob.Register(&key_based_loading_cb.SingleValidate{})
+	//gob.Register(&db_util.BucketWarmUpTask{})
 
 	r := sirius_documentation.Register{}
-	for _, i := range r.HelperStruct() {
-		gob.Register(i)
+	for _, taskReg := range r.RegisteredTasks() {
+		gob.Register(taskReg.Config)
+	}
+
+	for _, helper := range r.HelperStruct() {
+		gob.Register(helper)
 	}
 }
diff --git a/cmd/api/main.go b/cmd/api/main.go
index e93741c..5cc0c14 100644
--- a/cmd/api/main.go
+++ b/cmd/api/main.go
@@ -20,8 +20,13 @@ type Config struct {
 }
 
 func main() {
+	// Generate Sirius Documentation
+	go sirius_documentation.Generate()
+
+	// registers structures which will store and load from disk
 	registerInterfaces()
 
+	// setting up the log file.
 	logFile, err := os.OpenFile(getFileName(), os.O_RDWR|os.O_CREATE|os.O_APPEND, 0666)
 	if err != nil {
 		log.Fatalf("error opening file: %v", err)
@@ -30,13 +35,12 @@ func main() {
 	mw := io.MultiWriter(os.Stdout, logFile)
 	log.SetOutput(mw)
 
+	// app will contain entities which are required at top most level of sirius.
 	app := Config{
 		taskManager:    tasks_manager.NewTasKManager(TaskQueueSize),
 		serverRequests: server_requests.NewServerRequests(),
 	}
-	go sirius_documentation.Generate()
 
-	//define the server
 	log.Printf("Starting Document Loading Service at port %s\n", webPort)
 	srv := http.Server{
 		Addr:    fmt.Sprintf(":%s", webPort),
diff --git a/cmd/api/routes.go b/cmd/api/routes.go
index 1b3ef38..e43aaa6 100644
--- a/cmd/api/routes.go
+++ b/cmd/api/routes.go
@@ -32,31 +32,31 @@ func (app *Config) routes() http.Handler {
 	mux.Get("/check-online", app.testServer)
 	mux.Post("/result", app.taskResult)
 	mux.Post("/bulk-create", app.insertTask)
-	mux.Post("/bulk-delete", app.deleteTask)
-	mux.Post("/bulk-upsert", app.upsertTask)
-	mux.Post("/bulk-touch", app.touchTask)
-	mux.Post("/validate", app.validateTask)
+	//mux.Post("/bulk-delete", app.deleteTask)
+	//mux.Post("/bulk-upsert", app.upsertTask)
+	//mux.Post("/bulk-touch", app.touchTask)
+	//mux.Post("/validate", app.validateTask)
 	mux.Post("/clear_data", app.clearRequestFromServer)
-	mux.Post("/bulk-read", app.readTask)
-	mux.Post("/single-create", app.singleInsertTask)
-	mux.Post("/single-delete", app.singleDeleteTask)
-	mux.Post("/single-upsert", app.singleUpsertTask)
-	mux.Post("/single-read", app.singleReadTask)
-	mux.Post("/single-touch", app.singleTouchTask)
-	mux.Post("/single-replace", app.singleReplaceTask)
-	mux.Post("/run-template-query", app.runQueryTask)
-	mux.Post("/retry-exceptions", app.RetryExceptionTask)
-	mux.Post("/sub-doc-bulk-insert", app.SubDocInsertTask)
-	mux.Post("/sub-doc-bulk-upsert", app.SubDocUpsertTask)
-	mux.Post("/sub-doc-bulk-delete", app.SubDocDeleteTask)
-	mux.Post("/sub-doc-bulk-read", app.SubDocReadTask)
-	mux.Post("/sub-doc-bulk-replace", app.SubDocReplaceTask)
-	mux.Post("/single-sub-doc-insert", app.SingleSubDocInsert)
-	mux.Post("/single-sub-doc-upsert", app.SingleSubDocUpsert)
-	mux.Post("/single-sub-doc-replace", app.SingleSubDocReplace)
-	mux.Post("/single-sub-doc-delete", app.SingleSubDocDelete)
-	mux.Post("/single-sub-doc-read", app.SingleSubDocRead)
-	mux.Post("/single-doc-validate", app.SingleDocValidate)
+	//mux.Post("/bulk-read", app.readTask)
+	//mux.Post("/single-create", app.singleInsertTask)
+	//mux.Post("/single-delete", app.singleDeleteTask)
+	//mux.Post("/single-upsert", app.singleUpsertTask)
+	//mux.Post("/single-read", app.singleReadTask)
+	//mux.Post("/single-touch", app.singleTouchTask)
+	//mux.Post("/single-replace", app.singleReplaceTask)
+	//mux.Post("/run-template-query", app.runQueryTask)
+	//mux.Post("/retry-exceptions", app.RetryExceptionTask)
+	//mux.Post("/sub-doc-bulk-insert", app.SubDocInsertTask)
+	//mux.Post("/sub-doc-bulk-upsert", app.SubDocUpsertTask)
+	//mux.Post("/sub-doc-bulk-delete", app.SubDocDeleteTask)
+	//mux.Post("/sub-doc-bulk-read", app.SubDocReadTask)
+	//mux.Post("/sub-doc-bulk-replace", app.SubDocReplaceTask)
+	//mux.Post("/single-sub-doc-insert", app.SingleSubDocInsert)
+	//mux.Post("/single-sub-doc-upsert", app.SingleSubDocUpsert)
+	//mux.Post("/single-sub-doc-replace", app.SingleSubDocReplace)
+	//mux.Post("/single-sub-doc-delete", app.SingleSubDocDelete)
+	//mux.Post("/single-sub-doc-read", app.SingleSubDocRead)
+	//mux.Post("/single-doc-validate", app.SingleDocValidate)
 	mux.Post("/warmup-bucket", app.WarmUpBucket)
 
 	return mux
diff --git a/go.mod b/go.mod
index 967d6f1..81cff3b 100644
--- a/go.mod
+++ b/go.mod
@@ -15,4 +15,5 @@ require (
 	github.com/couchbase/gocbcore/v10 v10.2.1 // indirect
 	github.com/golang/snappy v0.0.4 // indirect
 	github.com/google/uuid v1.3.0 // indirect
+	github.com/shettyh/threadpool v0.0.0-20200323115144-b99fd8aaa945 // indirect
 )
diff --git a/go.sum b/go.sum
index 5b2f9cd..b64c15f 100644
--- a/go.sum
+++ b/go.sum
@@ -19,6 +19,8 @@ github.com/jaswdr/faker v1.16.0 h1:5ZjusQbqIZwJnUymPirNKJI1yFCuozdSR9oeYPgD5Uk=
 github.com/jaswdr/faker v1.16.0/go.mod h1:x7ZlyB1AZqwqKZgyQlnqEG8FDptmHlncA5u2zY/yi6w=
 github.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=
 github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=
+github.com/shettyh/threadpool v0.0.0-20200323115144-b99fd8aaa945 h1:u3vPe14JsH9ON3V1XDpTRzIxzBkyvQLC0gJilH1qGgc=
+github.com/shettyh/threadpool v0.0.0-20200323115144-b99fd8aaa945/go.mod h1:7zHtxRb7VmhasKcyiz/BTliHYG9pYIFr1Z7PpHkCT2A=
 github.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=
 github.com/stretchr/objx v0.4.0/go.mod h1:YvHI0jy2hoMjB+UWwv71VJQ9isScKT/TqJzVSSt89Yw=
 github.com/stretchr/objx v0.5.0 h1:1zr/of2m5FGMsad5YfcqgdqdWrIhu+EBEJRhR1U7z/c=
diff --git a/internal/cb_sdk/cluster.go b/internal/cb_sdk/cluster.go
index 5f7f70a..39b112d 100644
--- a/internal/cb_sdk/cluster.go
+++ b/internal/cb_sdk/cluster.go
@@ -3,13 +3,21 @@ package cb_sdk
 import (
 	"fmt"
 	"github.com/couchbase/gocb/v2"
-	"github.com/couchbaselabs/sirius/internal/task_errors"
+	"github.com/couchbaselabs/sirius/internal/err_sirius"
 	"log"
 	"time"
 )
 
 const WaitUnityReadyTime = 10
 const WaitUntilReadyTimeRetries = 5
+const (
+	ConnectTimeout      = "connectTimeout"
+	KVTimeout           = "kvTimeout"
+	KVDurableTimeout    = "kvDurableTimeout"
+	CompressionDisabled = "compressionDisabled"
+	CompressionMinSize  = "compressionMinSize"
+	CompressionMaxSize  = "compressionMinSize"
+)
 
 type TimeoutsConfig struct {
 	ConnectTimeout   int `json:"connectTimeout,omitempty" doc:"true"`
@@ -26,22 +34,22 @@ type CompressionConfig struct {
 }
 
 type ClusterConfig struct {
-	Username          string            `json:"username" doc:"true"`
-	Password          string            `json:"password" doc:"true"`
-	ConnectionString  string            `json:"connectionString" doc:"true"`
 	CompressionConfig CompressionConfig `json:"compressionConfig,omitempty" doc:"true"`
 	TimeoutsConfig    TimeoutsConfig    `json:"timeoutsConfig,omitempty" doc:"true"`
+	ConnectionString  string            `json:"connectionString,omitempty"`
+	Username          string            `json:"username,omitempty"`
+	Password          string            `json:"password,omitempty"`
 }
 
-func ValidateClusterConfig(c *ClusterConfig) error {
+func ValidateClusterConfig(connStr, username, password string, c *ClusterConfig) error {
 	if c == nil {
-		return task_errors.ErrParsingClusterConfig
+		c = &ClusterConfig{}
 	}
-	if c.ConnectionString == "" {
-		return task_errors.ErrInvalidConnectionString
+	if connStr == "" {
+		return err_sirius.InvalidConnectionString
 	}
-	if c.Username == "" || c.Password == "" {
-		return fmt.Errorf("connection string : %s | %w", c.ConnectionString, task_errors.ErrCredentialMissing)
+	if username == "" || password == "" {
+		return fmt.Errorf("connection string : %s | %w", connStr, err_sirius.CredentialMissing)
 	}
 	return nil
 }
diff --git a/internal/cb_sdk/cluster_collection_manager_test.go b/internal/cb_sdk/cluster_collection_manager_test.go
index 7230be7..3a68e58 100644
--- a/internal/cb_sdk/cluster_collection_manager_test.go
+++ b/internal/cb_sdk/cluster_collection_manager_test.go
@@ -15,20 +15,18 @@ import (
 
 func TestConfigConnectionManager(t *testing.T) {
 	cConfig := &ClusterConfig{
-		Username:          "Administrator",
-		Password:          "password",
-		ConnectionString:  "couchbases://172.23.100.12",
 		CompressionConfig: CompressionConfig{},
 		TimeoutsConfig:    TimeoutsConfig{},
 	}
 
 	cmObj := ConfigConnectionManager()
 
-	if _, err := cmObj.GetCluster(cConfig); err != nil {
+	if _, err := cmObj.GetCluster("couchbase://172.23.100.12", "Administrator", "password", cConfig); err != nil {
 		log.Println(err)
 	}
 
-	c, err := cmObj.GetCollection(cConfig, "lol", "_default", "_default")
+	c, err := cmObj.GetCollection("couchbase://172.23.100.12", "Administrator", "password", cConfig, "default",
+		"_default", "_default")
 	if err != nil {
 		log.Println(err.Error())
 		KVError := &gocb.KeyValueError{}
@@ -57,13 +55,11 @@ func TestConfigConnectionManager(t *testing.T) {
 			Template: temp,
 		}
 		gen := &docgenerator.Generator{
-			KeySize:   25,
-			DocType:   "json",
-			KeyPrefix: "",
-			KeySuffix: "",
-			Template:  template.InitialiseTemplate("person"),
+			KeySize:  25,
+			DocType:  "json",
+			Template: template.InitialiseTemplate("person"),
 		}
-		for i := int64(0); i < int64(10000); i++ {
+		for i := int64(0); i < int64(10); i++ {
 			key := i + cm1.Seed
 			docId := gen.BuildKey(key)
 			fake := faker.NewWithSeed(rand.NewSource(int64(key)))
@@ -75,7 +71,7 @@ func TestConfigConnectionManager(t *testing.T) {
 				t.Error(e)
 			}
 		}
-		for i := int64(0); i < int64(10000); i++ {
+		for i := int64(0); i < int64(10); i++ {
 			docId := gen.BuildKey(i + cm1.Seed)
 			r, e := c.Collection.Get(docId, nil)
 			if e != nil {
diff --git a/internal/cb_sdk/cluster_connection_manager.go b/internal/cb_sdk/cluster_connection_manager.go
index 51db04f..6ab3da3 100644
--- a/internal/cb_sdk/cluster_connection_manager.go
+++ b/internal/cb_sdk/cluster_connection_manager.go
@@ -1,10 +1,8 @@
 package cb_sdk
 
 import (
-	"errors"
-	"fmt"
 	"github.com/couchbase/gocb/v2"
-	"github.com/couchbaselabs/sirius/internal/task_errors"
+	"github.com/couchbaselabs/sirius/internal/err_sirius"
 	"strings"
 	"sync"
 	"time"
@@ -34,6 +32,19 @@ func addKVPoolSize(connStr string) string {
 	return connStr
 }
 
+// Disconnect disconnect a particular Clusters
+func (cm *ConnectionManager) Disconnect(connstr string) error {
+	clusterIdentifier, err := GetClusterIdentifier(connstr)
+	if err != nil {
+		return err
+	}
+	clusterObj, ok := cm.Clusters[clusterIdentifier]
+	if ok {
+		return clusterObj.Cluster.Close(nil)
+	}
+	return nil
+}
+
 // DisconnectAll disconnect all the Clusters used in a tasks.Request
 func (cm *ConnectionManager) DisconnectAll() {
 	defer cm.lock.Unlock()
@@ -70,37 +81,34 @@ func GetClusterIdentifier(connStr string) (string, error) {
 	} else if strings.Contains(connStr, "couchbase://") {
 		return getClusterIdentifierHelper(connStr, "couchbase://"), nil
 	} else {
-		return "", task_errors.ErrInvalidConnectionString
+		return "", err_sirius.InvalidConnectionString
 	}
 }
 
 // getClusterObject returns ClusterObject if cluster is already setup.
 // If not, then set up a ClusterObject using ClusterConfig.
-func (cm *ConnectionManager) getClusterObject(clusterConfig *ClusterConfig) (*ClusterObject, error) {
-
-	if clusterConfig == nil {
-		return nil, fmt.Errorf("unable to parse clusterConfig | %w", errors.New("clusterConfig is nil"))
-	}
+func (cm *ConnectionManager) getClusterObject(connStr, username, password string,
+	clusterConfig *ClusterConfig) (*ClusterObject, error) {
 
-	clusterIdentifier, err := GetClusterIdentifier(clusterConfig.ConnectionString)
+	clusterIdentifier, err := GetClusterIdentifier(connStr)
 	if err != nil {
 		return nil, err
 	}
 
-	clusterConfig.ConnectionString = addKVPoolSize(clusterConfig.ConnectionString)
+	connStr = addKVPoolSize(connStr)
 
 	_, ok := cm.Clusters[clusterIdentifier]
 	if !ok {
-		if err := ValidateClusterConfig(clusterConfig); err != nil {
+		if err := ValidateClusterConfig(connStr, username, password, clusterConfig); err != nil {
 			return nil, err
 		}
-		cluster, err := gocb.Connect(clusterConfig.ConnectionString, gocb.ClusterOptions{
+		cluster, err := gocb.Connect(connStr, gocb.ClusterOptions{
 			Authenticator: gocb.PasswordAuthenticator{
-				Username: clusterConfig.Username,
-				Password: clusterConfig.Password,
+				Username: username,
+				Password: password,
 			},
-			Username: clusterConfig.Username,
-			Password: clusterConfig.Password,
+			Username: username,
+			Password: password,
 			TimeoutsConfig: gocb.TimeoutsConfig{
 				ConnectTimeout:   time.Duration(clusterConfig.TimeoutsConfig.ConnectTimeout) * time.Second,
 				KVTimeout:        time.Duration(clusterConfig.TimeoutsConfig.KVTimeout) * time.Second,
@@ -136,12 +144,12 @@ func (cm *ConnectionManager) getClusterObject(clusterConfig *ClusterConfig) (*Cl
 }
 
 // GetCollection return a *gocb.Collection which represents a single Collection.
-func (cm *ConnectionManager) GetCollection(clusterConfig *ClusterConfig, bucketName, scopeName,
-	collectionName string) (*CollectionObject,
+func (cm *ConnectionManager) GetCollection(connStr, username, password string, clusterConfig *ClusterConfig,
+	bucketName, scopeName, collectionName string) (*CollectionObject,
 	error) {
 	defer cm.lock.Unlock()
 	cm.lock.Lock()
-	cObj, err1 := cm.getClusterObject(clusterConfig)
+	cObj, err1 := cm.getClusterObject(connStr, username, password, clusterConfig)
 	if err1 != nil {
 		return nil, err1
 	}
@@ -161,11 +169,11 @@ func (cm *ConnectionManager) GetCollection(clusterConfig *ClusterConfig, bucketN
 }
 
 // GetScope return a *gocb.Scope which represents  a single scope within a bucket.
-func (cm *ConnectionManager) GetScope(clusterConfig *ClusterConfig, bucketName, scopeName string) (*gocb.Scope,
-	error) {
+func (cm *ConnectionManager) GetScope(connStr, username, password string, clusterConfig *ClusterConfig, bucketName,
+	scopeName string) (*gocb.Scope, error) {
 	defer cm.lock.Unlock()
 	cm.lock.Lock()
-	cObj, err1 := cm.getClusterObject(clusterConfig)
+	cObj, err1 := cm.getClusterObject(connStr, username, password, clusterConfig)
 	if err1 != nil {
 		return nil, err1
 	}
@@ -181,11 +189,12 @@ func (cm *ConnectionManager) GetScope(clusterConfig *ClusterConfig, bucketName,
 }
 
 // GetBucket return a *gocb.Bucket which represents a single bucket within a Cluster.
-func (cm *ConnectionManager) GetBucket(clusterConfig *ClusterConfig, bucketName string) (*gocb.Bucket,
+func (cm *ConnectionManager) GetBucket(connStr, username, password string, clusterConfig *ClusterConfig,
+	bucketName string) (*gocb.Bucket,
 	error) {
 	defer cm.lock.Unlock()
 	cm.lock.Lock()
-	cObj, err1 := cm.getClusterObject(clusterConfig)
+	cObj, err1 := cm.getClusterObject(connStr, username, password, clusterConfig)
 	if err1 != nil {
 		return nil, err1
 	}
@@ -197,11 +206,11 @@ func (cm *ConnectionManager) GetBucket(clusterConfig *ClusterConfig, bucketName
 }
 
 // GetCluster return a *gocb.Cluster which represents connection to a specific Couchbase Cluster.
-func (cm *ConnectionManager) GetCluster(clusterConfig *ClusterConfig) (*gocb.Cluster,
+func (cm *ConnectionManager) GetCluster(connStr, username, password string, clusterConfig *ClusterConfig) (*gocb.Cluster,
 	error) {
 	defer cm.lock.Unlock()
 	cm.lock.Lock()
-	cObj, err1 := cm.getClusterObject(clusterConfig)
+	cObj, err1 := cm.getClusterObject(connStr, username, password, clusterConfig)
 	if err1 != nil {
 		return nil, err1
 	}
diff --git a/internal/cb_sdk/helper.go b/internal/cb_sdk/helper.go
index 5ba6d85..d6bc4a5 100644
--- a/internal/cb_sdk/helper.go
+++ b/internal/cb_sdk/helper.go
@@ -2,7 +2,7 @@ package cb_sdk
 
 import (
 	"github.com/couchbase/gocb/v2"
-	"github.com/couchbaselabs/sirius/internal/task_errors"
+	"github.com/couchbaselabs/sirius/internal/err_sirius"
 )
 
 const (
@@ -56,7 +56,7 @@ type InsertOptions struct {
 // ConfigInsertOptions configures and validate the InsertOptions
 func ConfigInsertOptions(i *InsertOptions) error {
 	if i == nil {
-		return task_errors.ErrParsingInsertOptions
+		return err_sirius.ParsingInsertOptions
 	}
 	if i.Timeout == 0 {
 		i.Timeout = 10
@@ -70,7 +70,7 @@ type TouchOptions struct {
 
 func ConfigTouchOptions(i *TouchOptions) error {
 	if i == nil {
-		return task_errors.ErrParsingTouchOptions
+		return err_sirius.ParsingTouchOptions
 	}
 	if i.Timeout == 0 {
 		i.Timeout = 10
@@ -89,7 +89,7 @@ type RemoveOptions struct {
 
 func ConfigRemoveOptions(r *RemoveOptions) error {
 	if r == nil {
-		return task_errors.ErrParsingRemoveOptions
+		return err_sirius.ParsingRemoveOptions
 	}
 	if r.Timeout == 0 {
 		r.Timeout = 10
@@ -108,7 +108,7 @@ type ReplaceOptions struct {
 
 func ConfigReplaceOptions(r *ReplaceOptions) error {
 	if r == nil {
-		return task_errors.ErrParsingReplaceOptions
+		return err_sirius.ParsingReplaceOptions
 	}
 	if r.Timeout == 0 {
 		r.Timeout = 10
@@ -125,7 +125,7 @@ type QueryOperationConfig struct {
 
 func ConfigQueryOperationConfig(s *QueryOperationConfig) error {
 	if s == nil {
-		return task_errors.ErrParsingQueryConfig
+		return err_sirius.ParsingQueryConfig
 	}
 
 	if s.Duration == 0 || s.Duration > MaxQueryRuntime {
@@ -140,7 +140,7 @@ type GetSpecOptions struct {
 
 func ConfigGetSpecOptions(g *GetSpecOptions) error {
 	if g == nil {
-		return task_errors.ErrParsingGetSpecOptions
+		return err_sirius.ParsingGetSpecOptions
 	}
 	return nil
 }
@@ -151,7 +151,7 @@ type LookupInOptions struct {
 
 func ConfigLookupInOptions(l *LookupInOptions) error {
 	if l == nil {
-		return task_errors.ErrParsingLookupInOptions
+		return err_sirius.ParsingLookupInOptions
 	}
 	return nil
 }
@@ -163,7 +163,7 @@ type InsertSpecOptions struct {
 
 func ConfigInsertSpecOptions(i *InsertSpecOptions) error {
 	if i == nil {
-		return task_errors.ErrParsingInsertSpecOptions
+		return err_sirius.ParsingInsertSpecOptions
 	}
 	return nil
 }
@@ -174,7 +174,7 @@ type RemoveSpecOptions struct {
 
 func ConfigRemoveSpecOptions(r *RemoveSpecOptions) error {
 	if r == nil {
-		return task_errors.ErrParsingRemoveSpecOptions
+		return err_sirius.ParsingRemoveSpecOptions
 	}
 	return nil
 }
@@ -185,7 +185,7 @@ type ReplaceSpecOptions struct {
 
 func ConfigReplaceSpecOptions(r *ReplaceSpecOptions) error {
 	if r == nil {
-		return task_errors.ErrParsingReplaceSpecOptions
+		return err_sirius.ParsingReplaceSpecOptions
 	}
 	return nil
 }
@@ -203,7 +203,7 @@ type MutateInOptions struct {
 
 func ConfigMutateInOptions(m *MutateInOptions) error {
 	if m == nil {
-		return task_errors.ErrParsingMutateInOptions
+		return err_sirius.ParsingMutateInOptions
 	}
 	return nil
 }
@@ -214,3 +214,7 @@ func GetStoreSemantic(storeSemantic int) gocb.StoreSemantics {
 	}
 	return gocb.StoreSemantics(storeSemantic)
 }
+
+func FillClusterConfig(clusteConfig *ClusterConfig, options map[string]any) {
+
+}
diff --git a/internal/db/couchbase.go b/internal/db/couchbase.go
new file mode 100644
index 0000000..c30aa07
--- /dev/null
+++ b/internal/db/couchbase.go
@@ -0,0 +1,646 @@
+package db
+
+import (
+	"errors"
+	"github.com/couchbase/gocb/v2"
+	"github.com/couchbaselabs/sirius/internal/cb_sdk"
+	"github.com/couchbaselabs/sirius/internal/template"
+	"time"
+)
+
+type CouchbaseOperationResult struct {
+	key    string
+	value  interface{}
+	error  error
+	status bool
+	cas    uint64
+}
+
+func newCouchbaseOperationResult(key string, value interface{}, err error, status bool, cas uint64) *CouchbaseOperationResult {
+	return &CouchbaseOperationResult{
+		key:    key,
+		value:  value,
+		error:  err,
+		status: status,
+		cas:    cas,
+	}
+}
+
+func (c *CouchbaseOperationResult) Key() string {
+	return c.key
+}
+
+func (c *CouchbaseOperationResult) Value() interface{} {
+	return c.value
+}
+
+func (c *CouchbaseOperationResult) GetStatus() bool {
+	return c.status
+}
+
+func (c *CouchbaseOperationResult) GetError() error {
+	return c.error
+}
+
+func (c *CouchbaseOperationResult) GetExtra() map[string]any {
+	return map[string]any{
+		"cas": c.cas,
+	}
+}
+
+type Couchbase struct {
+	connectionManager *cb_sdk.ConnectionManager
+}
+
+func NewCouchbaseConnectionManager() *Couchbase {
+	return &Couchbase{
+		connectionManager: cb_sdk.ConfigConnectionManager(),
+	}
+}
+
+func (c *Couchbase) Connect(connStr, username, password string, extra Extras) error {
+	if err := validateStrings(connStr, username, password); err != nil {
+		return err
+	}
+	clusterConfig := &cb_sdk.ClusterConfig{
+		CompressionConfig: cb_sdk.CompressionConfig{
+			Disabled: extra.CompressionDisabled,
+			MinSize:  extra.CompressionMinSize,
+			MinRatio: extra.CompressionMinRatio,
+		},
+		TimeoutsConfig: cb_sdk.TimeoutsConfig{
+			ConnectTimeout:   extra.ConnectionTimeout,
+			KVTimeout:        extra.KVTimeout,
+			KVDurableTimeout: extra.KVDurableTimeout,
+		},
+	}
+
+	if _, err := c.connectionManager.GetCluster(connStr, username, password, clusterConfig); err != nil {
+		return err
+	}
+
+	return nil
+}
+
+func (c *Couchbase) Create(connStr, username, password, key string, doc interface{}, extra Extras) OperationResult {
+	if err := validateStrings(connStr, username, password); err != nil {
+		return newCouchbaseOperationResult(key, doc, err, false, 0)
+	}
+
+	bucketName := extra.Bucket
+	scope := extra.Scope
+	collection := extra.Collection
+
+	if err := validateStrings(bucketName); err != nil {
+		return newCouchbaseOperationResult(key, doc, errors.New("bucket is missing"), false, 0)
+	}
+
+	if scope == "" {
+		scope = cb_sdk.DefaultScope
+	}
+
+	if collection == "" {
+		collection = cb_sdk.DefaultCollection
+	}
+
+	collectionObj, err1 := c.connectionManager.GetCollection(connStr, username, password, nil, bucketName,
+		scope, collection)
+	if err1 != nil {
+		return newCouchbaseOperationResult(key, doc, err1, false, 0)
+	}
+	result, err2 := collectionObj.Collection.Insert(key, doc, &gocb.InsertOptions{
+		Expiry:          time.Duration(extra.Expiry) * time.Second,
+		PersistTo:       extra.PersistTo,
+		ReplicateTo:     extra.ReplicateTo,
+		DurabilityLevel: cb_sdk.GetDurability(extra.Durability),
+		Timeout:         time.Duration(extra.OperationTimeout) * time.Second,
+	})
+	if err2 != nil {
+		return newCouchbaseOperationResult(key, doc, err2, false, 0)
+	}
+	return newCouchbaseOperationResult(key, doc, nil, true, uint64(result.Cas()))
+}
+
+func (c *Couchbase) Read(connStr, username, password, key string, extra Extras) OperationResult {
+	if err := validateStrings(connStr, username, password); err != nil {
+		return newCouchbaseOperationResult(key, nil, err, false, 0)
+	}
+
+	bucketName := extra.Bucket
+	scope := extra.Scope
+	collection := extra.Collection
+
+	if err := validateStrings(bucketName); err != nil {
+		return newCouchbaseOperationResult(key, nil, errors.New("bucket is missing"), false, 0)
+	}
+
+	if scope == "" {
+		scope = cb_sdk.DefaultScope
+	}
+
+	if collection == "" {
+		collection = cb_sdk.DefaultCollection
+	}
+
+	collectionObj, err1 := c.connectionManager.GetCollection(connStr, username, password, nil, bucketName,
+		scope, collection)
+	if err1 != nil {
+		return newCouchbaseOperationResult(key, nil, err1, false, 0)
+	}
+	result, err2 := collectionObj.Collection.Get(key, &gocb.GetOptions{
+		Timeout: time.Duration(extra.OperationTimeout) * time.Second,
+	})
+	if err2 != nil {
+		return newCouchbaseOperationResult(key, nil, err2, false, 0)
+	}
+	resultFromHost := make(map[string]any)
+	if err := result.Content(&resultFromHost); err != nil {
+		return newCouchbaseOperationResult(key, nil, err, false, 0)
+	}
+	return newCouchbaseOperationResult(key, resultFromHost, nil, true, uint64(result.Cas()))
+}
+
+func (c *Couchbase) Update(connStr, username, password, key string, doc interface{}, extra Extras) OperationResult {
+	if err := validateStrings(connStr, username, password); err != nil {
+		return newCouchbaseOperationResult(key, doc, err, false, 0)
+	}
+
+	bucketName := extra.Bucket
+	scope := extra.Scope
+	collection := extra.Collection
+
+	if err := validateStrings(bucketName); err != nil {
+		return newCouchbaseOperationResult(key, doc, errors.New("bucket is missing"), false, 0)
+	}
+
+	if scope == "" {
+		scope = cb_sdk.DefaultScope
+	}
+
+	if collection == "" {
+		collection = cb_sdk.DefaultCollection
+	}
+
+	collectionObj, err1 := c.connectionManager.GetCollection(connStr, username, password, nil, bucketName,
+		scope, collection)
+	if err1 != nil {
+		return newCouchbaseOperationResult(key, doc, err1, false, 0)
+	}
+	result, err2 := collectionObj.Collection.Upsert(key, doc, &gocb.UpsertOptions{
+		Expiry:          time.Duration(extra.Expiry) * time.Second,
+		PersistTo:       extra.PersistTo,
+		ReplicateTo:     extra.ReplicateTo,
+		DurabilityLevel: cb_sdk.GetDurability(extra.Durability),
+		Timeout:         time.Duration(extra.OperationTimeout) * time.Second,
+	})
+	if err2 != nil {
+		return newCouchbaseOperationResult(key, doc, err2, false, 0)
+	}
+	return newCouchbaseOperationResult(key, doc, nil, true, uint64(result.Cas()))
+}
+
+func (c *Couchbase) Delete(connStr, username, password, key string, extra Extras) OperationResult {
+	if err := validateStrings(connStr, username, password); err != nil {
+		return newCouchbaseOperationResult(key, nil, err, false, 0)
+	}
+
+	bucketName := extra.Bucket
+	scope := extra.Scope
+	collection := extra.Collection
+
+	if err := validateStrings(bucketName); err != nil {
+		return newCouchbaseOperationResult(key, nil, errors.New("bucket is missing"), false, 0)
+	}
+
+	if scope == "" {
+		scope = cb_sdk.DefaultScope
+	}
+
+	if collection == "" {
+		collection = cb_sdk.DefaultCollection
+	}
+
+	collectionObj, err1 := c.connectionManager.GetCollection(connStr, username, password, nil, bucketName,
+		scope, collection)
+	if err1 != nil {
+		return newCouchbaseOperationResult(key, nil, err1, false, 0)
+	}
+	result, err2 := collectionObj.Collection.Remove(key, &gocb.RemoveOptions{
+		Cas:             gocb.Cas(extra.Cas),
+		PersistTo:       extra.PersistTo,
+		ReplicateTo:     extra.ReplicateTo,
+		DurabilityLevel: cb_sdk.GetDurability(extra.Durability),
+		Timeout:         time.Duration(extra.OperationTimeout) * time.Second,
+	})
+	if err2 != nil {
+		return newCouchbaseOperationResult(key, nil, err2, false, 0)
+	}
+	return newCouchbaseOperationResult(key, nil, nil, true, uint64(result.Cas()))
+}
+
+func (c *Couchbase) Touch(connStr, username, password, key string, extra Extras) OperationResult {
+	if err := validateStrings(connStr, username, password); err != nil {
+		return newCouchbaseOperationResult(key, nil, err, false, 0)
+	}
+
+	bucketName := extra.Bucket
+	scope := extra.Scope
+	collection := extra.Collection
+
+	if err := validateStrings(bucketName); err != nil {
+		return newCouchbaseOperationResult(key, nil, errors.New("bucket is missing"), false, 0)
+	}
+
+	if scope == "" {
+		scope = cb_sdk.DefaultScope
+	}
+
+	if collection == "" {
+		collection = cb_sdk.DefaultCollection
+	}
+
+	collectionObj, err1 := c.connectionManager.GetCollection(connStr, username, password, nil, bucketName,
+		scope, collection)
+	if err1 != nil {
+		return newCouchbaseOperationResult(key, nil, err1, false, 0)
+	}
+	result, err2 := collectionObj.Collection.Touch(key, time.Duration(extra.Expiry)*time.Second, &gocb.TouchOptions{
+		Timeout: time.Duration(extra.OperationTimeout) * time.Second,
+	})
+	if err2 != nil {
+		return newCouchbaseOperationResult(key, nil, err2, false, 0)
+	}
+	return newCouchbaseOperationResult(key, nil, nil, true, uint64(result.Cas()))
+}
+
+func (c *Couchbase) InsertSubDoc(connStr, username, password, key string, subPathValues map[string]any,
+	extra Extras) OperationResult {
+	if err := validateStrings(connStr, username, password); err != nil {
+		return newCouchbaseOperationResult(key, nil, err, false, 0)
+	}
+
+	bucketName := extra.Bucket
+	scope := extra.Scope
+	collection := extra.Collection
+
+	if err := validateStrings(bucketName); err != nil {
+		return newCouchbaseOperationResult(key, nil, errors.New("bucket is missing"), false, 0)
+	}
+
+	if scope == "" {
+		scope = cb_sdk.DefaultScope
+	}
+
+	if collection == "" {
+		collection = cb_sdk.DefaultCollection
+	}
+
+	collectionObject, err1 := c.connectionManager.GetCollection(connStr, username, password, nil, bucketName,
+		scope, collection)
+	if err1 != nil {
+		return newCouchbaseOperationResult(key, nil, err1, false, 0)
+	}
+
+	var iOps []gocb.MutateInSpec
+	for path, value := range subPathValues {
+		iOps = append(iOps, gocb.InsertSpec(path, value, &gocb.InsertSpecOptions{
+			CreatePath: extra.CreatePath,
+			IsXattr:    extra.IsXattr,
+		}))
+	}
+
+	if extra.IsXattr {
+		iOps = append(iOps, gocb.IncrementSpec(template.MutatedPath,
+			int64(template.MutateFieldIncrement), &gocb.CounterSpecOptions{
+				CreatePath: true,
+				IsXattr:    false,
+			}))
+	}
+
+	result, err2 := collectionObject.Collection.MutateIn(key, iOps, &gocb.MutateInOptions{
+		Expiry:          time.Duration(extra.Expiry) * time.Second,
+		PersistTo:       extra.PersistTo,
+		ReplicateTo:     extra.ReplicateTo,
+		DurabilityLevel: cb_sdk.GetDurability(extra.Durability),
+		StoreSemantic:   cb_sdk.GetStoreSemantic(extra.StoreSemantic),
+		Timeout:         time.Duration(extra.OperationTimeout) * time.Second,
+		PreserveExpiry:  extra.PreserveExpiry,
+	})
+	if err2 != nil {
+		return newCouchbaseOperationResult(key, nil, err2, false, 0)
+	}
+	return newCouchbaseOperationResult(key, subPathValues, nil, true, uint64(result.Cas()))
+}
+
+func (c *Couchbase) UpsertSubDoc(connStr, username, password, key string, subPathValues map[string]any,
+	extra Extras) OperationResult {
+	if err := validateStrings(connStr, username, password); err != nil {
+		return newCouchbaseOperationResult(key, nil, err, false, 0)
+	}
+
+	bucketName := extra.Bucket
+	scope := extra.Scope
+	collection := extra.Collection
+
+	if err := validateStrings(bucketName); err != nil {
+		return newCouchbaseOperationResult(key, nil, errors.New("bucket is missing"), false, 0)
+	}
+
+	if scope == "" {
+		scope = cb_sdk.DefaultScope
+	}
+
+	if collection == "" {
+		collection = cb_sdk.DefaultCollection
+	}
+
+	collectionObject, err1 := c.connectionManager.GetCollection(connStr, username, password, nil, bucketName,
+		scope, collection)
+	if err1 != nil {
+		return newCouchbaseOperationResult(key, nil, err1, false, 0)
+	}
+
+	var iOps []gocb.MutateInSpec
+	for path, value := range subPathValues {
+		iOps = append(iOps, gocb.UpsertSpec(path, value, &gocb.UpsertSpecOptions{
+			CreatePath: extra.CreatePath,
+			IsXattr:    extra.IsXattr,
+		}))
+	}
+
+	if extra.IsXattr {
+		iOps = append(iOps, gocb.IncrementSpec(template.MutatedPath,
+			int64(template.MutateFieldIncrement), &gocb.CounterSpecOptions{
+				CreatePath: true,
+				IsXattr:    false,
+			}))
+	}
+
+	result, err2 := collectionObject.Collection.MutateIn(key, iOps, &gocb.MutateInOptions{
+		Expiry:          time.Duration(extra.Expiry) * time.Second,
+		PersistTo:       extra.PersistTo,
+		ReplicateTo:     extra.ReplicateTo,
+		DurabilityLevel: cb_sdk.GetDurability(extra.Durability),
+		StoreSemantic:   cb_sdk.GetStoreSemantic(extra.StoreSemantic),
+		Timeout:         time.Duration(extra.OperationTimeout) * time.Second,
+		PreserveExpiry:  extra.PreserveExpiry,
+	})
+	if err2 != nil {
+		return newCouchbaseOperationResult(key, nil, err2, false, 0)
+	}
+	return newCouchbaseOperationResult(key, subPathValues, nil, true, uint64(result.Cas()))
+}
+
+func (c *Couchbase) ReplaceSubDoc(connStr, username, password, key string, subPathValues map[string]any,
+	extra Extras) OperationResult {
+	if err := validateStrings(connStr, username, password); err != nil {
+		return newCouchbaseOperationResult(key, nil, err, false, 0)
+	}
+
+	bucketName := extra.Bucket
+	scope := extra.Scope
+	collection := extra.Collection
+
+	if err := validateStrings(bucketName); err != nil {
+		return newCouchbaseOperationResult(key, nil, errors.New("bucket is missing"), false, 0)
+	}
+
+	if scope == "" {
+		scope = cb_sdk.DefaultScope
+	}
+
+	if collection == "" {
+		collection = cb_sdk.DefaultCollection
+	}
+
+	collectionObject, err1 := c.connectionManager.GetCollection(connStr, username, password, nil, bucketName,
+		scope, collection)
+	if err1 != nil {
+		return newCouchbaseOperationResult(key, nil, err1, false, 0)
+	}
+
+	var iOps []gocb.MutateInSpec
+	for path, value := range subPathValues {
+		iOps = append(iOps, gocb.ReplaceSpec(path, value, &gocb.ReplaceSpecOptions{
+			IsXattr: extra.IsXattr,
+		}))
+	}
+
+	if extra.IsXattr {
+		iOps = append(iOps, gocb.IncrementSpec(template.MutatedPath,
+			int64(template.MutateFieldIncrement), &gocb.CounterSpecOptions{
+				CreatePath: true,
+				IsXattr:    false,
+			}))
+	}
+
+	result, err2 := collectionObject.Collection.MutateIn(key, iOps, &gocb.MutateInOptions{
+		Expiry:          time.Duration(extra.Expiry) * time.Second,
+		PersistTo:       extra.PersistTo,
+		ReplicateTo:     extra.ReplicateTo,
+		DurabilityLevel: cb_sdk.GetDurability(extra.Durability),
+		StoreSemantic:   cb_sdk.GetStoreSemantic(extra.StoreSemantic),
+		Timeout:         time.Duration(extra.OperationTimeout) * time.Second,
+		PreserveExpiry:  extra.PreserveExpiry,
+	})
+	if err2 != nil {
+		return newCouchbaseOperationResult(key, nil, err2, false, 0)
+	}
+	return newCouchbaseOperationResult(key, subPathValues, nil, true, uint64(result.Cas()))
+}
+
+func (c *Couchbase) ReadSubDoc(connStr, username, password, key string, subPathValues map[string]any,
+	extra Extras) OperationResult {
+	if err := validateStrings(connStr, username, password); err != nil {
+		return newCouchbaseOperationResult(key, nil, err, false, 0)
+	}
+
+	bucketName := extra.Bucket
+	scope := extra.Scope
+	collection := extra.Collection
+
+	if err := validateStrings(bucketName); err != nil {
+		return newCouchbaseOperationResult(key, nil, errors.New("bucket is missing"), false, 0)
+	}
+
+	if scope == "" {
+		scope = cb_sdk.DefaultScope
+	}
+
+	if collection == "" {
+		collection = cb_sdk.DefaultCollection
+	}
+
+	collectionObject, err1 := c.connectionManager.GetCollection(connStr, username, password, nil, bucketName,
+		scope, collection)
+	if err1 != nil {
+		return newCouchbaseOperationResult(key, nil, err1, false, 0)
+	}
+
+	var iOps []gocb.LookupInSpec
+
+	for path, _ := range subPathValues {
+		iOps = append(iOps, gocb.GetSpec(path, &gocb.GetSpecOptions{
+			IsXattr: extra.IsXattr,
+		}))
+	}
+
+	result, err2 := collectionObject.Collection.LookupIn(key, iOps, &gocb.LookupInOptions{
+		Timeout: time.Duration(extra.OperationTimeout) * time.Second,
+	})
+
+	if err2 != nil {
+		return newCouchbaseOperationResult(key, nil, err2, false, 0)
+	}
+
+	return newCouchbaseOperationResult(key, subPathValues, nil, true, uint64(result.Cas()))
+}
+
+func (c *Couchbase) DeleteSubDoc(connStr, username, password, key string, subPathValues map[string]any,
+	extra Extras) OperationResult {
+	if err := validateStrings(connStr, username, password); err != nil {
+		return newCouchbaseOperationResult(key, nil, err, false, 0)
+	}
+
+	bucketName := extra.Bucket
+	scope := extra.Scope
+	collection := extra.Collection
+
+	if err := validateStrings(bucketName); err != nil {
+		return newCouchbaseOperationResult(key, nil, errors.New("bucket is missing"), false, 0)
+	}
+
+	if scope == "" {
+		scope = cb_sdk.DefaultScope
+	}
+
+	if collection == "" {
+		collection = cb_sdk.DefaultCollection
+	}
+
+	collectionObject, err1 := c.connectionManager.GetCollection(connStr, username, password, nil, bucketName,
+		scope, collection)
+	if err1 != nil {
+		return newCouchbaseOperationResult(key, nil, err1, false, 0)
+	}
+
+	var iOps []gocb.MutateInSpec
+	for path, _ := range subPathValues {
+		iOps = append(iOps, gocb.RemoveSpec(path, &gocb.RemoveSpecOptions{
+			IsXattr: extra.IsXattr,
+		}))
+	}
+
+	if extra.IsXattr {
+		iOps = append(iOps, gocb.IncrementSpec(template.MutatedPath,
+			int64(template.MutateFieldIncrement), &gocb.CounterSpecOptions{
+				CreatePath: true,
+				IsXattr:    false,
+			}))
+	}
+
+	result, err2 := collectionObject.Collection.MutateIn(key, iOps, &gocb.MutateInOptions{
+		Expiry:          time.Duration(extra.Expiry) * time.Second,
+		PersistTo:       extra.PersistTo,
+		ReplicateTo:     extra.ReplicateTo,
+		DurabilityLevel: cb_sdk.GetDurability(extra.Durability),
+		StoreSemantic:   cb_sdk.GetStoreSemantic(extra.StoreSemantic),
+		Timeout:         time.Duration(extra.OperationTimeout) * time.Second,
+		PreserveExpiry:  extra.PreserveExpiry,
+	})
+	if err2 != nil {
+		return newCouchbaseOperationResult(key, nil, err2, false, 0)
+	}
+	return newCouchbaseOperationResult(key, subPathValues, nil, true, uint64(result.Cas()))
+}
+
+func (c *Couchbase) IncrementMutationCount(connStr, username, password, key string, subPathValues map[string]any,
+	extra Extras) OperationResult {
+	if err := validateStrings(connStr, username, password); err != nil {
+		return newCouchbaseOperationResult(key, nil, err, false, 0)
+	}
+
+	bucketName := extra.Bucket
+	scope := extra.Scope
+	collection := extra.Collection
+
+	if err := validateStrings(bucketName); err != nil {
+		return newCouchbaseOperationResult(key, nil, errors.New("bucket is missing"), false, 0)
+	}
+
+	if scope == "" {
+		scope = cb_sdk.DefaultScope
+	}
+
+	if collection == "" {
+		collection = cb_sdk.DefaultCollection
+	}
+
+	collectionObject, err1 := c.connectionManager.GetCollection(connStr, username, password, nil, bucketName,
+		scope, collection)
+	if err1 != nil {
+		return newCouchbaseOperationResult(key, nil, err1, false, 0)
+	}
+
+	var iOps []gocb.MutateInSpec
+
+	for path, _ := range subPathValues {
+		iOps = append(iOps, gocb.IncrementSpec(path, 1, &gocb.CounterSpecOptions{
+			CreatePath: extra.CreatePath,
+			IsXattr:    extra.IsXattr,
+		}))
+	}
+
+	if extra.IsXattr {
+		iOps = append(iOps, gocb.IncrementSpec(template.MutatedPath,
+			int64(template.MutateFieldIncrement), &gocb.CounterSpecOptions{
+				CreatePath: true,
+				IsXattr:    false,
+			}))
+	}
+
+	result, err2 := collectionObject.Collection.MutateIn(key, iOps, &gocb.MutateInOptions{
+		Expiry:          time.Duration(extra.Expiry) * time.Second,
+		PersistTo:       extra.PersistTo,
+		ReplicateTo:     extra.ReplicateTo,
+		DurabilityLevel: cb_sdk.GetDurability(extra.Durability),
+		StoreSemantic:   cb_sdk.GetStoreSemantic(extra.StoreSemantic),
+		Timeout:         time.Duration(extra.OperationTimeout) * time.Second,
+		PreserveExpiry:  extra.PreserveExpiry,
+	})
+
+	if err2 != nil {
+		return newCouchbaseOperationResult(key, nil, err2, false, 0)
+	}
+	return newCouchbaseOperationResult(key, subPathValues, nil, true, uint64(result.Cas()))
+}
+
+func (c *Couchbase) Warmup(connStr, username, password string, extra Extras) error {
+	if err := validateStrings(connStr, username, password); err != nil {
+		return err
+	}
+
+	bucketName := extra.Bucket
+	scope := extra.Scope
+	collection := extra.Collection
+
+	if err := validateStrings(bucketName); err != nil {
+		return errors.New("bucket is missing")
+	}
+
+	if scope == "" {
+		scope = cb_sdk.DefaultScope
+	}
+
+	if collection == "" {
+		collection = cb_sdk.DefaultCollection
+	}
+
+	_, err1 := c.connectionManager.GetCollection(connStr, username, password, nil, bucketName,
+		scope, collection)
+	return err1
+}
+
+func (c *Couchbase) Close(connStr string) error {
+	return c.connectionManager.Disconnect(connStr)
+}
diff --git a/internal/db/couchbase_test.go b/internal/db/couchbase_test.go
new file mode 100644
index 0000000..c006bcd
--- /dev/null
+++ b/internal/db/couchbase_test.go
@@ -0,0 +1,66 @@
+package db
+
+import (
+	"github.com/couchbaselabs/sirius/internal/docgenerator"
+	"github.com/couchbaselabs/sirius/internal/meta_data"
+	"github.com/couchbaselabs/sirius/internal/template"
+	"github.com/jaswdr/faker"
+	"log"
+	"math/rand"
+	"testing"
+)
+
+func TestNewCouchbaseConnectionManager(t *testing.T) {
+	db, err := ConfigDatabase("couchbase")
+	if err != nil {
+		t.Fatal(err)
+	}
+	connStr := "couchbase://172.23.100.12"
+	username := "Administrator"
+	password := "password"
+	if err := db.Connect(connStr, username, password, Extras{}); err != nil {
+		t.Error(err)
+	}
+
+	m := meta_data.NewMetaData()
+	cm1 := m.GetCollectionMetadata("x")
+
+	temp := template.InitialiseTemplate("person")
+	g := docgenerator.Generator{
+		Template: temp,
+	}
+	gen := &docgenerator.Generator{
+		KeySize:  25,
+		DocType:  "json",
+		Template: template.InitialiseTemplate("person"),
+	}
+	for i := int64(0); i < int64(10); i++ {
+		key := i + cm1.Seed
+		docId := gen.BuildKey(key)
+		fake := faker.NewWithSeed(rand.NewSource(int64(key)))
+		doc, _ := g.Template.GenerateDocument(&fake, 100)
+		//log.Println(docId, doc)
+		x := db.Update(connStr, username, password, docId, doc, Extras{
+			Bucket: "default",
+		})
+		if x.GetError() != nil {
+			t.Error(x.GetError())
+		} else {
+			log.Println(x)
+		}
+
+	}
+
+	for i := int64(0); i < int64(10); i++ {
+		docId := gen.BuildKey(i + cm1.Seed)
+		x := db.Read(connStr, username, password, docId, Extras{
+			Bucket: "default",
+		})
+		if x.GetError() != nil {
+			t.Error(x.GetError())
+		} else {
+			log.Println(x)
+		}
+	}
+	log.Println(db.Close(connStr))
+}
diff --git a/internal/db/db.go b/internal/db/db.go
new file mode 100644
index 0000000..aea5a47
--- /dev/null
+++ b/internal/db/db.go
@@ -0,0 +1,70 @@
+package db
+
+import (
+	"github.com/couchbaselabs/sirius/internal/err_sirius"
+	"sync"
+)
+
+const (
+	COUCHBASE_DB = "couchbase"
+	MONGO_DB     = "Mongo"
+)
+
+type OperationResult interface {
+	Key() string
+	Value() interface{}
+	GetStatus() bool
+	GetError() error
+	GetExtra() map[string]any
+}
+
+type Database interface {
+	Connect(connStr, username, password string, extra Extras) error
+	Create(connStr, username, password, key string, doc interface{}, extra Extras) OperationResult
+	Read(connStr, username, password, key string, extra Extras) OperationResult
+	Update(connStr, username, password, key string, doc interface{}, extra Extras) OperationResult
+	Delete(connStr, username, password, key string, extra Extras) OperationResult
+	Touch(connStr, username, password, key string, extra Extras) OperationResult
+	InsertSubDoc(connStr, username, password, key string, subPathValues map[string]any,
+		extra Extras) OperationResult
+	UpsertSubDoc(connStr, username, password, key string, subPathValues map[string]any,
+		extra Extras) OperationResult
+	ReplaceSubDoc(connStr, username, password, key string, subPathValues map[string]any,
+		extra Extras) OperationResult
+	ReadSubDoc(connStr, username, password, key string, subPathValues map[string]any,
+		extra Extras) OperationResult
+	DeleteSubDoc(connStr, username, password, key string, subPathValues map[string]any, extra Extras) OperationResult
+	IncrementMutationCount(connStr, username, password, key string, subPathValues map[string]any,
+		extra Extras) OperationResult
+	Warmup(connStr, username, password string, extra Extras) error
+	Close(connStr string) error
+}
+
+var couchbase *Couchbase
+var mongo *Mongo
+var lock = &sync.Mutex{}
+
+func ConfigDatabase(dbType string) (Database, error) {
+	switch dbType {
+	case MONGO_DB:
+		if mongo == nil {
+			lock.Lock()
+			defer lock.Unlock()
+			if mongo == nil {
+				mongo = &Mongo{}
+			}
+		}
+		return mongo, nil
+	case COUCHBASE_DB:
+		if couchbase == nil {
+			lock.Lock()
+			defer lock.Unlock()
+			if couchbase == nil {
+				couchbase = NewCouchbaseConnectionManager()
+			}
+		}
+		return couchbase, nil
+	default:
+		return nil, err_sirius.InvalidDatabase
+	}
+}
diff --git a/internal/db/helper.go b/internal/db/helper.go
new file mode 100644
index 0000000..771d474
--- /dev/null
+++ b/internal/db/helper.go
@@ -0,0 +1,73 @@
+package db
+
+import (
+	"fmt"
+	"github.com/couchbaselabs/sirius/internal/err_sirius"
+)
+
+//const (
+//	couchbaseClusterConfig = "couchbaseClusterConfig"
+//	couchbaseInsertOptions = "couchbaseInsertOptions"
+//)
+//
+//type RegisterDatabaseHelper struct {
+//}
+//
+//func (r RegisterDatabaseHelper) Helper() map[string]any {
+//	return map[string]any{
+//		couchbaseClusterConfig:           &cb_sdk.ClusterConfig{},
+//		"couchbaseCompressionConfig":     &cb_sdk.CompressionConfig{},
+//		"couchbaseClusterTimeoutsConfig": &cb_sdk.TimeoutsConfig{},
+//		"operationConfig":                &bulk_loading.OperationConfig{},
+//		couchbaseInsertOptions:           &cb_sdk.InsertOptions{},
+//		"couchbaseRemoveOptions":         &cb_sdk.RemoveOptions{},
+//		"couchbaseReplaceOption":         &cb_sdk.ReplaceOptions{},
+//		"couchbaseTouchOptions":          &cb_sdk.TouchOptions{},
+//		"couchbaseSingleOperationConfig": &key_based_loading_cb.SingleOperationConfig{},
+//		"bulkError":                      &task_result.FailedDocument{},
+//		"retriedError":                   &task_result.FailedDocument{},
+//		"singleResult":                   &task_result.SingleOperationResult{},
+//		"couchbaseQueryOperationConfig":  &cb_sdk.QueryOperationConfig{},
+//		"exceptions":                     &bulk_loading.Exceptions{},
+//		"couchbaseMutateInOptions":       &cb_sdk.MutateInOptions{},
+//		"couchbaseInsertSpecOptions":     &cb_sdk.InsertSpecOptions{},
+//		"couchbaseRemoveSpecOptions":     &cb_sdk.RemoveSpecOptions{},
+//		"couchbaseGetSpecOptions":        &cb_sdk.GetSpecOptions{},
+//		"couchbaseLookupInOptions":       &cb_sdk.LookupInOptions{},
+//		"couchbaseReplaceSpecOptions":    &cb_sdk.ReplaceSpecOptions{},
+//		"singleSubDocOperationConfig":    &key_based_loading_cb.SingleSubDocOperationConfig{},
+//		"sdkTimings":                     &task_result.SDKTiming{},
+//	}
+//
+//}
+
+type Extras struct {
+	CompressionDisabled bool    `json:"compressionDisabled,omitempty" doc:"true"`
+	CompressionMinSize  uint32  `json:"compressionMinSize,omitempty" doc:"true"`
+	CompressionMinRatio float64 `json:"compressionMinRatio,omitempty" doc:"true"`
+	ConnectionTimeout   int     `json:"connectionTimeout,omitempty" doc:"true"`
+	KVTimeout           int     `json:"KVTimeout,omitempty" doc:"true"`
+	KVDurableTimeout    int     `json:"KVDurableTimeout,omitempty" doc:"true"`
+	Bucket              string  `json:"bucket,omitempty" doc:"true"`
+	Scope               string  `json:"scope,omitempty" doc:"true"`
+	Collection          string  `json:"collection,omitempty" doc:"true"`
+	Expiry              int     `json:"expiry,omitempty" doc:"true"`
+	PersistTo           uint    `json:"persistTo,omitempty" doc:"true"`
+	ReplicateTo         uint    `json:"replicateTo,omitempty" doc:"true"`
+	Durability          string  `json:"durability,omitempty" doc:"true"`
+	OperationTimeout    int     `json:"operationTimeout,omitempty" doc:"true"`
+	Cas                 uint64  `json:"cas,omitempty" doc:"true"`
+	IsXattr             bool    `json:"isXattr,omitempty" doc:"true"`
+	StoreSemantic       int     `json:"storeSemantic,omitempty" doc:"true"`
+	PreserveExpiry      bool    `json:"preserveExpiry,omitempty" doc:"true"`
+	CreatePath          bool    `json:"createPath,omitempty" doc:"true"`
+}
+
+func validateStrings(values ...string) error {
+	for _, v := range values {
+		if v == "" {
+			return fmt.Errorf("%s %w", v, err_sirius.InvalidInfo)
+		}
+	}
+	return nil
+}
diff --git a/internal/db/mongo.go b/internal/db/mongo.go
new file mode 100644
index 0000000..62703e8
--- /dev/null
+++ b/internal/db/mongo.go
@@ -0,0 +1,80 @@
+package db
+
+type Mongo struct {
+}
+
+func (m *Mongo) Connect(connStr, username, password string, extra Extras) error {
+	//TODO implement me
+	panic("implement me")
+}
+
+func (m *Mongo) Create(connStr, username, password, key string, doc interface{}, extra Extras) OperationResult {
+	//TODO implement me
+	panic("implement me")
+}
+
+func (m *Mongo) Read(connStr, username, password, key string, extra Extras) OperationResult {
+	//TODO implement me
+	panic("implement me")
+}
+
+func (m *Mongo) Update(connStr, username, password, key string, doc interface{}, extra Extras) OperationResult {
+	//TODO implement me
+	panic("implement me")
+}
+
+func (m *Mongo) Delete(connStr, username, password, key string, extra Extras) OperationResult {
+	//TODO implement me
+	panic("implement me")
+}
+
+func (m *Mongo) Touch(connStr, username, password, key string, extra Extras) OperationResult {
+	//TODO implement me
+	panic("implement me")
+}
+
+func (m *Mongo) InsertSubDoc(connStr, username, password, key string, subPathValues map[string]any,
+	extra Extras) OperationResult {
+	//TODO implement me
+	panic("implement me")
+}
+
+func (m *Mongo) UpsertSubDoc(connStr, username, password, key string, subPathValues map[string]any,
+	extra Extras) OperationResult {
+	//TODO implement me
+	panic("implement me")
+}
+
+func (m *Mongo) ReplaceSubDoc(connStr, username, password, key string, subPathValues map[string]any,
+	extra Extras) OperationResult {
+	//TODO implement me
+	panic("implement me")
+}
+
+func (m *Mongo) ReadSubDoc(connStr, username, password, key string, subPathValues map[string]any,
+	extra Extras) OperationResult {
+	//TODO implement me
+	panic("implement me")
+}
+
+func (m *Mongo) DeleteSubDoc(connStr, username, password, key string, subPathValues map[string]any,
+	extra Extras) OperationResult {
+	//TODO implement me
+	panic("implement me")
+}
+
+func (m *Mongo) IncrementMutationCount(connStr, username, password, key string, subPathValues map[string]any,
+	extra Extras) OperationResult {
+	//TODO implement me
+	panic("implement me")
+}
+
+func (m *Mongo) Warmup(connStr, username, password string, extra Extras) error {
+	//TODO implement me
+	panic("implement me")
+}
+
+func (m *Mongo) Close(connStr string) error {
+	//TODO implement me
+	panic("implement me")
+}
diff --git a/internal/docgenerator/docgenerator.go b/internal/docgenerator/docgenerator.go
index 569d7ff..6ba2f9a 100644
--- a/internal/docgenerator/docgenerator.go
+++ b/internal/docgenerator/docgenerator.go
@@ -9,36 +9,27 @@ import (
 type DocumentType string
 
 const (
-	JsonDocument     string = "json"
-	BinaryDocument   string = "binary"
-	DefaultKeyPrefix        = ""
-	DefaultKeySuffix        = ""
-	DefaultTemplate         = "person"
-	DefaultDocSize   int    = 128
-	DefaultKeySize   int    = 250
+	JsonDocument   string = "json"
+	BinaryDocument string = "binary"
+	DefaultDocSize int    = 128
+	DefaultKeySize int    = 250
 )
 
 // Generator helps to sirius_documentation random document for inserting and updating random
 // as per the doc loading task requirement.
 type Generator struct {
-	KeySize   int               `json:"keySize"`
-	DocSize   int               `json:"docSize"`
-	DocType   string            `json:"docType"`
-	KeyPrefix string            `json:"keyPrefix"`
-	KeySuffix string            `json:"keySuffix"`
-	Template  template.Template `json:"template"`
+	KeySize  int               `json:"keySize"`
+	DocSize  int               `json:"docSize"`
+	DocType  string            `json:"docType"`
+	Template template.Template `json:"template"`
 }
 
-func ConfigGenerator(keySize, docSize int, doctype, keyPrefix, keySuffix string,
-	template template.Template) *Generator {
+func ConfigGenerator(keySize, docSize int, template template.Template) *Generator {
 
 	return &Generator{
-		KeySize:   keySize,
-		DocSize:   docSize,
-		DocType:   doctype,
-		KeyPrefix: keyPrefix,
-		KeySuffix: keySuffix,
-		Template:  template,
+		KeySize:  keySize,
+		DocSize:  docSize,
+		Template: template,
 	}
 }
 
@@ -54,13 +45,13 @@ func ConfigQueryGenerator(template template.Template) *QueryGenerator {
 
 // BuildKey returns the formatted key with unique identifier.
 func (g *Generator) BuildKey(key int64) string {
-	tempKey := fmt.Sprintf("%s%d%s", g.KeyPrefix, key, g.KeySuffix)
+	tempKey := fmt.Sprintf("%d", key)
 	if g.KeySize >= 0 && len(tempKey) < g.KeySize {
 		tempKey += strings.Repeat("a", g.KeySize-len(tempKey))
 	}
 	return tempKey
 }
 
-func Reset(keySize, docSize int, docType, keyPrefix, keySuffix, templateName string) *Generator {
-	return ConfigGenerator(keySize, docSize, docType, keyPrefix, keySuffix, template.InitialiseTemplate(templateName))
+func Reset(keySize, docSize int, templateName string) *Generator {
+	return ConfigGenerator(keySize, docSize, template.InitialiseTemplate(templateName))
 }
diff --git a/internal/err_sirius/errors.go b/internal/err_sirius/errors.go
new file mode 100644
index 0000000..27beda2
--- /dev/null
+++ b/internal/err_sirius/errors.go
@@ -0,0 +1,39 @@
+package err_sirius
+
+type Error string
+
+func (e Error) Error() string { return string(e) }
+
+const (
+	RequestIsNil                       = Error("internal request.Request struct is nil")
+	TaskStateIsNil                     = Error("task State is nil")
+	InvalidInfo                        = Error("information(connection string,username,password) is nil")
+	ParsingClusterConfig               = Error("unable to parse clusterConfig")
+	CredentialMissing                  = Error("missing credentials for authentication")
+	InvalidConnectionString            = Error("empty or invalid connection string")
+	InvalidUsername                    = Error("empty or invalid username")
+	ParsingSingleOperationConfig       = Error("unable to parse SingleOperationConfig")
+	ParsingQueryConfig                 = Error("unable to parse QueryOperationConfig")
+	ParsingOperatingConfig             = Error("unable to parse operationConfig")
+	MalformedOperationRange            = Error("operation start to end range is malformed")
+	ParsingInsertOptions               = Error("unable to parse InsertOptions")
+	ParsingTouchOptions                = Error("unable to parse TouchOptions")
+	ParsingRemoveOptions               = Error("unable to parse RemoveOptions")
+	ParsingReplaceOptions              = Error("unable to parse ReplaceOptions")
+	ParsingSubDocOperatingConfig       = Error("unable to parse SubDocOperatingConfig")
+	ParsingGetSpecOptions              = Error("unable to parse GetSpecOptions")
+	ParsingLookupInOptions             = Error("unable to parse LookupInOptions")
+	ParsingInsertSpecOptions           = Error("unable to parse InsertSpecOptions")
+	ParsingRemoveSpecOptions           = Error("unable to parse RemoveSpecOptions")
+	ParsingReplaceSpecOptions          = Error("unable to parse ReplaceSpecOptions")
+	ParsingSingleSubDocOperationConfig = Error("unable to parse SingleSubDocOperationConfig")
+	ParsingMutateInOptions             = Error("unable to parse MutateInOptions")
+	NilOperationConfig                 = Error("no operation found for the given offset")
+	TaskingRetryFailed                 = Error("task is still in pending state before retrying")
+	TaskInPendingState                 = Error("current task is still in progress")
+	InvalidDatabase                    = Error("invalid database in sirius")
+	BucketIsMisssing                   = Error("bucket value is missing in extra parameters for couchbase cluster")
+	CollectionIsMissing                = Error("collection is in extra parameters for mongo cluster")
+	InternalErrorSetOperationType      = Error(
+		"operation type not set in the handler of route before configuring the generic loading task")
+)
diff --git a/internal/server_requests/server_requests.go b/internal/server_requests/server_requests.go
index 56811ed..4b8f7f5 100644
--- a/internal/server_requests/server_requests.go
+++ b/internal/server_requests/server_requests.go
@@ -214,7 +214,6 @@ func (sr *ServerRequests) ClearIdentifierAndRequest(identifier string) error {
 		req, ok := r.(*tasks.Request)
 		if ok && req != nil {
 			req.Cancel()
-			req.DisconnectConnectionManager()
 			req.ClearAllTask()
 		}
 		req = nil
diff --git a/internal/sirius_documentation/generate.go b/internal/sirius_documentation/generate.go
index 459080e..d5c15e0 100644
--- a/internal/sirius_documentation/generate.go
+++ b/internal/sirius_documentation/generate.go
@@ -2,6 +2,7 @@ package sirius_documentation
 
 import (
 	"fmt"
+	"github.com/couchbaselabs/sirius/internal/db"
 	"github.com/couchbaselabs/sirius/internal/task_result"
 	"github.com/couchbaselabs/sirius/internal/tasks"
 	"github.com/couchbaselabs/sirius/internal/tasks/util_sirius"
@@ -42,13 +43,13 @@ configuration that is also available on a per-task basis:
 
 	for _, k := range keys {
 		entry := tk[k]
-		x, ok := entry.config.(tasks.Task)
+		x, ok := entry.Config.(tasks.Task)
 		if !ok {
 			continue
 		}
 		val := reflect.ValueOf(x)
 		output += fmt.Sprintf("#### %s\n\n", k)
-		output += fmt.Sprintf(" REST : %s\n\n", entry.httpMethod)
+		output += fmt.Sprintf(" REST : %s\n\n", entry.HttpMethod)
 		output += fmt.Sprintf("Description : %s\n\n", x.Describe())
 
 		if !val.IsValid() {
@@ -101,10 +102,95 @@ configuration that is also available on a per-task basis:
 			// Close last table column
 			output += " |\n"
 		}
+		s := tasks.DatabaseInformation{}
+		hVal := reflect.ValueOf(&s)
+		for i := 0; i < hVal.Elem().NumField(); i++ {
+			f := hVal.Elem().Type().Field(i)
+			if _, ok := f.Tag.Lookup("json"); !ok {
+				continue
+			}
+
+			// doc
+			if tagContent, ok := f.Tag.Lookup("doc"); !ok {
+				continue
+			} else {
+				if tagContent == "false" {
+					continue
+				}
+			}
+
+			// Name
+			output += "| `" + f.Name + "` "
+
+			// Type
+			n := f.Type.Name()
+			k := f.Type.Kind().String()
+			if n == k {
+				output += "| `" + n + "` "
+			} else {
+				output += "| `" + k + "` "
+			}
+
+			for _, tagName := range []string{"json"} {
+				if tagContents, ok := f.Tag.Lookup(tagName); ok {
+					output += "| `" + tagName + ":" + tagContents + "` "
+					continue
+				}
+				output += "| "
+			}
+			// Close last table column
+			output += " |\n"
+		}
 		output += "\n---\n"
 	}
 
-	output += "**Description of JSON tags used in routes**.\n\n"
+	// END - API ends here
+
+	output += "**Description of Extra Parameters**.\n\n"
+	s := db.Extras{}
+	hVal := reflect.ValueOf(&s)
+	output += "| Name | Type | JSON Tag |\n"
+	output += "| ---- | ---- | -------- |\n"
+	for i := 0; i < hVal.Elem().NumField(); i++ {
+		f := hVal.Elem().Type().Field(i)
+		if _, ok := f.Tag.Lookup("json"); !ok {
+			continue
+		}
+
+		// doc
+		if tagContent, ok := f.Tag.Lookup("doc"); !ok {
+			continue
+		} else {
+			if tagContent == "false" {
+				continue
+			}
+		}
+
+		// Name
+		output += "| `" + f.Name + "` "
+
+		// Type
+		n := f.Type.Name()
+		k := f.Type.Kind().String()
+		if n == k {
+			output += "| `" + n + "` "
+		} else {
+			output += "| `" + k + "` "
+		}
+
+		for _, tagName := range []string{"json"} {
+			if tagContents, ok := f.Tag.Lookup(tagName); ok {
+				output += "| `" + tagName + ":" + tagContents + "` "
+				continue
+			}
+			output += "| "
+		}
+		// Close last table column
+		output += " |\n"
+	}
+
+	output += "\n---\n"
+	// End - Extra Parameter ends here
 
 	tt := t.HelperStruct()
 	tagKeys := make([]string, 0, len(t.HelperStruct()))
diff --git a/internal/sirius_documentation/register.go b/internal/sirius_documentation/register.go
index e06d2db..d94edb1 100644
--- a/internal/sirius_documentation/register.go
+++ b/internal/sirius_documentation/register.go
@@ -1,18 +1,15 @@
 package sirius_documentation
 
 import (
-	"github.com/couchbaselabs/sirius/internal/cb_sdk"
 	"github.com/couchbaselabs/sirius/internal/task_result"
-	"github.com/couchbaselabs/sirius/internal/tasks/bulk_loading_cb"
-	"github.com/couchbaselabs/sirius/internal/tasks/bulk_query_cb"
-	"github.com/couchbaselabs/sirius/internal/tasks/key_based_loading_cb"
-	"github.com/couchbaselabs/sirius/internal/tasks/util_cb"
+	"github.com/couchbaselabs/sirius/internal/tasks/bulk_loading"
+	"github.com/couchbaselabs/sirius/internal/tasks/db_util"
 	"github.com/couchbaselabs/sirius/internal/tasks/util_sirius"
 )
 
 type TaskRegister struct {
-	httpMethod string
-	config     interface{}
+	HttpMethod string
+	Config     interface{}
 }
 
 type Register struct {
@@ -20,61 +17,45 @@ type Register struct {
 
 func (r *Register) RegisteredTasks() map[string]TaskRegister {
 	return map[string]TaskRegister{
-		"/bulk-create":            {"POST", &bulk_loading_cb.InsertTask{}},
-		"/bulk-delete":            {"POST", &bulk_loading_cb.DeleteTask{}},
-		"/bulk-upsert":            {"POST", &bulk_loading_cb.UpsertTask{}},
-		"/bulk-touch":             {"POST", &bulk_loading_cb.TouchTask{}},
-		"/validate":               {"POST", &bulk_loading_cb.ValidateTask{}},
-		"/result":                 {"POST", &util_sirius.TaskResult{}},
-		"/clear_data":             {"POST", &util_sirius.ClearTask{}},
-		"/bulk-read":              {"POST", &bulk_loading_cb.ReadTask{}},
-		"/single-create":          {"POST", &key_based_loading_cb.SingleInsertTask{}},
-		"/single-delete":          {"POST", &key_based_loading_cb.SingleDeleteTask{}},
-		"/single-upsert":          {"POST", &key_based_loading_cb.SingleUpsertTask{}},
-		"/single-read":            {"POST", &key_based_loading_cb.SingleReadTask{}},
-		"/single-touch":           {"POST", &key_based_loading_cb.SingleTouchTask{}},
-		"/single-replace":         {"POST", &key_based_loading_cb.SingleReplaceTask{}},
-		"/run-template-query":     {"POST", &bulk_query_cb.QueryTask{}},
-		"/retry-exceptions":       {"POST", &bulk_loading_cb.RetryExceptions{}},
-		"/sub-doc-bulk-insert":    {"POST", &bulk_loading_cb.SubDocInsert{}},
-		"/sub-doc-bulk-upsert":    {"POST", &bulk_loading_cb.SubDocUpsert{}},
-		"/sub-doc-bulk-delete":    {"POST", &bulk_loading_cb.SubDocDelete{}},
-		"/sub-doc-bulk-read":      {"POST", &bulk_loading_cb.SubDocRead{}},
-		"/sub-doc-bulk-replace":   {"POST", &bulk_loading_cb.SubDocReplace{}},
-		"/single-sub-doc-insert":  {"POST", &key_based_loading_cb.SingleSubDocInsert{}},
-		"/single-sub-doc-upsert":  {"POST", &key_based_loading_cb.SingleSubDocUpsert{}},
-		"/single-sub-doc-replace": {"POST", &key_based_loading_cb.SingleSubDocReplace{}},
-		"/single-sub-doc-delete":  {"POST", &key_based_loading_cb.SingleSubDocDelete{}},
-		"/single-sub-doc-read":    {"POST", &key_based_loading_cb.SingleSubDocRead{}},
-		"/single-doc-validate":    {"POST", &key_based_loading_cb.SingleValidate{}},
-		"/warmup-bucket":          {"POST", &util_cb.BucketWarmUpTask{}},
+		"/bulk-create": {"POST", &bulk_loading.GenericLoadingTask{}},
+		//"/bulk-delete": {"POST", &bulk_loading.DeleteTask{}},
+		//"/bulk-upsert": {"POST", &bulk_loading.UpsertTask{}},
+		//"/bulk-touch":  {"POST", &bulk_loading.TouchTask{}},
+		//"/validate":    {"POST", &bulk_loading.ValidateTask{}},
+		"/result":     {"POST", &util_sirius.TaskResult{}},
+		"/clear_data": {"POST", &util_sirius.ClearTask{}},
+		//"/bulk-read":              {"POST", &bulk_loading.ReadTask{}},
+		//"/single-create":          {"POST", &key_based_loading_cb.SingleInsertTask{}},
+		//"/single-delete":          {"POST", &key_based_loading_cb.SingleDeleteTask{}},
+		//"/single-upsert":          {"POST", &key_based_loading_cb.SingleUpsertTask{}},
+		//"/single-read":            {"POST", &key_based_loading_cb.SingleReadTask{}},
+		//"/single-touch":           {"POST", &key_based_loading_cb.SingleTouchTask{}},
+		//"/single-replace":         {"POST", &key_based_loading_cb.SingleReplaceTask{}},
+		//"/run-template-query":     {"POST", &bulk_query_cb.QueryTask{}},
+		//"/retry-exceptions":       {"POST", &bulk_loading.RetryExceptions{}},
+		//"/sub-doc-bulk-insert":    {"POST", &bulk_loading.SubDocInsert{}},
+		//"/sub-doc-bulk-upsert":    {"POST", &bulk_loading.SubDocUpsert{}},
+		//"/sub-doc-bulk-delete":    {"POST", &bulk_loading.SubDocDelete{}},
+		//"/sub-doc-bulk-read":      {"POST", &bulk_loading.SubDocRead{}},
+		//"/sub-doc-bulk-replace":   {"POST", &bulk_loading.SubDocReplace{}},
+		//"/single-sub-doc-insert":  {"POST", &key_based_loading_cb.SingleSubDocInsert{}},
+		//"/single-sub-doc-upsert":  {"POST", &key_based_loading_cb.SingleSubDocUpsert{}},
+		//"/single-sub-doc-replace": {"POST", &key_based_loading_cb.SingleSubDocReplace{}},
+		//"/single-sub-doc-delete":  {"POST", &key_based_loading_cb.SingleSubDocDelete{}},
+		//"/single-sub-doc-read":    {"POST", &key_based_loading_cb.SingleSubDocRead{}},
+		//"/single-doc-validate":    {"POST", &key_based_loading_cb.SingleValidate{}},
+		"/warmup-bucket": {"POST", &db_util.BucketWarmUpTask{}},
 	}
 }
 
 func (r *Register) HelperStruct() map[string]any {
 	return map[string]any{
-		"clusterConfig":               &cb_sdk.ClusterConfig{},
-		"compressionConfig":           &cb_sdk.CompressionConfig{},
-		"timeoutsConfig":              &cb_sdk.TimeoutsConfig{},
-		"operationConfig":             &bulk_loading_cb.OperationConfig{},
-		"insertOptions":               &cb_sdk.InsertOptions{},
-		"removeOptions":               &cb_sdk.RemoveOptions{},
-		"replaceOption":               &cb_sdk.ReplaceOptions{},
-		"touchOptions":                &cb_sdk.TouchOptions{},
-		"singleOperationConfig":       &key_based_loading_cb.SingleOperationConfig{},
-		"bulkError":                   &task_result.FailedDocument{},
-		"retriedError":                &task_result.FailedDocument{},
-		"singleResult":                &task_result.SingleOperationResult{},
-		"queryOperationConfig":        &cb_sdk.QueryOperationConfig{},
-		"exceptions":                  &bulk_loading_cb.Exceptions{},
-		"mutateInOptions":             &cb_sdk.MutateInOptions{},
-		"insertSpecOptions":           &cb_sdk.InsertSpecOptions{},
-		"removeSpecOptions":           &cb_sdk.RemoveSpecOptions{},
-		"getSpecOptions":              &cb_sdk.GetSpecOptions{},
-		"lookupInOptions":             &cb_sdk.LookupInOptions{},
-		"replaceSpecOptions":          &cb_sdk.ReplaceSpecOptions{},
-		"singleSubDocOperationConfig": &key_based_loading_cb.SingleSubDocOperationConfig{},
-		"sdkTimings":                  &task_result.SDKTiming{},
+		"operationConfig": &bulk_loading.OperationConfig{},
+		"bulkError":       &task_result.FailedDocument{},
+		"retriedError":    &task_result.FailedDocument{},
+		"exceptions":      &bulk_loading.Exceptions{},
+		"sdkTimings":      &task_result.SDKTiming{},
+		"singleResult":    &task_result.SingleOperationResult{},
 	}
 
 }
diff --git a/internal/task_errors/errors.go b/internal/task_errors/errors.go
deleted file mode 100644
index b499d37..0000000
--- a/internal/task_errors/errors.go
+++ /dev/null
@@ -1,30 +0,0 @@
-package task_errors
-
-import "errors"
-
-var (
-	ErrRequestIsNil                       = errors.New("internal request.Request struct is nil")
-	ErrTaskStateIsNil                     = errors.New("task State is nil")
-	ErrParsingClusterConfig               = errors.New("unable to parse clusterConfig")
-	ErrCredentialMissing                  = errors.New("missing credentials for authentication")
-	ErrInvalidConnectionString            = errors.New("empty or invalid connection string")
-	ErrParsingSingleOperationConfig       = errors.New("unable to parse SingleOperationConfig")
-	ErrParsingQueryConfig                 = errors.New("unable to parse QueryOperationConfig")
-	ErrParsingOperatingConfig             = errors.New("unable to parse operationConfig")
-	ErrMalformedOperationRange            = errors.New("operation start to end range is malformed")
-	ErrParsingInsertOptions               = errors.New("unable to parse InsertOptions")
-	ErrParsingTouchOptions                = errors.New("unable to parse TouchOptions")
-	ErrParsingRemoveOptions               = errors.New("unable to parse RemoveOptions")
-	ErrParsingReplaceOptions              = errors.New("unable to parse ReplaceOptions")
-	ErrParsingSubDocOperatingConfig       = errors.New("unable to parse SubDocOperatingConfig")
-	ErrParsingGetSpecOptions              = errors.New("unable to parse GetSpecOptions")
-	ErrParsingLookupInOptions             = errors.New("unable to parse LookupInOptions")
-	ErrParsingInsertSpecOptions           = errors.New("unable to parse InsertSpecOptions")
-	ErrParsingRemoveSpecOptions           = errors.New("unable to parse RemoveSpecOptions")
-	ErrParsingReplaceSpecOptions          = errors.New("unable to parse ReplaceSpecOptions")
-	ErrParsingSingleSubDocOperationConfig = errors.New("unable to parse SingleSubDocOperationConfig")
-	ErrParsingMutateInOptions             = errors.New("unable to parse MutateInOptions")
-	ErrNilOperationConfig                 = errors.New("no operation found for the given offset")
-	ErrTaskingRetryFailed                 = errors.New("task is still in pending state before retrying")
-	ErrTaskInPendingState                 = errors.New("current task is still in progress")
-)
diff --git a/internal/task_state/task_state.go b/internal/task_state/task_state.go
index 918ae67..ed2e89f 100644
--- a/internal/task_state/task_state.go
+++ b/internal/task_state/task_state.go
@@ -25,7 +25,7 @@ type StateHelper struct {
 
 type KeyStates struct {
 	Completed []int64 `json:"completed"`
-	Err       []int64 `json:"err"`
+	Err       []int64 `json:"err_sirius"`
 }
 
 type TaskState struct {
diff --git a/internal/tasks/bulk_loading/bulk_loading_task.go b/internal/tasks/bulk_loading/bulk_loading_task.go
new file mode 100644
index 0000000..157f646
--- /dev/null
+++ b/internal/tasks/bulk_loading/bulk_loading_task.go
@@ -0,0 +1,105 @@
+package bulk_loading
+
+import (
+	"github.com/couchbaselabs/sirius/internal/db"
+	"github.com/couchbaselabs/sirius/internal/docgenerator"
+	"github.com/couchbaselabs/sirius/internal/task_result"
+	"github.com/couchbaselabs/sirius/internal/task_state"
+	"github.com/couchbaselabs/sirius/internal/tasks"
+	"sync"
+)
+
+type BulkTask interface {
+	tasks.Task
+	PostTaskExceptionHandling()
+	MatchResultSeed(resultSeed string) (bool, error)
+	SetException(exceptions Exceptions)
+	GetOperationConfig() (*OperationConfig, *task_state.TaskState)
+	MetaDataIdentifier() string
+}
+
+type loadingTask struct {
+	start           int64
+	end             int64
+	operationConfig *OperationConfig
+	seed            int64
+	operation       string
+	rerun           bool
+	gen             *docgenerator.Generator
+	state           *task_state.TaskState
+	result          *task_result.TaskResult
+	databaseInfo    tasks.DatabaseInformation
+	extra           db.Extras
+	req             *tasks.Request
+	identifier      string
+	wg              *sync.WaitGroup
+}
+
+func newLoadingTask(start, end, seed int64, operationConfig *OperationConfig,
+	operation string, rerun bool, gen *docgenerator.Generator,
+	state *task_state.TaskState, result *task_result.TaskResult, databaseInfo tasks.DatabaseInformation,
+	extra db.Extras, req *tasks.Request, identifier string, wg *sync.WaitGroup) *loadingTask {
+	return &loadingTask{
+		start:           start,
+		end:             end,
+		seed:            seed,
+		operationConfig: operationConfig,
+		operation:       operation,
+		rerun:           rerun,
+		gen:             gen,
+		state:           state,
+		result:          result,
+		databaseInfo:    databaseInfo,
+		extra:           extra,
+		req:             req,
+		identifier:      identifier,
+		wg:              wg,
+	}
+}
+
+func (l *loadingTask) Run() {
+	switch l.operation {
+	case tasks.InsertOperation:
+		{
+			insertDocuments(l.start, l.end, l.seed, l.operationConfig, l.rerun, l.gen, l.state, l.result,
+				l.databaseInfo, l.extra, l.wg)
+		}
+	case tasks.UpsertOperation:
+		{
+			upsertDocuments(l.start, l.end, l.seed, l.operationConfig, l.rerun, l.gen, l.state, l.result,
+				l.databaseInfo, l.extra, l.req, l.identifier, l.wg)
+		}
+	case tasks.DeleteOperation:
+		{
+
+		}
+	case tasks.ReadOperation:
+		{
+
+		}
+	case tasks.TouchOperation:
+		{
+
+		}
+	case tasks.SubDocInsertOperation:
+		{
+
+		}
+	case tasks.SubDocDeleteOperation:
+		{
+
+		}
+	case tasks.SubDocReadOperation:
+		{
+
+		}
+	case tasks.SubDocReplaceOperation:
+		{
+
+		}
+	case tasks.SubDocUpsertOperation:
+		{
+
+		}
+	}
+}
diff --git a/internal/tasks/bulk_loading/generic_loading.go b/internal/tasks/bulk_loading/generic_loading.go
new file mode 100644
index 0000000..2d74179
--- /dev/null
+++ b/internal/tasks/bulk_loading/generic_loading.go
@@ -0,0 +1,379 @@
+package bulk_loading
+
+import (
+	"errors"
+	"fmt"
+	"github.com/couchbaselabs/sirius/internal/db"
+	"github.com/couchbaselabs/sirius/internal/docgenerator"
+	"github.com/couchbaselabs/sirius/internal/err_sirius"
+	"github.com/couchbaselabs/sirius/internal/meta_data"
+	"github.com/couchbaselabs/sirius/internal/task_result"
+	"github.com/couchbaselabs/sirius/internal/task_state"
+	"github.com/couchbaselabs/sirius/internal/tasks"
+	"github.com/couchbaselabs/sirius/internal/template"
+	"golang.org/x/sync/errgroup"
+	"log"
+	"strings"
+	"sync"
+	"time"
+)
+
+type GenericLoadingTask struct {
+	IdentifierToken string `json:"identifierToken" doc:"true"`
+	tasks.DatabaseInformation
+	ResultSeed      int64                         `json:"resultSeed" doc:"false"`
+	TaskPending     bool                          `json:"taskPending" doc:"false"`
+	State           *task_state.TaskState         `json:"State" doc:"false"`
+	MetaData        *meta_data.CollectionMetaData `json:"metaData" doc:"false"`
+	OperationConfig *OperationConfig              `json:"operationConfig" doc:"true"`
+	Operation       string                        `json:"operation" doc:"false"`
+	Result          *task_result.TaskResult       `json:"-" doc:"false"`
+	gen             *docgenerator.Generator       `json:"-" doc:"false"`
+	req             *tasks.Request                `json:"-" doc:"false"`
+	rerun           bool                          `json:"-" doc:"false"`
+	lock            sync.Mutex                    `json:"-" doc:"false"`
+}
+
+func (t *GenericLoadingTask) Describe() string {
+	return " Insert t uploads documents in bulk into a bucket.\n" +
+		"The durability while inserting a document can be set using following values in the 'durability' JSON tag :-\n" +
+		"1. MAJORITY\n" +
+		"2. MAJORITY_AND_PERSIST_TO_ACTIVE\n" +
+		"3. PERSIST_TO_MAJORITY\n"
+}
+
+func (t *GenericLoadingTask) MetaDataIdentifier() string {
+	if t.DBType == db.COUCHBASE_DB {
+		return strings.Join([]string{t.IdentifierToken, t.ConnStr, t.Extra.Bucket, t.Extra.Scope,
+			t.Extra.Collection}, ":")
+	} else if t.DBType == db.MONGO_DB {
+		return strings.Join([]string{t.IdentifierToken, t.ConnStr, t.Extra.Collection}, ":")
+	} else {
+		return strings.Join([]string{t.IdentifierToken, t.ConnStr}, ":")
+	}
+}
+
+func (t *GenericLoadingTask) CheckIfPending() bool {
+	return t.TaskPending
+}
+
+// Config configures  the insert task
+func (t *GenericLoadingTask) Config(req *tasks.Request, reRun bool) (int64, error) {
+	t.TaskPending = true
+	t.req = req
+
+	if t.req == nil {
+		t.TaskPending = false
+		return 0, err_sirius.RequestIsNil
+	}
+
+	if database, err := db.ConfigDatabase(t.DBType); err != nil {
+		return 0, err
+	} else {
+		if err = database.Connect(t.ConnStr, t.Username, t.Password, t.Extra); err != nil {
+			return 0, err
+		}
+	}
+
+	t.lock = sync.Mutex{}
+	t.rerun = reRun
+
+	if t.Operation == "" {
+		return 0, err_sirius.InternalErrorSetOperationType
+	}
+
+	if !reRun {
+		t.ResultSeed = int64(time.Now().UnixNano())
+
+		if err := ConfigureOperationConfig(t.OperationConfig); err != nil {
+			t.TaskPending = false
+			return 0, err
+		}
+
+		if err := configExtraParameters(t.DBType, &t.Extra); err != nil {
+			return 0, err
+		}
+
+		t.MetaData = t.req.MetaData.GetCollectionMetadata(t.MetaDataIdentifier())
+
+		t.req.Lock()
+		if t.OperationConfig.End+t.MetaData.Seed > t.MetaData.SeedEnd {
+			t.req.AddToSeedEnd(t.MetaData, (t.OperationConfig.End+t.MetaData.Seed)-(t.MetaData.SeedEnd))
+		}
+
+		t.State = task_state.ConfigTaskState(t.MetaData.Seed, t.MetaData.SeedEnd, t.ResultSeed)
+		t.req.Unlock()
+
+	} else {
+		if t.State == nil {
+			return t.ResultSeed, err_sirius.TaskStateIsNil
+		}
+		t.State.SetupStoringKeys()
+		_ = task_result.DeleteResultFile(t.ResultSeed)
+		log.Println("retrying :- ", t.Operation, t.IdentifierToken, t.ResultSeed)
+	}
+	return t.ResultSeed, nil
+}
+
+func (t *GenericLoadingTask) TearUp() error {
+	//Use this case to store t's state on disk when required
+	//if err_sirius := t.State.SaveTaskSateOnDisk(); err_sirius != nil {
+	//	log.Println("Error in storing TASK State on DISK")
+	//}
+	t.Result.StopStoringResult()
+	if err := t.Result.SaveResultIntoFile(); err != nil {
+		log.Println("not able to save Result into ", t.ResultSeed, t.Operation)
+	}
+	t.Result = nil
+	t.State.StopStoringState()
+	t.TaskPending = false
+	return t.req.SaveRequestIntoFile()
+}
+
+func (t *GenericLoadingTask) Do() {
+
+	t.Result = task_result.ConfigTaskResult(t.Operation, t.ResultSeed)
+	t.gen = docgenerator.ConfigGenerator(
+		t.OperationConfig.KeySize,
+		t.OperationConfig.DocSize,
+		template.InitialiseTemplate(t.OperationConfig.TemplateName))
+
+	database, err := db.ConfigDatabase(t.DBType)
+	if err != nil {
+		t.Result.ErrorOther = err.Error()
+		t.Result.FailWholeBulkOperation(t.OperationConfig.Start, t.OperationConfig.End,
+			err, t.State, t.gen, t.MetaData.Seed)
+		_ = t.TearUp()
+	}
+	err = database.Warmup(t.ConnStr, t.Username, t.Password, t.Extra)
+	if err != nil {
+		t.Result.ErrorOther = err.Error()
+		t.Result.FailWholeBulkOperation(t.OperationConfig.Start, t.OperationConfig.End,
+			err, t.State, t.gen, t.MetaData.Seed)
+		_ = t.TearUp()
+	}
+
+	loadDocumentsInBatches(t)
+
+	t.Result.Success = t.OperationConfig.End - t.OperationConfig.Start - t.Result.Failure
+
+	_ = t.TearUp()
+}
+
+// loadDocumentsInBatches divides the load into batches and will allocate to one of go routine.
+func loadDocumentsInBatches(task *GenericLoadingTask) {
+
+	// This is stop processing loading operation if user has already cancelled all the tasks from sirius
+	if task.req.ContextClosed() {
+		return
+	}
+
+	numOfBatches := (task.OperationConfig.End - task.OperationConfig.Start) / tasks.BatchSize
+
+	wg := &sync.WaitGroup{}
+
+	for i := int64(0); i < numOfBatches; i++ {
+		batchStart := i * tasks.BatchSize
+		batchEnd := (i + 1) * tasks.BatchSize
+		t := newLoadingTask(batchStart,
+			batchEnd,
+			task.MetaData.Seed,
+			task.OperationConfig,
+			task.Operation,
+			task.rerun,
+			task.gen,
+			task.State,
+			task.Result,
+			task.DatabaseInformation,
+			task.Extra,
+			task.req,
+			task.MetaDataIdentifier(),
+			wg)
+		loadBatch(task, t, batchStart, batchEnd)
+		wg.Add(1)
+	}
+
+	remainingItems := (task.OperationConfig.End - task.OperationConfig.Start) - (numOfBatches * tasks.BatchSize)
+	if remainingItems > 0 {
+		t := newLoadingTask(
+			numOfBatches*tasks.BatchSize,
+			task.OperationConfig.End,
+			task.MetaData.Seed,
+			task.OperationConfig,
+			task.Operation,
+			task.rerun,
+			task.gen,
+			task.State,
+			task.Result,
+			task.DatabaseInformation,
+			task.Extra,
+			task.req,
+			task.MetaDataIdentifier(),
+			wg)
+		loadBatch(task, t, numOfBatches*tasks.BatchSize, task.OperationConfig.End)
+		wg.Add(1)
+	}
+
+	wg.Wait()
+	task.PostTaskExceptionHandling()
+	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+}
+
+// loadBatch will enqueue the batch to thread pool. if the queue is full,
+// it will wait for sometime any thread to pick it up.
+func loadBatch(task *GenericLoadingTask, t *loadingTask, batchStart int64, batchEnd int64) {
+	retryBatchCounter := 10
+	for ; retryBatchCounter > 0; retryBatchCounter-- {
+		if err := tasks.Pool.Execute(t); err == nil {
+			break
+		}
+		time.Sleep(1 * time.Minute)
+	}
+	if retryBatchCounter == 0 {
+		task.Result.FailWholeBulkOperation(batchStart, batchEnd, errors.New("internal error, "+
+			"sirius is overloaded"), task.State, task.gen, task.MetaData.Seed)
+	}
+}
+
+func (t *GenericLoadingTask) PostTaskExceptionHandling() {
+	t.Result.StopStoringResult()
+	t.State.StopStoringState()
+
+	if t.OperationConfig.Exceptions.RetryAttempts <= 0 {
+		return
+	}
+
+	// Get all the errorOffset
+	errorOffsetMaps := t.State.ReturnErrOffset()
+	// Get all the completed offset
+	completedOffsetMaps := t.State.ReturnCompletedOffset()
+
+	// For the offset in ignore exceptions :-> move them from error to completed
+	shiftErrToCompletedOnIgnore(t.OperationConfig.Exceptions.IgnoreExceptions, t.Result, errorOffsetMaps,
+		completedOffsetMaps)
+
+	if t.OperationConfig.Exceptions.RetryAttempts > 0 {
+
+		exceptionList := GetExceptions(t.Result, t.OperationConfig.Exceptions.RetryExceptions)
+
+		// For the retry exceptions :-> move them on success after retrying from err_sirius to completed
+		for _, exception := range exceptionList {
+
+			errorOffsetListMap := make([]map[int64]RetriedResult, 0)
+			for _, failedDocs := range t.Result.BulkError[exception] {
+				m := make(map[int64]RetriedResult)
+				m[failedDocs.Offset] = RetriedResult{}
+				errorOffsetListMap = append(errorOffsetListMap, m)
+			}
+
+			routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
+			dataChannel := make(chan map[int64]RetriedResult, tasks.MaxConcurrentRoutines)
+			wg := errgroup.Group{}
+			for _, x := range errorOffsetListMap {
+				dataChannel <- x
+				routineLimiter <- struct{}{}
+				wg.Go(func() error {
+					//m := <-dataChannel
+					//var offset = int64(-1)
+					//for k, _ := range m {
+					//	offset = k
+					//}
+					//key := offset + t.MetaData.Seed
+					//docId := t.gen.BuildKey(key)
+					//
+					//fake := faker.NewWithSeed(rand.NewSource(int64(key)))
+					//doc, _ := t.gen.Template.GenerateDocument(&fake, t.OperationConfig.DocSize)
+					//
+					//retry := 0
+					//var err error
+					//result := &gocb.MutationResult{}
+					//
+					//initTime := time.Now().UTC().Format(time.RFC850)
+					//
+					//for retry = 0; retry <= t.OperationConfig.Exceptions.RetryAttempts; retry++ {
+					//	result, err = collectionObject.Collection.Insert(docId, doc, &gocb.InsertOptions{
+					//		DurabilityLevel: cb_sdk.GetDurability(t.InsertOptions.Durability),
+					//		PersistTo:       t.InsertOptions.PersistTo,
+					//		ReplicateTo:     t.InsertOptions.ReplicateTo,
+					//		Timeout:         time.Duration(t.InsertOptions.Timeout) * time.Second,
+					//		Expiry:          time.Duration(t.InsertOptions.Expiry) * time.Second,
+					//	})
+					//
+					//	if err == nil {
+					//		break
+					//	}
+					//}
+					//
+					//if err != nil {
+					//	if errors.Is(err, gocb.ErrDocumentExists) {
+					//		if tempResult, err1 := collectionObject.Collection.Get(docId, &gocb.GetOptions{
+					//			Timeout: 5 * time.Second,
+					//		}); err1 == nil {
+					//			m[offset] = RetriedResult{
+					//				Status:   true,
+					//				CAS:      uint64(tempResult.Cas()),
+					//				InitTime: initTime,
+					//				AckTime:  time.Now().UTC().Format(time.RFC850),
+					//			}
+					//		} else {
+					//			m[offset] = RetriedResult{
+					//				Status:   true,
+					//				InitTime: initTime,
+					//				AckTime:  time.Now().UTC().Format(time.RFC850),
+					//			}
+					//		}
+					//	} else {
+					//		m[offset] = RetriedResult{
+					//			InitTime: initTime,
+					//			AckTime:  time.Now().UTC().Format(time.RFC850),
+					//		}
+					//	}
+					//} else {
+					//	m[offset] = RetriedResult{
+					//		Status:   true,
+					//		CAS:      uint64(result.Cas()),
+					//		InitTime: initTime,
+					//		AckTime:  time.Now().UTC().Format(time.RFC850),
+					//	}
+					//}
+					<-dataChannel
+					<-routineLimiter
+					return nil
+				})
+			}
+			_ = wg.Wait()
+
+			shiftErrToCompletedOnRetrying(exception, t.Result, errorOffsetListMap, errorOffsetMaps,
+				completedOffsetMaps)
+		}
+	}
+
+	t.State.MakeCompleteKeyFromMap(completedOffsetMaps)
+	t.State.MakeErrorKeyFromMap(errorOffsetMaps)
+	t.Result.Failure = int64(len(t.State.KeyStates.Err))
+	t.Result.Success = t.OperationConfig.End - t.OperationConfig.Start - t.Result.Failure
+	log.Println("completed retrying:- ", t.Operation, t.IdentifierToken, t.ResultSeed)
+}
+
+func (t *GenericLoadingTask) MatchResultSeed(resultSeed string) (bool, error) {
+	defer t.lock.Unlock()
+	t.lock.Lock()
+	if fmt.Sprintf("%d", t.ResultSeed) == resultSeed {
+		if t.TaskPending {
+			return true, err_sirius.TaskInPendingState
+		}
+		if t.Result == nil {
+			t.Result = task_result.ConfigTaskResult(t.Operation, t.ResultSeed)
+		}
+		return true, nil
+	}
+	return false, nil
+}
+
+func (t *GenericLoadingTask) SetException(exceptions Exceptions) {
+	t.OperationConfig.Exceptions = exceptions
+}
+
+func (t *GenericLoadingTask) GetOperationConfig() (*OperationConfig, *task_state.TaskState) {
+	return t.OperationConfig, t.State
+}
diff --git a/internal/tasks/bulk_loading/helper.go b/internal/tasks/bulk_loading/helper.go
new file mode 100644
index 0000000..1dba06b
--- /dev/null
+++ b/internal/tasks/bulk_loading/helper.go
@@ -0,0 +1,484 @@
+package bulk_loading
+
+import (
+	"errors"
+	"fmt"
+	"github.com/couchbase/gocb/v2"
+	"github.com/couchbaselabs/sirius/internal/cb_sdk"
+	"github.com/couchbaselabs/sirius/internal/db"
+	"github.com/couchbaselabs/sirius/internal/docgenerator"
+	"github.com/couchbaselabs/sirius/internal/err_sirius"
+	"github.com/couchbaselabs/sirius/internal/task_result"
+	"github.com/couchbaselabs/sirius/internal/tasks"
+	"github.com/jaswdr/faker"
+	"golang.org/x/exp/slices"
+)
+
+// OperationConfig contains all the configuration for document operation.
+type OperationConfig struct {
+	DocSize        int        `json:"docSize" doc:"true"`
+	DocType        string     `json:"docType,omitempty" doc:"true"`
+	KeySize        int        `json:"keySize,omitempty" doc:"true"`
+	TemplateName   string     `json:"template" doc:"true"`
+	Start          int64      `json:"start" doc:"true"`
+	End            int64      `json:"end" doc:"true"`
+	FieldsToChange []string   `json:"fieldsToChange" doc:"true"`
+	Exceptions     Exceptions `json:"exceptions,omitempty" doc:"true"`
+}
+
+// ConfigureOperationConfig configures and validate the OperationConfig
+func ConfigureOperationConfig(o *OperationConfig) error {
+	if o == nil {
+		return err_sirius.ParsingOperatingConfig
+	}
+	if o.DocType == "" {
+		o.DocType = docgenerator.JsonDocument
+	}
+
+	if o.KeySize > docgenerator.DefaultKeySize {
+		o.KeySize = docgenerator.DefaultKeySize
+	}
+	if o.DocSize <= 0 {
+		o.DocSize = docgenerator.DefaultDocSize
+	}
+	if o.Start < 0 {
+		o.Start = 0
+		o.End = 0
+	}
+	if o.Start > o.End {
+		o.End = o.Start
+		return err_sirius.MalformedOperationRange
+	}
+	return nil
+}
+
+// // checkBulkWriteOperation is used to check if the Write operation is on main doc or sub doc
+//
+//	func checkBulkWriteOperation(operation string, subDocFlag bool) bool {
+//		if subDocFlag {
+//			switch operation {
+//			case tasks.SubDocInsertOperation, tasks.SubDocUpsertOperation, tasks.SingleSubDocReplaceOperation:
+//				return true
+//			default:
+//				return false
+//			}
+//		} else {
+//			switch operation {
+//			case tasks.InsertOperation, tasks.UpsertOperation, tasks.TouchOperation:
+//				return true
+//			default:
+//				return false
+//			}
+//		}
+//	}
+//
+// // retrieveLastConfig retrieves the OperationConfig for the offset for a successful Sirius operation.
+//
+//	func retrieveLastConfig(r *tasks.Request, offset int64, subDocFlag bool) (OperationConfig, error) {
+//		if r == nil {
+//			return OperationConfig{}, err_sirius.RequestIsNil
+//		}
+//		for i := range r.Tasks {
+//			if checkBulkWriteOperation(r.Tasks[len(r.Tasks)-i-1].Operation, subDocFlag) {
+//				task, ok := r.Tasks[len(r.Tasks)-i-1].Task.(BulkTask)
+//				if ok {
+//					operationConfig, taskState := task.GetOperationConfig()
+//					if operationConfig == nil {
+//						continue
+//					} else {
+//						if offset >= (operationConfig.Start) && (offset < operationConfig.End) {
+//							if _, ok := taskState.ReturnCompletedOffset()[offset]; ok {
+//								return *operationConfig, nil
+//							}
+//						}
+//					}
+//				}
+//			}
+//		}
+//		return OperationConfig{}, err_sirius.NilOperationConfig
+//	}
+//
+// // retracePreviousFailedInsertions returns a lookup table representing the offsets which are not inserted properly..
+// func retracePreviousFailedInsertions(r *tasks.Request, collectionIdentifier string,
+//
+//		resultSeed int64) (map[int64]struct{}, error) {
+//		if r == nil {
+//			return map[int64]struct{}{}, err_sirius.RequestIsNil
+//		}
+//		defer r.Unlock()
+//		r.Lock()
+//		result := make(map[int64]struct{})
+//		for i := range r.Tasks {
+//			td := r.Tasks[i]
+//			if td.Operation == tasks.InsertOperation {
+//				if task, ok := td.Task.(BulkTask); ok {
+//					u, ok1 := task.(*GenericLoadingTask)
+//					if ok1 {
+//						if collectionIdentifier != u.MetaDataIdentifier() {
+//							continue
+//						}
+//						if resultSeed != u.ResultSeed {
+//							errorOffSet := u.State.ReturnErrOffset()
+//							for offSet, _ := range errorOffSet {
+//								result[offSet] = struct{}{}
+//							}
+//						}
+//					}
+//				}
+//			}
+//		}
+//		return result, nil
+//	}
+//
+// // retracePreviousDeletions returns a lookup table representing the offsets which are successfully deleted.
+// func retracePreviousDeletions(r *tasks.Request, collectionIdentifier string, resultSeed int64) (map[int64]struct{},
+//
+//		error) {
+//		if r == nil {
+//			return map[int64]struct{}{}, err_sirius.RequestIsNil
+//		}
+//		defer r.Unlock()
+//		r.Lock()
+//		result := make(map[int64]struct{})
+//		for i := range r.Tasks {
+//			td := r.Tasks[i]
+//			if td.Operation == tasks.DeleteOperation {
+//				if task, ok := td.Task.(BulkTask); ok {
+//					u, ok1 := task.(*DeleteTask)
+//					if ok1 {
+//						if collectionIdentifier != u.MetaDataIdentifier() {
+//							continue
+//						}
+//						if resultSeed != u.ResultSeed {
+//							completedOffSet := u.State.ReturnCompletedOffset()
+//							for deletedOffset, _ := range completedOffSet {
+//								result[deletedOffset] = struct{}{}
+//							}
+//						}
+//					}
+//				}
+//			}
+//		}
+//		return result, nil
+//	}
+//
+// // retracePreviousSubDocDeletions  returns a lookup table representing the offsets which are successfully deleted.
+// func retracePreviousSubDocDeletions(r *tasks.Request, collectionIdentifier string,
+//
+//		resultSeed int64) (map[int64]struct{}, error) {
+//		if r == nil {
+//			return map[int64]struct{}{}, err_sirius.RequestIsNil
+//		}
+//		defer r.Unlock()
+//		r.Lock()
+//		result := make(map[int64]struct{})
+//		if r == nil {
+//			return result, err_sirius.RequestIsNil
+//		}
+//		for i := range r.Tasks {
+//			td := r.Tasks[i]
+//			if td.Operation == tasks.SubDocDeleteOperation {
+//				if task, ok := td.Task.(BulkTask); ok {
+//					u, ok1 := task.(*SubDocDelete)
+//					if ok1 {
+//						if collectionIdentifier != u.MetaDataIdentifier() {
+//							continue
+//						}
+//						if resultSeed != u.ResultSeed {
+//							completedOffSet := u.State.ReturnCompletedOffset()
+//							for deletedOffset, _ := range completedOffSet {
+//								result[deletedOffset] = struct{}{}
+//							}
+//						}
+//					}
+//				}
+//			}
+//		}
+//		return result, nil
+//	}
+//
+// retracePreviousMutations returns an updated document after mutating the original documents.
+func retracePreviousMutations(r *tasks.Request, collectionIdentifier string, offset int64, doc interface{},
+	gen *docgenerator.Generator, fake *faker.Faker, resultSeed int64) (interface{}, error) {
+	if r == nil {
+		return doc, err_sirius.RequestIsNil
+	}
+	defer r.Unlock()
+	r.Lock()
+	for i := range r.Tasks {
+		td := r.Tasks[i]
+		if td.Operation == tasks.UpsertOperation {
+			if tempX, ok := td.Task.(BulkTask); ok {
+				u, ok := tempX.(*GenericLoadingTask)
+				if ok {
+					if collectionIdentifier != u.MetaDataIdentifier() {
+						continue
+					}
+					if offset >= (u.OperationConfig.Start) && (offset < u.OperationConfig.End) && resultSeed != u.
+						ResultSeed {
+						if u.State == nil {
+							return doc, fmt.Errorf("Unable to retrace previous mutations on sirius for " + u.MetaDataIdentifier())
+						}
+						errOffset := u.State.ReturnErrOffset()
+						if _, ok := errOffset[offset]; ok {
+							continue
+						} else {
+							doc, _ = gen.Template.UpdateDocument(u.OperationConfig.FieldsToChange, doc,
+								u.OperationConfig.DocSize, fake)
+						}
+					}
+
+				}
+			}
+		}
+	}
+	return doc, nil
+}
+
+//func retracePreviousSubDocMutations(r *tasks.Request, collectionIdentifier string, offset int64,
+//	gen docgenerator.Generator,
+//	fake *faker.Faker, resultSeed int64,
+//	subDocumentMap map[string]any) (map[string]any, error) {
+//	if r == nil {
+//		return map[string]any{}, err_sirius.RequestIsNil
+//	}
+//	defer r.Unlock()
+//	r.Lock()
+//	var result map[string]any = subDocumentMap
+//	for i := range r.Tasks {
+//		td := r.Tasks[i]
+//		if td.Operation == tasks.SubDocUpsertOperation {
+//			if task, ok := td.Task.(BulkTask); ok {
+//				u, ok1 := task.(*SubDocUpsert)
+//				if ok1 {
+//					if collectionIdentifier != u.MetaDataIdentifier() {
+//						continue
+//					}
+//					if offset >= (u.OperationConfig.Start) && (offset < u.OperationConfig.End) && resultSeed != u.
+//						ResultSeed {
+//						errOffset := u.State.ReturnErrOffset()
+//						if _, ok := errOffset[offset]; ok {
+//							continue
+//						} else {
+//							result = gen.Template.GenerateSubPathAndValue(fake, u.OperationConfig.DocSize)
+//						}
+//					}
+//				}
+//			}
+//		}
+//	}
+//	return result, nil
+//}
+//
+//// countMutation return the number of mutation happened on an offset
+//func countMutation(r *tasks.Request, collectionIdentifier string, offset int64, resultSeed int64) (int, error) {
+//	if r == nil {
+//		return 0, err_sirius.RequestIsNil
+//	}
+//	defer r.Unlock()
+//	r.Lock()
+//	var result int = 0
+//	for i := range r.Tasks {
+//		td := r.Tasks[i]
+//		if td.Operation == tasks.SubDocUpsertOperation {
+//			if task, ok := td.Task.(BulkTask); ok {
+//				u, ok1 := task.(*SubDocUpsert)
+//				if ok1 {
+//					if collectionIdentifier != u.MetaDataIdentifier() {
+//						continue
+//					}
+//					if offset >= (u.OperationConfig.Start) && (offset < u.OperationConfig.End) && resultSeed != u.
+//						ResultSeed {
+//						completeOffset := u.State.ReturnCompletedOffset()
+//						if _, ok := completeOffset[offset]; ok {
+//							result++
+//						}
+//					}
+//				}
+//			}
+//		} else if td.Operation == tasks.SubDocDeleteOperation {
+//			if task, ok := td.Task.(BulkTask); ok {
+//				u, ok1 := task.(*SubDocDelete)
+//				if ok1 {
+//					if collectionIdentifier != u.MetaDataIdentifier() {
+//						continue
+//					}
+//					if offset >= (u.OperationConfig.Start) && (offset < u.OperationConfig.End) && resultSeed != u.
+//						ResultSeed {
+//						completeOffset := u.State.ReturnCompletedOffset()
+//						if _, ok := completeOffset[offset]; ok {
+//							result++
+//						}
+//					}
+//				}
+//			}
+//		} else if td.Operation == tasks.SubDocReplaceOperation {
+//			if task, ok := td.Task.(BulkTask); ok {
+//				u, ok1 := task.(*SubDocReplace)
+//				if ok1 {
+//					if collectionIdentifier != u.MetaDataIdentifier() {
+//						continue
+//					}
+//					if offset >= (u.OperationConfig.Start) && (offset < u.OperationConfig.End) && resultSeed != u.
+//						ResultSeed {
+//						completeOffset := u.State.ReturnCompletedOffset()
+//						if _, ok := completeOffset[offset]; ok {
+//							result++
+//						}
+//					}
+//				}
+//			}
+//		} else if td.Operation == tasks.SubDocInsertOperation {
+//			if task, ok := td.Task.(BulkTask); ok {
+//				u, ok1 := task.(*SubDocInsert)
+//				if ok1 {
+//					if collectionIdentifier != u.MetaDataIdentifier() {
+//						continue
+//					}
+//					if offset >= (u.OperationConfig.Start) && (offset < u.OperationConfig.End) && resultSeed != u.
+//						ResultSeed {
+//						completeOffset := u.State.ReturnCompletedOffset()
+//						if _, ok := completeOffset[offset]; ok {
+//							result++
+//						}
+//					}
+//				}
+//			}
+//		}
+//	}
+//	return result, nil
+//
+//}
+
+type RetriedResult struct {
+	Status   bool   `json:"status" doc:"true"`
+	CAS      uint64 `json:"cas" doc:"true"`
+	InitTime string `json:"initTime" doc:"true"`
+	AckTime  string `json:"ackTime" doc:"true"`
+}
+
+type Exceptions struct {
+	IgnoreExceptions []string `json:"ignoreExceptions,omitempty" doc:"true"`
+	RetryExceptions  []string `json:"retryExceptions,omitempty" doc:"true"`
+	RetryAttempts    int      `json:"retryAttempts,omitempty" doc:"true"`
+}
+
+func GetExceptions(result *task_result.TaskResult, RetryExceptions []string) []string {
+	var exceptionList []string
+	if len(RetryExceptions) == 0 {
+		for exception, _ := range result.BulkError {
+			exceptionList = append(exceptionList, exception)
+		}
+	} else {
+		exceptionList = RetryExceptions
+	}
+	return exceptionList
+}
+
+// shiftErrToCompletedOnRetrying will bring the offset which successfully completed their respective operation on
+// retrying
+func shiftErrToCompletedOnRetrying(exception string, result *task_result.TaskResult,
+	errorOffsetListMap []map[int64]RetriedResult, errorOffsetMaps, completedOffsetMaps map[int64]struct{}) {
+	if _, ok := result.BulkError[exception]; ok {
+		for _, x := range errorOffsetListMap {
+			for offset, retryResult := range x {
+				if retryResult.Status == true {
+					delete(errorOffsetMaps, offset)
+					completedOffsetMaps[offset] = struct{}{}
+					for index := range result.BulkError[exception] {
+						if result.BulkError[exception][index].Offset == offset {
+
+							offsetRetriedIndex := slices.IndexFunc(result.RetriedError[exception],
+								func(document task_result.FailedDocument) bool {
+									return document.Offset == offset
+								})
+
+							if offsetRetriedIndex == -1 {
+								result.RetriedError[exception] = append(result.RetriedError[exception], result.BulkError[exception][index])
+
+								result.RetriedError[exception][len(result.RetriedError[exception])-1].
+									Status = retryResult.Status
+
+								result.RetriedError[exception][len(result.RetriedError[exception])-1].
+									Cas = retryResult.CAS
+
+								result.RetriedError[exception][len(result.RetriedError[exception])-1].
+									SDKTiming.SendTime = retryResult.InitTime
+
+								result.RetriedError[exception][len(result.RetriedError[exception])-1].
+									SDKTiming.AckTime = retryResult.AckTime
+
+							} else {
+								result.RetriedError[exception][offsetRetriedIndex].Status = retryResult.Status
+								result.RetriedError[exception][offsetRetriedIndex].Cas = retryResult.CAS
+								result.RetriedError[exception][offsetRetriedIndex].SDKTiming.SendTime =
+									retryResult.InitTime
+								result.RetriedError[exception][offsetRetriedIndex].SDKTiming.AckTime =
+									retryResult.AckTime
+							}
+
+							result.BulkError[exception][index] = result.BulkError[exception][len(
+								result.BulkError[exception])-1]
+
+							result.BulkError[exception] = result.BulkError[exception][:len(
+								result.BulkError[exception])-1]
+
+							break
+						}
+					}
+				} else {
+					for index := range result.BulkError[exception] {
+						if result.BulkError[exception][index].Offset == offset {
+							result.BulkError[exception][index].SDKTiming.SendTime = retryResult.InitTime
+							result.BulkError[exception][index].SDKTiming.AckTime = retryResult.AckTime
+							result.RetriedError[exception] = append(result.RetriedError[exception],
+								result.BulkError[exception][index])
+							break
+						}
+					}
+				}
+			}
+		}
+	}
+}
+
+// shiftErrToCompletedOnIgnore will ignore retrying operation for offset lying in ignore exception category
+func shiftErrToCompletedOnIgnore(ignoreExceptions []string, result *task_result.TaskResult, errorOffsetMaps,
+	completedOffsetMaps map[int64]struct{}) {
+	for _, exception := range ignoreExceptions {
+		for _, failedDocs := range result.BulkError[exception] {
+			if _, ok := errorOffsetMaps[failedDocs.Offset]; ok {
+				delete(errorOffsetMaps, failedDocs.Offset)
+				completedOffsetMaps[failedDocs.Offset] = struct{}{}
+			}
+		}
+		delete(result.BulkError, exception)
+	}
+}
+
+func configExtraParameters(dbType string, d *db.Extras) error {
+	if dbType == db.COUCHBASE_DB {
+		if d.Bucket == "" {
+			return err_sirius.BucketIsMisssing
+		}
+		if d.Scope == "" {
+			d.Scope = cb_sdk.DefaultScope
+		}
+		if d.Collection == "" {
+			d.Collection = cb_sdk.DefaultCollection
+		}
+	}
+	if dbType == db.MONGO_DB {
+		if d.Collection == "" {
+			return err_sirius.CollectionIsMissing
+		}
+	}
+	return nil
+}
+
+func checkInsertAllowedError(err error) bool {
+	if errors.Is(err, gocb.ErrDocumentExists) {
+		return true
+	}
+	return false
+}
diff --git a/internal/tasks/bulk_loading/task_bulk_delete.go b/internal/tasks/bulk_loading/task_bulk_delete.go
new file mode 100644
index 0000000..4906dcb
--- /dev/null
+++ b/internal/tasks/bulk_loading/task_bulk_delete.go
@@ -0,0 +1,367 @@
+package bulk_loading
+
+//
+//import (
+//	"errors"
+//	"fmt"
+//	"github.com/couchbase/gocb/v2"
+//	"github.com/couchbaselabs/sirius/internal/cb_sdk"
+//	"github.com/couchbaselabs/sirius/internal/docgenerator"
+//	"github.com/couchbaselabs/sirius/internal/err_sirius"
+//	"github.com/couchbaselabs/sirius/internal/meta_data"
+//	"github.com/couchbaselabs/sirius/internal/task_result"
+//	"github.com/couchbaselabs/sirius/internal/task_state"
+//	"github.com/couchbaselabs/sirius/internal/tasks"
+//	"github.com/couchbaselabs/sirius/internal/template"
+//	"golang.org/x/sync/errgroup"
+//	"log"
+//	"math"
+//	"strings"
+//	"sync"
+//	"time"
+//)
+//
+//type DeleteTask struct {
+//	IdentifierToken string                        `json:"identifierToken" doc:"true"`
+//	ClusterConfig   *cb_sdk.ClusterConfig         `json:"clusterConfig" doc:"true"`
+//	Bucket          string                        `json:"bucket" doc:"true"`
+//	Scope           string                        `json:"scope,omitempty" doc:"true"`
+//	Collection      string                        `json:"collection,omitempty" doc:"true"`
+//	RemoveOptions   *cb_sdk.RemoveOptions         `json:"removeOptions,omitempty" doc:"true"`
+//	OperationConfig *OperationConfig              `json:"operationConfig,omitempty" doc:"true"`
+//	Operation       string                        `json:"operation" doc:"false"`
+//	ResultSeed      int64                         `json:"resultSeed" doc:"false"`
+//	TaskPending     bool                          `json:"taskPending" doc:"false"`
+//	State           *task_state.TaskState         `json:"State" doc:"false"`
+//	MetaData        *meta_data.CollectionMetaData `json:"metaData" doc:"false"`
+//	Result          *task_result.TaskResult       `json:"-" doc:"false"`
+//	gen             *docgenerator.Generator       `json:"-" doc:"false"`
+//	req             *tasks.Request                `json:"-" doc:"false"`
+//	rerun           bool                          `json:"-" doc:"false"`
+//	lock            sync.Mutex                    `json:"-" doc:"false"`
+//}
+//
+//func (task *DeleteTask) Describe() string {
+//	return `Delete task deletes documents in bulk into a bucket.
+//The task will delete documents from [start,end] inclusive.`
+//}
+//
+//func (task *DeleteTask) MetaDataIdentifier() string {
+//	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
+//	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
+//		task.Collection}, ":")
+//}
+//
+//func (task *DeleteTask) CheckIfPending() bool {
+//	return task.TaskPending
+//}
+//
+//// Config checks the validity of DeleteTask
+//func (task *DeleteTask) Config(req *tasks.Request, reRun bool) (int64, error) {
+//	task.TaskPending = true
+//	task.req = req
+//
+//	if task.req == nil {
+//		task.TaskPending = false
+//		return 0, err_sirius.RequestIsNil
+//	}
+//
+//	task.req.ReconnectionManager()
+//	if _, err_sirius := task.req.GetCluster(task.ClusterConfig); err_sirius != nil {
+//		task.TaskPending = false
+//		return 0, err_sirius
+//	}
+//
+//	task.lock = sync.Mutex{}
+//	task.rerun = reRun
+//
+//	if !reRun {
+//		task.ResultSeed = int64(time.Now().UnixNano())
+//		task.Operation = tasks.DeleteOperation
+//
+//		if task.Bucket == "" {
+//			task.Bucket = cb_sdk.DefaultBucket
+//		}
+//		if task.Scope == "" {
+//			task.Scope = cb_sdk.DefaultScope
+//		}
+//		if task.Collection == "" {
+//			task.Collection = cb_sdk.DefaultCollection
+//		}
+//
+//		if err_sirius := cb_sdk.ConfigRemoveOptions(task.RemoveOptions); err_sirius != nil {
+//			task.TaskPending = false
+//			return 0, err_sirius
+//		}
+//
+//		if err_sirius := ConfigureOperationConfig(task.OperationConfig); err_sirius != nil {
+//			task.TaskPending = false
+//			return 0, err_sirius
+//		}
+//
+//		task.MetaData = task.req.MetaData.GetCollectionMetadata(task.MetaDataIdentifier())
+//
+//		task.req.Lock()
+//		task.State = task_state.ConfigTaskState(task.MetaData.Seed, task.MetaData.SeedEnd, task.ResultSeed)
+//		task.req.Unlock()
+//
+//	} else {
+//		if task.State == nil {
+//			return task.ResultSeed, err_sirius.TaskStateIsNil
+//		}
+//
+//		task.State.SetupStoringKeys()
+//		_ = task_result.DeleteResultFile(task.ResultSeed)
+//		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//	}
+//	return task.ResultSeed, nil
+//}
+//
+//func (task *DeleteTask) TearUp() error {
+//	task.Result.StopStoringResult()
+//	if err_sirius := task.Result.SaveResultIntoFile(); err_sirius != nil {
+//		log.Println("not able to save Result into ", task.ResultSeed, task.Operation)
+//	}
+//	task.Result = nil
+//	task.State.StopStoringState()
+//	task.TaskPending = false
+//	return task.req.SaveRequestIntoFile()
+//}
+//
+//func (task *DeleteTask) Do() error {
+//
+//	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
+//
+//	collectionObject, err1 := task.GetCollectionObject()
+//
+//	task.gen = docgenerator.ConfigGenerator(
+//		task.OperationConfig.KeySize,
+//		task.OperationConfig.DocSize,
+//		task.OperationConfig.DocType,
+//		task.OperationConfig.KeyPrefix,
+//		task.OperationConfig.KeySuffix,
+//		template.InitialiseTemplate(task.OperationConfig.TemplateName))
+//
+//	if err1 != nil {
+//		task.Result.ErrorOther = err1.Error()
+//		task.Result.FailWholeBulkOperation(task.OperationConfig.Start, task.OperationConfig.End, err1, task.State,
+//			task.gen, task.MetaData.Seed)
+//		return task.TearUp()
+//	}
+//
+//	deleteDocuments(task, collectionObject)
+//	task.Result.Success = task.OperationConfig.End - task.OperationConfig.Start - task.Result.Failure
+//
+//	return task.TearUp()
+//}
+//
+//// deleteDocuments delete the document stored on a host from start to end.
+//func deleteDocuments(task *DeleteTask, collectionObject *cb_sdk.CollectionObject) {
+//
+//	if task.req.ContextClosed() {
+//		return
+//	}
+//
+//	skip := make(map[int64]struct{})
+//	for _, offset := range task.State.KeyStates.Completed {
+//		skip[offset] = struct{}{}
+//	}
+//	for _, offset := range task.State.KeyStates.Err {
+//		skip[offset] = struct{}{}
+//	}
+//
+//	routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
+//	dataChannel := make(chan int64, tasks.MaxConcurrentRoutines)
+//	group := errgroup.Group{}
+//
+//	for i := task.OperationConfig.Start; i < task.OperationConfig.End; i++ {
+//
+//		if task.req.ContextClosed() {
+//			close(routineLimiter)
+//			close(dataChannel)
+//			return
+//		}
+//
+//		routineLimiter <- struct{}{}
+//		dataChannel <- i
+//		group.Go(func() error {
+//			offset := <-dataChannel
+//			key := task.MetaData.Seed + offset
+//			docId := task.gen.BuildKey(key)
+//			if _, ok := skip[offset]; ok {
+//				<-routineLimiter
+//				return fmt.Errorf("alreday performed operation on " + docId)
+//			}
+//
+//			var err_sirius error
+//			initTime := time.Now().UTC().Format(time.RFC850)
+//			for retry := 0; retry < int(math.Max(float64(1), float64(task.OperationConfig.Exceptions.
+//				RetryAttempts))); retry++ {
+//				initTime = time.Now().UTC().Format(time.RFC850)
+//				_, err_sirius = collectionObject.Collection.Remove(docId, &gocb.RemoveOptions{
+//					Cas:             gocb.Cas(task.RemoveOptions.Cas),
+//					PersistTo:       task.RemoveOptions.PersistTo,
+//					ReplicateTo:     task.RemoveOptions.ReplicateTo,
+//					DurabilityLevel: cb_sdk.GetDurability(task.RemoveOptions.Durability),
+//					Timeout:         time.Duration(task.RemoveOptions.Timeout) * time.Second,
+//				})
+//				if err_sirius == nil {
+//					break
+//				}
+//			}
+//			if err_sirius != nil {
+//				if errors.Is(err_sirius, gocb.ErrDocumentNotFound) && task.rerun {
+//					task.State.StateChannel <- task_state.StateHelper{Status: task_state.COMPLETED, Offset: offset}
+//					<-routineLimiter
+//					return nil
+//				} else {
+//					task.Result.IncrementFailure(initTime, docId, err_sirius, false, uint64(0), offset)
+//					task.State.StateChannel <- task_state.StateHelper{Status: task_state.ERR, Offset: offset}
+//					<-routineLimiter
+//					return err_sirius
+//				}
+//			}
+//
+//			task.State.StateChannel <- task_state.StateHelper{Status: task_state.COMPLETED, Offset: offset}
+//			<-routineLimiter
+//			return nil
+//		})
+//	}
+//	_ = group.Wait()
+//	close(routineLimiter)
+//	close(dataChannel)
+//	task.PostTaskExceptionHandling(collectionObject)
+//	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//}
+//
+//func (task *DeleteTask) PostTaskExceptionHandling(collectionObject *cb_sdk.CollectionObject) {
+//	task.Result.StopStoringResult()
+//	task.State.StopStoringState()
+//	if task.OperationConfig.Exceptions.RetryAttempts <= 0 {
+//		return
+//	}
+//
+//	// Get all the errorOffset
+//	errorOffsetMaps := task.State.ReturnErrOffset()
+//	// Get all the completed offset
+//	completedOffsetMaps := task.State.ReturnCompletedOffset()
+//
+//	// For the offset in ignore exceptions :-> move them from error to completed
+//	shiftErrToCompletedOnIgnore(task.OperationConfig.Exceptions.IgnoreExceptions, task.Result, errorOffsetMaps, completedOffsetMaps)
+//
+//	if task.OperationConfig.Exceptions.RetryAttempts > 0 {
+//
+//		exceptionList := GetExceptions(task.Result, task.OperationConfig.Exceptions.RetryExceptions)
+//
+//		// For the retry exceptions :-> move them on success after retrying from err_sirius to completed
+//		for _, exception := range exceptionList {
+//
+//			errorOffsetListMap := make([]map[int64]RetriedResult, 0)
+//			for _, failedDocs := range task.Result.BulkError[exception] {
+//				m := make(map[int64]RetriedResult)
+//				m[failedDocs.Offset] = RetriedResult{}
+//				errorOffsetListMap = append(errorOffsetListMap, m)
+//			}
+//
+//			routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
+//			dataChannel := make(chan map[int64]RetriedResult, tasks.MaxConcurrentRoutines)
+//			wg := errgroup.Group{}
+//			for _, x := range errorOffsetListMap {
+//				dataChannel <- x
+//				routineLimiter <- struct{}{}
+//				wg.Go(func() error {
+//					m := <-dataChannel
+//					var offset = int64(-1)
+//					for k, _ := range m {
+//						offset = k
+//					}
+//					key := task.MetaData.Seed + offset
+//					docId := task.gen.BuildKey(key)
+//
+//					retry := 0
+//					var err_sirius error
+//					result := &gocb.MutationResult{}
+//
+//					initTime := time.Now().UTC().Format(time.RFC850)
+//					for retry = 0; retry <= task.OperationConfig.Exceptions.RetryAttempts; retry++ {
+//						result, err_sirius = collectionObject.Collection.Remove(docId, &gocb.RemoveOptions{
+//							Cas:             gocb.Cas(task.RemoveOptions.Cas),
+//							PersistTo:       task.RemoveOptions.PersistTo,
+//							ReplicateTo:     task.RemoveOptions.ReplicateTo,
+//							DurabilityLevel: cb_sdk.GetDurability(task.RemoveOptions.Durability),
+//							Timeout:         time.Duration(task.RemoveOptions.Timeout) * time.Second,
+//						})
+//
+//						if err_sirius == nil {
+//							break
+//						}
+//					}
+//
+//					if err_sirius != nil {
+//						if errors.Is(err_sirius, gocb.ErrDocumentNotFound) {
+//							m[offset] = RetriedResult{
+//								Status:   true,
+//								CAS:      0,
+//								InitTime: initTime,
+//								AckTime:  time.Now().UTC().Format(time.RFC850),
+//							}
+//						} else {
+//							m[offset] = RetriedResult{
+//								InitTime: initTime,
+//								AckTime:  time.Now().UTC().Format(time.RFC850),
+//							}
+//						}
+//					} else {
+//						m[offset] = RetriedResult{
+//							Status:   true,
+//							CAS:      uint64(result.Cas()),
+//							InitTime: initTime,
+//							AckTime:  time.Now().UTC().Format(time.RFC850),
+//						}
+//					}
+//
+//					<-routineLimiter
+//					return nil
+//				})
+//			}
+//			_ = wg.Wait()
+//
+//			shiftErrToCompletedOnRetrying(exception, task.Result, errorOffsetListMap, errorOffsetMaps, completedOffsetMaps)
+//		}
+//	}
+//
+//	task.State.MakeCompleteKeyFromMap(completedOffsetMaps)
+//	task.State.MakeErrorKeyFromMap(errorOffsetMaps)
+//	task.Result.Failure = int64(len(task.State.KeyStates.Err))
+//	task.Result.Success = task.OperationConfig.End - task.OperationConfig.Start - task.Result.Failure
+//	log.Println("completed retrying:- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//
+//}
+//
+//func (task *DeleteTask) MatchResultSeed(resultSeed string) (bool, error) {
+//	defer task.lock.Unlock()
+//	task.lock.Lock()
+//	if fmt.Sprintf("%d", task.ResultSeed) == resultSeed {
+//		if task.TaskPending {
+//			return true, err_sirius.TaskInPendingState
+//		}
+//		if task.Result == nil {
+//			task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
+//		}
+//		return true, nil
+//	}
+//	return false, nil
+//}
+//
+//func (task *DeleteTask) GetCollectionObject() (*cb_sdk.CollectionObject, error) {
+//	return task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope,
+//		task.Collection)
+//}
+//
+//func (task *DeleteTask) SetException(exceptions Exceptions) {
+//	task.OperationConfig.Exceptions = exceptions
+//}
+//
+//func (task *DeleteTask) GetOperationConfig() (*OperationConfig, *task_state.TaskState) {
+//	return task.OperationConfig, task.State
+//}
diff --git a/internal/tasks/bulk_loading/task_bulk_insert.go b/internal/tasks/bulk_loading/task_bulk_insert.go
new file mode 100644
index 0000000..7f46b39
--- /dev/null
+++ b/internal/tasks/bulk_loading/task_bulk_insert.go
@@ -0,0 +1,53 @@
+package bulk_loading
+
+import (
+	"github.com/couchbaselabs/sirius/internal/db"
+	"github.com/couchbaselabs/sirius/internal/docgenerator"
+	"github.com/couchbaselabs/sirius/internal/task_result"
+	"github.com/couchbaselabs/sirius/internal/task_state"
+	"github.com/couchbaselabs/sirius/internal/tasks"
+	"github.com/jaswdr/faker"
+	"math/rand"
+	"sync"
+	"time"
+)
+
+func insertDocuments(start, end, seed int64, operationConfig *OperationConfig,
+	rerun bool, gen *docgenerator.Generator, state *task_state.TaskState, result *task_result.TaskResult,
+	databaseInfo tasks.DatabaseInformation, extra db.Extras, wg *sync.WaitGroup) {
+
+	defer wg.Done()
+
+	database, err := db.ConfigDatabase(databaseInfo.DBType)
+	if err != nil {
+		result.FailWholeBulkOperation(start, end, err, state, gen, seed)
+		return
+	}
+
+	for offset := start; offset < end; offset++ {
+
+		key := offset + seed
+		docId := gen.BuildKey(key)
+		fake := faker.NewWithSeed(rand.NewSource(int64(key)))
+		initTime := time.Now().UTC().Format(time.RFC850)
+		doc, err := gen.Template.GenerateDocument(&fake, operationConfig.DocSize)
+
+		if err != nil {
+			result.IncrementFailure(initTime, docId, err, false, 0, offset)
+			continue
+		}
+
+		operationResult := database.Create(databaseInfo.ConnStr, databaseInfo.Username, databaseInfo.Password, docId, doc, extra)
+
+		if operationResult.GetError() != nil {
+			if checkInsertAllowedError(operationResult.GetError()) && rerun {
+				state.StateChannel <- task_state.StateHelper{Status: task_state.COMPLETED, Offset: offset}
+			} else {
+				result.IncrementFailure(initTime, docId, err, false, 0, offset)
+				state.StateChannel <- task_state.StateHelper{Status: task_state.ERR, Offset: offset}
+			}
+		}
+
+		state.StateChannel <- task_state.StateHelper{Status: task_state.COMPLETED, Offset: offset}
+	}
+}
diff --git a/internal/tasks/bulk_loading/task_bulk_read.go b/internal/tasks/bulk_loading/task_bulk_read.go
new file mode 100644
index 0000000..384df54
--- /dev/null
+++ b/internal/tasks/bulk_loading/task_bulk_read.go
@@ -0,0 +1,330 @@
+package bulk_loading
+
+//
+//import (
+//	"fmt"
+//	"github.com/couchbase/gocb/v2"
+//	"github.com/couchbaselabs/sirius/internal/cb_sdk"
+//	"github.com/couchbaselabs/sirius/internal/docgenerator"
+//	"github.com/couchbaselabs/sirius/internal/err_sirius"
+//	"github.com/couchbaselabs/sirius/internal/meta_data"
+//	"github.com/couchbaselabs/sirius/internal/task_result"
+//	"github.com/couchbaselabs/sirius/internal/task_state"
+//	"github.com/couchbaselabs/sirius/internal/tasks"
+//	"github.com/couchbaselabs/sirius/internal/template"
+//	"golang.org/x/sync/errgroup"
+//	"log"
+//	"math"
+//	"strings"
+//	"sync"
+//	"time"
+//)
+//
+//type ReadTask struct {
+//	IdentifierToken string                        `json:"identifierToken" doc:"true"`
+//	ClusterConfig   *cb_sdk.ClusterConfig         `json:"clusterConfig" doc:"true"`
+//	Bucket          string                        `json:"bucket" doc:"true"`
+//	Scope           string                        `json:"scope,omitempty" doc:"true"`
+//	Collection      string                        `json:"collection,omitempty" doc:"true"`
+//	OperationConfig *OperationConfig              `json:"operationConfig,omitempty" doc:"true"`
+//	Operation       string                        `json:"operation" doc:"false"`
+//	ResultSeed      int64                         `json:"resultSeed" doc:"false"`
+//	TaskPending     bool                          `json:"taskPending" doc:"false"`
+//	State           *task_state.TaskState         `json:"State" doc:"false"`
+//	MetaData        *meta_data.CollectionMetaData `json:"metaData" doc:"false"`
+//	Result          *task_result.TaskResult       `json:"Result" doc:"false"`
+//	gen             *docgenerator.Generator       `json:"-" doc:"false"`
+//	req             *tasks.Request                `json:"-" doc:"false"`
+//	rerun           bool                          `json:"-" doc:"false"`
+//	lock            sync.Mutex                    `json:"-" doc:"false"`
+//}
+//
+//func (task *ReadTask) Describe() string {
+//	return "Read BulkTask get documents from bucket and validate them with the expected ones"
+//}
+//
+//func (task *ReadTask) MetaDataIdentifier() string {
+//	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
+//	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
+//		task.Collection}, ":")
+//}
+//
+//func (task *ReadTask) CheckIfPending() bool {
+//	return task.TaskPending
+//}
+//
+//func (task *ReadTask) TearUp() error {
+//	task.Result.StopStoringResult()
+//	if err_sirius := task.Result.SaveResultIntoFile(); err_sirius != nil {
+//		log.Println("not able to save Result into ", task.ResultSeed, task.Operation)
+//	}
+//	task.Result = nil
+//	task.State.StopStoringState()
+//	task.TaskPending = false
+//	return task.req.SaveRequestIntoFile()
+//}
+//
+//func (task *ReadTask) Config(req *tasks.Request, reRun bool) (int64, error) {
+//	task.TaskPending = true
+//	task.req = req
+//
+//	if task.req == nil {
+//		task.TaskPending = false
+//		return 0, err_sirius.RequestIsNil
+//	}
+//
+//	task.req.ReconnectionManager()
+//	if _, err_sirius := task.req.GetCluster(task.ClusterConfig); err_sirius != nil {
+//		task.TaskPending = false
+//		return 0, err_sirius
+//	}
+//
+//	task.lock = sync.Mutex{}
+//	task.rerun = reRun
+//
+//	if !reRun {
+//		task.ResultSeed = int64(time.Now().UnixNano())
+//		task.Operation = tasks.ReadOperation
+//
+//		if task.Bucket == "" {
+//			task.Bucket = cb_sdk.DefaultBucket
+//		}
+//		if task.Scope == "" {
+//			task.Scope = cb_sdk.DefaultScope
+//		}
+//		if task.Collection == "" {
+//			task.Collection = cb_sdk.DefaultCollection
+//		}
+//
+//		if err_sirius := ConfigureOperationConfig(task.OperationConfig); err_sirius != nil {
+//			task.TaskPending = false
+//			return 0, fmt.Errorf(err_sirius.Error())
+//		}
+//
+//		task.MetaData = task.req.MetaData.GetCollectionMetadata(task.MetaDataIdentifier())
+//		task.req.Lock()
+//		task.State = task_state.ConfigTaskState(task.MetaData.Seed, task.MetaData.SeedEnd, task.ResultSeed)
+//		task.req.Unlock()
+//
+//	} else {
+//		if task.State == nil {
+//			return task.ResultSeed, err_sirius.TaskStateIsNil
+//		}
+//
+//		task.State.SetupStoringKeys()
+//		_ = task_result.DeleteResultFile(task.ResultSeed)
+//		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//	}
+//	return task.ResultSeed, nil
+//}
+//
+//func (task *ReadTask) Do() error {
+//
+//	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
+//
+//	collectionObject, err1 := task.GetCollectionObject()
+//
+//	task.gen = docgenerator.ConfigGenerator(
+//		task.OperationConfig.KeySize,
+//		task.OperationConfig.DocSize,
+//		task.OperationConfig.DocType,
+//		task.OperationConfig.KeyPrefix,
+//		task.OperationConfig.KeySuffix,
+//		template.InitialiseTemplate(task.OperationConfig.TemplateName))
+//
+//	if err1 != nil {
+//		task.Result.ErrorOther = err1.Error()
+//		task.Result.FailWholeBulkOperation(task.OperationConfig.Start, task.OperationConfig.End,
+//			err1, task.State, task.gen, task.MetaData.Seed)
+//		return task.TearUp()
+//	}
+//
+//	getDocuments(task, collectionObject)
+//	task.Result.Success = task.OperationConfig.End - task.OperationConfig.Start - task.Result.Failure
+//
+//	return task.TearUp()
+//}
+//
+//// getDocuments reads the documents in the bucket
+//func getDocuments(task *ReadTask, collectionObject *cb_sdk.CollectionObject) {
+//
+//	if task.req.ContextClosed() {
+//		return
+//	}
+//
+//	routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
+//	dataChannel := make(chan int64, tasks.MaxConcurrentRoutines)
+//	skip := make(map[int64]struct{})
+//	for _, offset := range task.State.KeyStates.Completed {
+//		skip[offset] = struct{}{}
+//	}
+//	for _, offset := range task.State.KeyStates.Err {
+//		skip[offset] = struct{}{}
+//	}
+//
+//	group := errgroup.Group{}
+//	for i := task.OperationConfig.Start; i < task.OperationConfig.End; i++ {
+//
+//		if task.req.ContextClosed() {
+//			close(routineLimiter)
+//			close(dataChannel)
+//			return
+//		}
+//
+//		routineLimiter <- struct{}{}
+//		dataChannel <- i
+//		group.Go(func() error {
+//			offset := <-dataChannel
+//			key := task.MetaData.Seed + offset
+//			docId := task.gen.BuildKey(key)
+//			if _, ok := skip[offset]; ok {
+//				<-routineLimiter
+//				return fmt.Errorf("alreday performed operation on " + docId)
+//			}
+//
+//			initTime := time.Now().UTC().Format(time.RFC850)
+//			var err_sirius error
+//			for retry := 0; retry < int(math.Max(float64(1), float64(task.OperationConfig.Exceptions.
+//				RetryAttempts))); retry++ {
+//				initTime = time.Now().UTC().Format(time.RFC850)
+//				_, err_sirius = collectionObject.Collection.Get(docId, nil)
+//				if err_sirius == nil {
+//					break
+//				}
+//			}
+//
+//			if err_sirius != nil {
+//				task.State.StateChannel <- task_state.StateHelper{Status: task_state.ERR, Offset: offset}
+//				task.Result.IncrementFailure(initTime, docId, err_sirius, false, 0, offset)
+//				<-routineLimiter
+//				return err_sirius
+//			}
+//
+//			task.State.StateChannel <- task_state.StateHelper{Status: task_state.COMPLETED, Offset: offset}
+//			<-routineLimiter
+//			return nil
+//		})
+//	}
+//	_ = group.Wait()
+//	close(routineLimiter)
+//	close(dataChannel)
+//	task.PostTaskExceptionHandling(collectionObject)
+//	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//}
+//
+//func (task *ReadTask) PostTaskExceptionHandling(collectionObject *cb_sdk.CollectionObject) {
+//	task.Result.StopStoringResult()
+//	task.State.StopStoringState()
+//	if task.OperationConfig.Exceptions.RetryAttempts <= 0 {
+//		return
+//	}
+//
+//	// Get all the errorOffset
+//	errorOffsetMaps := task.State.ReturnErrOffset()
+//	// Get all the completed offset
+//	completedOffsetMaps := task.State.ReturnCompletedOffset()
+//
+//	// For the offset in ignore exceptions :-> move them from error to completed
+//	shiftErrToCompletedOnIgnore(task.OperationConfig.Exceptions.IgnoreExceptions, task.Result, errorOffsetMaps,
+//		completedOffsetMaps)
+//
+//	if task.OperationConfig.Exceptions.RetryAttempts > 0 {
+//
+//		exceptionList := GetExceptions(task.Result, task.OperationConfig.Exceptions.RetryExceptions)
+//
+//		// For the retry exceptions :-> move them on success after retrying from err_sirius to completed
+//		for _, exception := range exceptionList {
+//
+//			errorOffsetListMap := make([]map[int64]RetriedResult, 0)
+//			for _, failedDocs := range task.Result.BulkError[exception] {
+//				m := make(map[int64]RetriedResult)
+//				m[failedDocs.Offset] = RetriedResult{}
+//				errorOffsetListMap = append(errorOffsetListMap, m)
+//			}
+//
+//			routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
+//			dataChannel := make(chan map[int64]RetriedResult, tasks.MaxConcurrentRoutines)
+//			wg := errgroup.Group{}
+//			for _, x := range errorOffsetListMap {
+//				dataChannel <- x
+//				routineLimiter <- struct{}{}
+//				wg.Go(func() error {
+//					m := <-dataChannel
+//					var offset = int64(-1)
+//					for k, _ := range m {
+//						offset = k
+//					}
+//					key := task.MetaData.Seed + offset
+//					docId := task.gen.BuildKey(key)
+//
+//					retry := 0
+//					result := &gocb.GetResult{}
+//					var err_sirius error
+//					initTime := time.Now().UTC().Format(time.RFC850)
+//					for retry = 0; retry <= task.OperationConfig.Exceptions.RetryAttempts; retry++ {
+//						result, err_sirius = collectionObject.Collection.Get(docId, nil)
+//
+//						if err_sirius == nil {
+//							break
+//						}
+//					}
+//
+//					if err_sirius == nil {
+//						m[offset] = RetriedResult{
+//							Status:   true,
+//							CAS:      uint64(result.Cas()),
+//							InitTime: initTime,
+//							AckTime:  time.Now().UTC().Format(time.RFC850),
+//						}
+//					} else {
+//						m[offset] = RetriedResult{
+//							InitTime: initTime,
+//							AckTime:  time.Now().UTC().Format(time.RFC850),
+//						}
+//					}
+//
+//					<-routineLimiter
+//					return nil
+//				})
+//			}
+//			_ = wg.Wait()
+//
+//			shiftErrToCompletedOnRetrying(exception, task.Result, errorOffsetListMap, errorOffsetMaps,
+//				completedOffsetMaps)
+//		}
+//	}
+//
+//	task.State.MakeCompleteKeyFromMap(completedOffsetMaps)
+//	task.State.MakeErrorKeyFromMap(errorOffsetMaps)
+//	task.Result.Failure = int64(len(task.State.KeyStates.Err))
+//	task.Result.Success = task.OperationConfig.End - task.OperationConfig.Start - task.Result.Failure
+//	log.Println("completed retrying:- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//}
+//
+//func (task *ReadTask) MatchResultSeed(resultSeed string) (bool, error) {
+//	defer task.lock.Unlock()
+//	task.lock.Lock()
+//	if fmt.Sprintf("%d", task.ResultSeed) == resultSeed {
+//		if task.TaskPending {
+//			return true, err_sirius.TaskInPendingState
+//		}
+//		if task.Result == nil {
+//			task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
+//		}
+//		return true, nil
+//	}
+//	return false, nil
+//}
+//
+//func (task *ReadTask) GetCollectionObject() (*cb_sdk.CollectionObject, error) {
+//	return task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope,
+//		task.Collection)
+//}
+//
+//func (task *ReadTask) SetException(exceptions Exceptions) {
+//	task.OperationConfig.Exceptions = exceptions
+//}
+//
+//func (task *ReadTask) GetOperationConfig() (*OperationConfig, *task_state.TaskState) {
+//	return task.OperationConfig, task.State
+//}
diff --git a/internal/tasks/bulk_loading/task_bulk_touch.go b/internal/tasks/bulk_loading/task_bulk_touch.go
new file mode 100644
index 0000000..e6d8bd5
--- /dev/null
+++ b/internal/tasks/bulk_loading/task_bulk_touch.go
@@ -0,0 +1,352 @@
+package bulk_loading
+
+//
+//import (
+//	"fmt"
+//	"github.com/couchbase/gocb/v2"
+//	"github.com/couchbaselabs/sirius/internal/cb_sdk"
+//	"github.com/couchbaselabs/sirius/internal/docgenerator"
+//	"github.com/couchbaselabs/sirius/internal/err_sirius"
+//	"github.com/couchbaselabs/sirius/internal/meta_data"
+//	"github.com/couchbaselabs/sirius/internal/task_result"
+//	"github.com/couchbaselabs/sirius/internal/task_state"
+//	"github.com/couchbaselabs/sirius/internal/tasks"
+//	"github.com/couchbaselabs/sirius/internal/template"
+//	"golang.org/x/sync/errgroup"
+//	"log"
+//	"math"
+//	"strings"
+//	"sync"
+//	"time"
+//)
+//
+//type TouchTask struct {
+//	IdentifierToken string                        `json:"identifierToken" doc:"true"`
+//	ClusterConfig   *cb_sdk.ClusterConfig         `json:"clusterConfig" doc:"true"`
+//	Bucket          string                        `json:"bucket" doc:"true"`
+//	Scope           string                        `json:"scope,omitempty" doc:"true"`
+//	Collection      string                        `json:"collection,omitempty" doc:"true"`
+//	TouchOptions    *cb_sdk.TouchOptions          `json:"touchOptions,omitempty" doc:"true"`
+//	Expiry          int64                         `json:"expiry" doc:"true"`
+//	OperationConfig *OperationConfig              `json:"operationConfig,omitempty" doc:"true"`
+//	Operation       string                        `json:"operation" doc:"false"`
+//	ResultSeed      int64                         `json:"resultSeed" doc:"false"`
+//	TaskPending     bool                          `json:"taskPending" doc:"false"`
+//	State           *task_state.TaskState         `json:"State" doc:"false"`
+//	MetaData        *meta_data.CollectionMetaData `json:"metaData" doc:"false"`
+//	Result          *task_result.TaskResult       `json:"-" doc:"false"`
+//	gen             *docgenerator.Generator       `json:"-" doc:"false"`
+//	req             *tasks.Request                `json:"-" doc:"false"`
+//	rerun           bool                          `json:"-" doc:"false"`
+//	lock            sync.Mutex                    `json:"-" doc:"false"`
+//}
+//
+//func (task *TouchTask) MetaDataIdentifier() string {
+//	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
+//	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
+//		task.Collection}, ":")
+//}
+//
+//func (task *TouchTask) Describe() string {
+//	return `Upsert task mutates documents in bulk into a bucket.
+//The task will update the fields in a documents ranging from [start,end] inclusive.
+//We need to share the fields we want to update in a json document using SQL++ syntax.`
+//}
+//
+//func (task *TouchTask) CheckIfPending() bool {
+//	return task.TaskPending
+//}
+//
+//func (task *TouchTask) Config(req *tasks.Request, reRun bool) (int64, error) {
+//	task.TaskPending = true
+//	task.req = req
+//
+//	if task.req == nil {
+//		task.TaskPending = false
+//		return 0, err_sirius.RequestIsNil
+//	}
+//
+//	task.req.ReconnectionManager()
+//	if _, err_sirius := task.req.GetCluster(task.ClusterConfig); err_sirius != nil {
+//		task.TaskPending = false
+//		return 0, err_sirius
+//	}
+//
+//	task.lock = sync.Mutex{}
+//	task.rerun = reRun
+//
+//	if !reRun {
+//		task.ResultSeed = int64(time.Now().UnixNano())
+//		task.Operation = tasks.TouchOperation
+//
+//		if task.Bucket == "" {
+//			task.Bucket = cb_sdk.DefaultBucket
+//		}
+//		if task.Scope == "" {
+//			task.Scope = cb_sdk.DefaultScope
+//		}
+//		if task.Collection == "" {
+//			task.Collection = cb_sdk.DefaultCollection
+//		}
+//
+//		if err_sirius := cb_sdk.ConfigTouchOptions(task.TouchOptions); err_sirius != nil {
+//			task.TaskPending = false
+//			return 0, err_sirius
+//		}
+//
+//		if err_sirius := ConfigureOperationConfig(task.OperationConfig); err_sirius != nil {
+//			task.TaskPending = false
+//			return 0, err_sirius
+//		}
+//
+//		task.MetaData = task.req.MetaData.GetCollectionMetadata(task.MetaDataIdentifier())
+//
+//		task.req.Lock()
+//		if task.OperationConfig.End+task.MetaData.Seed > task.MetaData.SeedEnd {
+//			task.req.AddToSeedEnd(task.MetaData, (task.OperationConfig.End+task.MetaData.Seed)-(task.MetaData.SeedEnd))
+//		}
+//		task.State = task_state.ConfigTaskState(task.MetaData.Seed, task.MetaData.SeedEnd, task.ResultSeed)
+//		task.req.Unlock()
+//
+//	} else {
+//		if task.State == nil {
+//			return task.ResultSeed, err_sirius.TaskStateIsNil
+//		}
+//
+//		task.State.SetupStoringKeys()
+//		_ = task_result.DeleteResultFile(task.ResultSeed)
+//		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//	}
+//	return task.ResultSeed, nil
+//}
+//
+//func (task *TouchTask) TearUp() error {
+//	task.Result.StopStoringResult()
+//	if err_sirius := task.Result.SaveResultIntoFile(); err_sirius != nil {
+//		log.Println("not able to save Result into ", task.ResultSeed, task.Operation)
+//	}
+//	task.Result.StopStoringResult()
+//	task.Result = nil
+//	task.State.StopStoringState()
+//	task.TaskPending = false
+//	return task.req.SaveRequestIntoFile()
+//}
+//
+//func (task *TouchTask) Do() error {
+//
+//	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
+//
+//	collectionObject, err1 := task.GetCollectionObject()
+//
+//	task.gen = docgenerator.ConfigGenerator(
+//		task.OperationConfig.KeySize,
+//		task.OperationConfig.DocSize,
+//		task.OperationConfig.DocType,
+//		task.OperationConfig.KeyPrefix,
+//		task.OperationConfig.KeySuffix,
+//		template.InitialiseTemplate(task.OperationConfig.TemplateName))
+//
+//	if err1 != nil {
+//		task.Result.ErrorOther = err1.Error()
+//		task.Result.FailWholeBulkOperation(task.OperationConfig.Start, task.OperationConfig.End,
+//			err1, task.State, task.gen, task.MetaData.Seed)
+//		return task.TearUp()
+//	}
+//
+//	touchDocuments(task, collectionObject)
+//	task.Result.Success = task.OperationConfig.End - task.OperationConfig.Start - task.Result.Failure
+//
+//	return task.TearUp()
+//}
+//
+//func touchDocuments(task *TouchTask, collectionObject *cb_sdk.CollectionObject) {
+//
+//	if task.req.ContextClosed() {
+//		return
+//	}
+//
+//	routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
+//	dataChannel := make(chan int64, tasks.MaxConcurrentRoutines)
+//
+//	skip := make(map[int64]struct{})
+//	for _, offset := range task.State.KeyStates.Completed {
+//		skip[offset] = struct{}{}
+//	}
+//	for _, offset := range task.State.KeyStates.Err {
+//		skip[offset] = struct{}{}
+//	}
+//
+//	group := errgroup.Group{}
+//	for i := task.OperationConfig.Start; i < task.OperationConfig.End; i++ {
+//
+//		if task.req.ContextClosed() {
+//			close(routineLimiter)
+//			close(dataChannel)
+//			return
+//		}
+//
+//		routineLimiter <- struct{}{}
+//		dataChannel <- i
+//		group.Go(func() error {
+//			var err_sirius error
+//			offset := <-dataChannel
+//			key := task.MetaData.Seed + offset
+//			docId := task.gen.BuildKey(key)
+//			if _, ok := skip[offset]; ok {
+//				<-routineLimiter
+//				return fmt.Errorf("alreday performed operation on " + docId)
+//			}
+//
+//			initTime := time.Now().UTC().Format(time.RFC850)
+//
+//			for retry := 0; retry < int(math.Max(float64(1), float64(task.OperationConfig.Exceptions.
+//				RetryAttempts))); retry++ {
+//				initTime = time.Now().UTC().Format(time.RFC850)
+//				_, err_sirius = collectionObject.Collection.Touch(docId, time.Duration(task.Expiry)*time.Second,
+//					&gocb.TouchOptions{
+//						Timeout: time.Duration(task.TouchOptions.Timeout) * time.Second,
+//					})
+//
+//				if err_sirius == nil {
+//					break
+//				}
+//			}
+//
+//			if err_sirius != nil {
+//				task.Result.IncrementFailure(initTime, docId, err_sirius, false, 0, offset)
+//				task.State.StateChannel <- task_state.StateHelper{Status: task_state.ERR, Offset: offset}
+//				<-routineLimiter
+//				return err_sirius
+//			}
+//
+//			task.State.StateChannel <- task_state.StateHelper{Status: task_state.COMPLETED, Offset: offset}
+//
+//			<-routineLimiter
+//			return nil
+//		})
+//	}
+//	_ = group.Wait()
+//	close(routineLimiter)
+//	close(dataChannel)
+//	task.PostTaskExceptionHandling(collectionObject)
+//	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//}
+//
+//func (task *TouchTask) PostTaskExceptionHandling(collectionObject *cb_sdk.CollectionObject) {
+//	task.Result.StopStoringResult()
+//	task.State.StopStoringState()
+//	if task.OperationConfig.Exceptions.RetryAttempts <= 0 {
+//		return
+//	}
+//
+//	// Get all the errorOffset
+//	errorOffsetMaps := task.State.ReturnErrOffset()
+//	// Get all the completed offset
+//	completedOffsetMaps := task.State.ReturnCompletedOffset()
+//
+//	// For the offset in ignore exceptions :-> move them from error to completed
+//	shiftErrToCompletedOnIgnore(task.OperationConfig.Exceptions.IgnoreExceptions, task.Result, errorOffsetMaps,
+//		completedOffsetMaps)
+//
+//	if task.OperationConfig.Exceptions.RetryAttempts > 0 {
+//
+//		exceptionList := GetExceptions(task.Result, task.OperationConfig.Exceptions.RetryExceptions)
+//
+//		// For the retry exceptions :-> move them on success after retrying from err_sirius to completed
+//		for _, exception := range exceptionList {
+//
+//			errorOffsetListMap := make([]map[int64]RetriedResult, 0)
+//			for _, failedDocs := range task.Result.BulkError[exception] {
+//				m := make(map[int64]RetriedResult)
+//				m[failedDocs.Offset] = RetriedResult{}
+//				errorOffsetListMap = append(errorOffsetListMap, m)
+//			}
+//
+//			routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
+//			dataChannel := make(chan map[int64]RetriedResult, tasks.MaxConcurrentRoutines)
+//			wg := errgroup.Group{}
+//			for _, x := range errorOffsetListMap {
+//				dataChannel <- x
+//				routineLimiter <- struct{}{}
+//				wg.Go(func() error {
+//					m := <-dataChannel
+//					var offset = int64(-1)
+//					for k, _ := range m {
+//						offset = k
+//					}
+//					key := task.MetaData.Seed + offset
+//					docId := task.gen.BuildKey(key)
+//
+//					result := &gocb.MutationResult{}
+//					var err_sirius error
+//					initTime := time.Now().UTC().Format(time.RFC850)
+//					for retry := 0; retry <= task.OperationConfig.Exceptions.RetryAttempts; retry++ {
+//						_, err_sirius := collectionObject.Collection.Touch(docId, time.Duration(task.Expiry)*time.Second,
+//							&gocb.TouchOptions{
+//								Timeout: time.Duration(task.TouchOptions.Timeout) * time.Second,
+//							})
+//
+//						if err_sirius == nil {
+//							break
+//						}
+//					}
+//
+//					if err_sirius == nil {
+//						m[offset] = RetriedResult{
+//							Status:   true,
+//							CAS:      uint64(result.Cas()),
+//							InitTime: initTime,
+//							AckTime:  time.Now().UTC().Format(time.RFC850),
+//						}
+//					} else {
+//						m[offset] = RetriedResult{
+//							InitTime: initTime,
+//							AckTime:  time.Now().UTC().Format(time.RFC850),
+//						}
+//					}
+//
+//					<-routineLimiter
+//					return nil
+//				})
+//			}
+//			_ = wg.Wait()
+//
+//			shiftErrToCompletedOnRetrying(exception, task.Result, errorOffsetListMap, errorOffsetMaps,
+//				completedOffsetMaps)
+//		}
+//	}
+//
+//	task.State.MakeCompleteKeyFromMap(completedOffsetMaps)
+//	task.State.MakeErrorKeyFromMap(errorOffsetMaps)
+//	task.Result.Failure = int64(len(task.State.KeyStates.Err))
+//	task.Result.Success = task.OperationConfig.End - task.OperationConfig.Start - task.Result.Failure
+//	log.Println("completed retrying:- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//}
+//
+//func (task *TouchTask) MatchResultSeed(resultSeed string) (bool, error) {
+//	defer task.lock.Unlock()
+//	task.lock.Lock()
+//	if fmt.Sprintf("%d", task.ResultSeed) == resultSeed {
+//		if task.TaskPending {
+//			return true, err_sirius.TaskInPendingState
+//		}
+//		if task.Result == nil {
+//			task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
+//		}
+//		return true, nil
+//	}
+//	return false, nil
+//}
+//
+//func (task *TouchTask) GetCollectionObject() (*cb_sdk.CollectionObject, error) {
+//	return task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope,
+//		task.Collection)
+//}
+//
+//func (task *TouchTask) SetException(exceptions Exceptions) {
+//	task.OperationConfig.Exceptions = exceptions
+//}
+//
+//func (task *TouchTask) GetOperationConfig() (*OperationConfig, *task_state.TaskState) {
+//	return task.OperationConfig, task.State
+//}
diff --git a/internal/tasks/bulk_loading/task_bulk_upsert.go b/internal/tasks/bulk_loading/task_bulk_upsert.go
new file mode 100644
index 0000000..10af12b
--- /dev/null
+++ b/internal/tasks/bulk_loading/task_bulk_upsert.go
@@ -0,0 +1,62 @@
+package bulk_loading
+
+import (
+	"github.com/couchbaselabs/sirius/internal/db"
+	"github.com/couchbaselabs/sirius/internal/docgenerator"
+	"github.com/couchbaselabs/sirius/internal/task_result"
+	"github.com/couchbaselabs/sirius/internal/task_state"
+	"github.com/couchbaselabs/sirius/internal/tasks"
+	"github.com/jaswdr/faker"
+	"math/rand"
+	"sync"
+	"time"
+)
+
+func upsertDocuments(start, end, seed int64, operationConfig *OperationConfig,
+	_ bool, gen *docgenerator.Generator, state *task_state.TaskState, result *task_result.TaskResult,
+	databaseInfo tasks.DatabaseInformation, extra db.Extras, req *tasks.Request, identifier string, wg *sync.WaitGroup) {
+
+	defer wg.Done()
+
+	database, err := db.ConfigDatabase(databaseInfo.DBType)
+	if err != nil {
+		result.FailWholeBulkOperation(start, end, err, state, gen, seed)
+		return
+	}
+
+	for offset := start; offset < end; offset++ {
+
+		key := offset + seed
+		docId := gen.BuildKey(key)
+		fake := faker.NewWithSeed(rand.NewSource(int64(key)))
+		initTime := time.Now().UTC().Format(time.RFC850)
+
+		originalDoc, err := gen.Template.GenerateDocument(&fake, operationConfig.DocSize)
+		if err != nil {
+			result.IncrementFailure(initTime, docId, err, false, 0, offset)
+			continue
+		}
+		originalDoc, err = retracePreviousMutations(req, identifier, offset, originalDoc, gen, &fake, result.ResultSeed)
+		if err != nil {
+			result.IncrementFailure(initTime, docId, err, false, 0, offset)
+			continue
+		}
+
+		docUpdated, err := gen.Template.UpdateDocument(operationConfig.FieldsToChange, originalDoc,
+			operationConfig.DocSize, &fake)
+		if err != nil {
+			result.IncrementFailure(initTime, docId, err, false, 0, offset)
+			continue
+		}
+
+		operationResult := database.Update(databaseInfo.ConnStr, databaseInfo.Username, databaseInfo.Password, docId,
+			docUpdated, extra)
+
+		if operationResult.GetError() != nil {
+			result.IncrementFailure(initTime, docId, err, false, 0, offset)
+			state.StateChannel <- task_state.StateHelper{Status: task_state.ERR, Offset: offset}
+		}
+
+		state.StateChannel <- task_state.StateHelper{Status: task_state.COMPLETED, Offset: offset}
+	}
+}
diff --git a/internal/tasks/bulk_loading/task_retry_exceptions.go b/internal/tasks/bulk_loading/task_retry_exceptions.go
new file mode 100644
index 0000000..5f9e585
--- /dev/null
+++ b/internal/tasks/bulk_loading/task_retry_exceptions.go
@@ -0,0 +1,106 @@
+package bulk_loading
+
+//
+//import (
+//	"errors"
+//	"fmt"
+//	"github.com/couchbaselabs/sirius/internal/cb_sdk"
+//	"github.com/couchbaselabs/sirius/internal/err_sirius"
+//	"github.com/couchbaselabs/sirius/internal/task_state"
+//	"github.com/couchbaselabs/sirius/internal/tasks"
+//)
+//
+//type RetryExceptions struct {
+//	IdentifierToken string         `json:"identifierToken" doc:"true"`
+//	ResultSeed      string         `json:"resultSeed" doc:"true"`
+//	Exceptions      Exceptions     `json:"exceptions" doc:"true"`
+//	Task            BulkTask       `json:"-" doc:"false"`
+//	req             *tasks.Request `json:"-" doc:"false"`
+//}
+//
+//func (r *RetryExceptions) Describe() string {
+//	return "Retry Exception reties failed operations.\n" +
+//		"IgnoreExceptions will ignore failed operation occurred in this category. \n" +
+//		"RetryExceptions will retry failed operation occurred in this category. \n" +
+//		"RetryAttempts is the number of retry attempts.\n"
+//}
+//
+//func (r *RetryExceptions) Do() error {
+//	if r.req.ContextClosed() {
+//		return errors.New("req is cleared")
+//	}
+//
+//	c, e := r.Task.GetCollectionObject()
+//	if e != nil {
+//		r.Task.TearUp()
+//		return nil
+//	}
+//	r.Task.SetException(r.Exceptions)
+//	r.Task.PostTaskExceptionHandling(c)
+//	r.Task.TearUp()
+//	return nil
+//}
+//
+//func (r *RetryExceptions) Config(req *tasks.Request, reRun bool) (int64, error) {
+//	r.req = req
+//	if r.req == nil {
+//		return 0, err_sirius.RequestIsNil
+//	}
+//
+//	if r.req.Tasks == nil {
+//		return 0, fmt.Errorf("request.Task struct is nil")
+//	}
+//	for i := range r.req.Tasks {
+//		if bulkTask, ok := r.req.Tasks[i].Task.(BulkTask); ok {
+//			if ok, err_sirius := bulkTask.MatchResultSeed(r.ResultSeed); ok {
+//				if err_sirius != nil {
+//					return 0, err_sirius
+//				} else {
+//					r.Task = bulkTask
+//					break
+//				}
+//			}
+//		}
+//	}
+//
+//	if r.Task == nil {
+//		return 0, fmt.Errorf("no bulk loading task found for %s : %s", r.req.Identifier, r.ResultSeed)
+//	}
+//
+//	return r.Task.Config(req, true)
+//
+//}
+//
+//func (r *RetryExceptions) MetaDataIdentifier() string {
+//	return r.Task.MetaDataIdentifier()
+//}
+//
+//func (r *RetryExceptions) CheckIfPending() bool {
+//	return r.Task.CheckIfPending()
+//}
+//
+//func (r *RetryExceptions) PostTaskExceptionHandling(_ *cb_sdk.CollectionObject) {
+//
+//}
+//
+//func (r *RetryExceptions) TearUp() error {
+//	return r.Task.TearUp()
+//}
+//func (r *RetryExceptions) MatchResultSeed(resultSeed string) (bool, error) {
+//	return r.Task.MatchResultSeed(resultSeed)
+//}
+//
+//func (r *RetryExceptions) GetCollectionObject() (*cb_sdk.CollectionObject, error) {
+//	return r.Task.GetCollectionObject()
+//}
+//
+//func (r *RetryExceptions) SetException(exceptions Exceptions) {
+//	r.Task.SetException(r.Exceptions)
+//}
+//
+//func (r *RetryExceptions) GetOperationConfig() (*OperationConfig, *task_state.TaskState) {
+//	if r.Task != nil {
+//		return r.Task.GetOperationConfig()
+//	}
+//	return nil, nil
+//}
diff --git a/internal/tasks/bulk_loading/task_sub_doc_delete.go b/internal/tasks/bulk_loading/task_sub_doc_delete.go
new file mode 100644
index 0000000..c43f78c
--- /dev/null
+++ b/internal/tasks/bulk_loading/task_sub_doc_delete.go
@@ -0,0 +1,402 @@
+package bulk_loading
+
+//
+//import (
+//	"fmt"
+//	"github.com/couchbase/gocb/v2"
+//	"github.com/couchbaselabs/sirius/internal/cb_sdk"
+//	"github.com/couchbaselabs/sirius/internal/docgenerator"
+//	"github.com/couchbaselabs/sirius/internal/err_sirius"
+//	"github.com/couchbaselabs/sirius/internal/meta_data"
+//	"github.com/couchbaselabs/sirius/internal/task_result"
+//	"github.com/couchbaselabs/sirius/internal/task_state"
+//	"github.com/couchbaselabs/sirius/internal/tasks"
+//	"github.com/couchbaselabs/sirius/internal/template"
+//	"github.com/jaswdr/faker"
+//	"golang.org/x/sync/errgroup"
+//	"log"
+//	"math"
+//	"math/rand"
+//	"strings"
+//	"sync"
+//	"time"
+//)
+//
+//type SubDocDelete struct {
+//	IdentifierToken   string                        `json:"identifierToken" doc:"true"`
+//	ClusterConfig     *cb_sdk.ClusterConfig         `json:"clusterConfig" doc:"true"`
+//	Bucket            string                        `json:"bucket" doc:"true"`
+//	Scope             string                        `json:"scope,omitempty" doc:"true"`
+//	Collection        string                        `json:"collection,omitempty" doc:"true"`
+//	OperationConfig   *OperationConfig              `json:"operationConfig" doc:"true"`
+//	RemoveSpecOptions *cb_sdk.RemoveSpecOptions     `json:"removeSpecOptions" doc:"true"`
+//	MutateInOptions   *cb_sdk.MutateInOptions       `json:"mutateInOptions" doc:"true"`
+//	Operation         string                        `json:"operation" doc:"false"`
+//	ResultSeed        int64                         `json:"resultSeed" doc:"false"`
+//	TaskPending       bool                          `json:"taskPending" doc:"false"`
+//	State             *task_state.TaskState         `json:"State" doc:"false"`
+//	MetaData          *meta_data.CollectionMetaData `json:"metaData" doc:"false"`
+//	Result            *task_result.TaskResult       `json:"-" doc:"false"`
+//	gen               *docgenerator.Generator       `json:"-" doc:"false"`
+//	req               *tasks.Request                `json:"-" doc:"false"`
+//	rerun             bool                          `json:"-" doc:"false"`
+//	lock              sync.Mutex                    `json:"lock" doc:"false"`
+//}
+//
+//func (task *SubDocDelete) Describe() string {
+//	return " SubDocDelete deletes sub-documents in bulk"
+//}
+//
+//func (task *SubDocDelete) MetaDataIdentifier() string {
+//	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
+//	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
+//		task.Collection}, ":")
+//}
+//
+//func (task *SubDocDelete) CheckIfPending() bool {
+//	return task.TaskPending
+//}
+//
+//// Config configures  the insert task
+//func (task *SubDocDelete) Config(req *tasks.Request, reRun bool) (int64, error) {
+//	task.TaskPending = true
+//	task.req = req
+//
+//	if task.req == nil {
+//		task.TaskPending = false
+//		return 0, err_sirius.RequestIsNil
+//	}
+//
+//	task.req.ReconnectionManager()
+//	if _, err_sirius := task.req.GetCluster(task.ClusterConfig); err_sirius != nil {
+//		task.TaskPending = false
+//		return 0, err_sirius
+//	}
+//
+//	task.lock = sync.Mutex{}
+//	task.rerun = reRun
+//
+//	if !reRun {
+//		task.ResultSeed = int64(time.Now().UnixNano())
+//		task.Operation = tasks.SubDocDeleteOperation
+//
+//		if task.Bucket == "" {
+//			task.Bucket = cb_sdk.DefaultBucket
+//		}
+//		if task.Scope == "" {
+//			task.Scope = cb_sdk.DefaultScope
+//		}
+//		if task.Collection == "" {
+//			task.Collection = cb_sdk.DefaultCollection
+//		}
+//
+//		if err_sirius := ConfigureOperationConfig(task.OperationConfig); err_sirius != nil {
+//			task.TaskPending = false
+//			return 0, err_sirius
+//		}
+//
+//		if err_sirius := cb_sdk.ConfigRemoveSpecOptions(task.RemoveSpecOptions); err_sirius != nil {
+//			task.TaskPending = false
+//			return 0, err_sirius
+//		}
+//
+//		if err_sirius := cb_sdk.ConfigMutateInOptions(task.MutateInOptions); err_sirius != nil {
+//			task.TaskPending = false
+//			return 0, err_sirius
+//		}
+//
+//		task.MetaData = task.req.MetaData.GetCollectionMetadata(task.MetaDataIdentifier())
+//
+//		task.req.Lock()
+//		task.State = task_state.ConfigTaskState(task.MetaData.Seed, task.MetaData.SeedEnd, task.ResultSeed)
+//		task.req.Unlock()
+//
+//	} else {
+//		if task.State == nil {
+//			return task.ResultSeed, err_sirius.TaskStateIsNil
+//		}
+//		task.State.SetupStoringKeys()
+//		_ = task_result.DeleteResultFile(task.ResultSeed)
+//		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//	}
+//	return task.ResultSeed, nil
+//}
+//
+//func (task *SubDocDelete) TearUp() error {
+//	//Use this case to store task's state on disk when required
+//	//if err_sirius := task.State.SaveTaskSateOnDisk(); err_sirius != nil {
+//	//	log.Println("Error in storing TASK State on DISK")
+//	//}
+//	task.Result.StopStoringResult()
+//	if err_sirius := task.Result.SaveResultIntoFile(); err_sirius != nil {
+//		log.Println("not able to save Result into ", task.ResultSeed, task.Operation)
+//	}
+//	task.Result = nil
+//	task.State.StopStoringState()
+//	task.TaskPending = false
+//	return task.req.SaveRequestIntoFile()
+//}
+//
+//func (task *SubDocDelete) Do() error {
+//
+//	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
+//
+//	collectionObject, err1 := task.GetCollectionObject()
+//
+//	task.gen = docgenerator.ConfigGenerator(
+//		task.OperationConfig.KeySize,
+//		task.OperationConfig.DocSize,
+//		task.OperationConfig.DocType,
+//		task.OperationConfig.KeyPrefix,
+//		task.OperationConfig.KeySuffix,
+//		template.InitialiseTemplate(task.OperationConfig.TemplateName))
+//
+//	if err1 != nil {
+//		task.Result.ErrorOther = err1.Error()
+//		task.Result.FailWholeBulkOperation(task.OperationConfig.Start, task.OperationConfig.End,
+//			err1, task.State, task.gen, task.MetaData.Seed)
+//		return task.TearUp()
+//	}
+//
+//	deleteSubDocuments(task, collectionObject)
+//	task.Result.Success = (task.OperationConfig.End - task.OperationConfig.Start) - task.Result.Failure
+//
+//	return task.TearUp()
+//}
+//
+//// insertDocuments uploads new documents in a bucket.scope.collection in a defined batch size at multiple iterations.
+//func deleteSubDocuments(task *SubDocDelete, collectionObject *cb_sdk.CollectionObject) {
+//
+//	if task.req.ContextClosed() {
+//		return
+//	}
+//
+//	routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
+//	dataChannel := make(chan int64, tasks.MaxConcurrentRoutines)
+//
+//	skip := make(map[int64]struct{})
+//	for _, offset := range task.State.KeyStates.Completed {
+//		skip[offset] = struct{}{}
+//	}
+//	for _, offset := range task.State.KeyStates.Err {
+//		skip[offset] = struct{}{}
+//	}
+//	group := errgroup.Group{}
+//	for iteration := task.OperationConfig.Start; iteration < task.OperationConfig.End; iteration++ {
+//
+//		if task.req.ContextClosed() {
+//			close(routineLimiter)
+//			close(dataChannel)
+//			return
+//		}
+//
+//		routineLimiter <- struct{}{}
+//		dataChannel <- iteration
+//		group.Go(func() error {
+//			offset := <-dataChannel
+//			key := offset + task.MetaData.Seed
+//			docId := task.gen.BuildKey(key)
+//
+//			if _, ok := skip[offset]; ok {
+//				<-routineLimiter
+//				return fmt.Errorf("alreday performed operation on " + docId)
+//			}
+//			fake := faker.NewWithSeed(rand.NewSource(int64(key)))
+//
+//			var err_sirius error
+//			initTime := time.Now().UTC().Format(time.RFC850)
+//			for retry := 0; retry < int(math.Max(float64(1), float64(task.OperationConfig.Exceptions.
+//				RetryAttempts))); retry++ {
+//
+//				var iOps []gocb.MutateInSpec
+//				for path, _ := range task.gen.Template.GenerateSubPathAndValue(&fake, task.OperationConfig.DocSize) {
+//					iOps = append(iOps, gocb.RemoveSpec(path, &gocb.RemoveSpecOptions{
+//						IsXattr: task.RemoveSpecOptions.IsXattr,
+//					}))
+//				}
+//
+//				if !task.RemoveSpecOptions.IsXattr {
+//					iOps = append(iOps, gocb.IncrementSpec(template.MutatedPath,
+//						int64(template.MutateFieldIncrement), &gocb.CounterSpecOptions{
+//							CreatePath: false,
+//							IsXattr:    false,
+//						}))
+//				}
+//
+//				initTime = time.Now().UTC().Format(time.RFC850)
+//				_, err_sirius = collectionObject.Collection.MutateIn(docId, iOps, &gocb.MutateInOptions{
+//					Expiry:          time.Duration(task.MutateInOptions.Expiry) * time.Second,
+//					PersistTo:       task.MutateInOptions.PersistTo,
+//					ReplicateTo:     task.MutateInOptions.ReplicateTo,
+//					DurabilityLevel: cb_sdk.GetDurability(task.MutateInOptions.Durability),
+//					StoreSemantic:   cb_sdk.GetStoreSemantic(task.MutateInOptions.StoreSemantic),
+//					Timeout:         time.Duration(task.MutateInOptions.Expiry) * time.Second,
+//					PreserveExpiry:  task.MutateInOptions.PreserveExpiry,
+//				})
+//
+//				if err_sirius == nil {
+//					break
+//				}
+//			}
+//			if err_sirius != nil {
+//				task.Result.IncrementFailure(initTime, docId, err_sirius, false, 0, offset)
+//				task.State.StateChannel <- task_state.StateHelper{Status: task_state.ERR, Offset: offset}
+//				<-routineLimiter
+//				return err_sirius
+//
+//			}
+//
+//			task.State.StateChannel <- task_state.StateHelper{Status: task_state.COMPLETED, Offset: offset}
+//			<-routineLimiter
+//			return nil
+//		})
+//	}
+//
+//	_ = group.Wait()
+//	close(routineLimiter)
+//	close(dataChannel)
+//	task.PostTaskExceptionHandling(collectionObject)
+//	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//}
+//
+//func (task *SubDocDelete) PostTaskExceptionHandling(collectionObject *cb_sdk.CollectionObject) {
+//	task.Result.StopStoringResult()
+//	task.State.StopStoringState()
+//	if task.OperationConfig.Exceptions.RetryAttempts <= 0 {
+//		return
+//	}
+//
+//	// Get all the errorOffset
+//	errorOffsetMaps := task.State.ReturnErrOffset()
+//	// Get all the completed offset
+//	completedOffsetMaps := task.State.ReturnCompletedOffset()
+//
+//	// For the offset in ignore exceptions :-> move them from error to completed
+//	shiftErrToCompletedOnIgnore(task.OperationConfig.Exceptions.IgnoreExceptions, task.Result, errorOffsetMaps, completedOffsetMaps)
+//
+//	if task.OperationConfig.Exceptions.RetryAttempts > 0 {
+//
+//		exceptionList := GetExceptions(task.Result, task.OperationConfig.Exceptions.RetryExceptions)
+//
+//		// For the retry exceptions :-> move them on success after retrying from err_sirius to completed
+//		for _, exception := range exceptionList {
+//
+//			errorOffsetListMap := make([]map[int64]RetriedResult, 0)
+//			for _, failedDocs := range task.Result.BulkError[exception] {
+//				m := make(map[int64]RetriedResult)
+//				m[failedDocs.Offset] = RetriedResult{}
+//				errorOffsetListMap = append(errorOffsetListMap, m)
+//			}
+//
+//			routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
+//			dataChannel := make(chan map[int64]RetriedResult, tasks.MaxConcurrentRoutines)
+//			wg := errgroup.Group{}
+//			for _, x := range errorOffsetListMap {
+//				dataChannel <- x
+//				routineLimiter <- struct{}{}
+//				wg.Go(func() error {
+//					m := <-dataChannel
+//					var offset = int64(-1)
+//					for k, _ := range m {
+//						offset = k
+//					}
+//					key := offset + task.MetaData.Seed
+//					docId := task.gen.BuildKey(key)
+//					fake := faker.NewWithSeed(rand.NewSource(int64(key)))
+//
+//					retry := 0
+//					var err_sirius error
+//					result := &gocb.MutateInResult{}
+//					initTime := time.Now().UTC().Format(time.RFC850)
+//					for retry = 0; retry <= task.OperationConfig.Exceptions.RetryAttempts; retry++ {
+//
+//						var iOps []gocb.MutateInSpec
+//						for path, _ := range task.gen.Template.GenerateSubPathAndValue(&fake, task.OperationConfig.DocSize) {
+//							iOps = append(iOps, gocb.RemoveSpec(path, &gocb.RemoveSpecOptions{
+//								IsXattr: task.RemoveSpecOptions.IsXattr,
+//							}))
+//						}
+//
+//						if !task.RemoveSpecOptions.IsXattr {
+//							iOps = append(iOps, gocb.IncrementSpec(template.MutatedPath,
+//								int64(template.MutateFieldIncrement), &gocb.CounterSpecOptions{
+//									CreatePath: false,
+//									IsXattr:    false,
+//								}))
+//						}
+//
+//						initTime = time.Now().UTC().Format(time.RFC850)
+//						result, err_sirius = collectionObject.Collection.MutateIn(docId, iOps, &gocb.MutateInOptions{
+//							Expiry:          time.Duration(task.MutateInOptions.Expiry) * time.Second,
+//							PersistTo:       task.MutateInOptions.PersistTo,
+//							ReplicateTo:     task.MutateInOptions.ReplicateTo,
+//							DurabilityLevel: cb_sdk.GetDurability(task.MutateInOptions.Durability),
+//							StoreSemantic:   cb_sdk.GetStoreSemantic(task.MutateInOptions.StoreSemantic),
+//							Timeout:         time.Duration(task.MutateInOptions.Expiry) * time.Second,
+//							PreserveExpiry:  task.MutateInOptions.PreserveExpiry,
+//						})
+//
+//						if err_sirius == nil {
+//							break
+//						}
+//					}
+//
+//					if err_sirius != nil {
+//						m[offset] = RetriedResult{
+//							Status:   true,
+//							CAS:      0,
+//							InitTime: initTime,
+//							AckTime:  time.Now().UTC().Format(time.RFC850),
+//						}
+//					} else {
+//						m[offset] = RetriedResult{
+//							Status:   true,
+//							CAS:      uint64(result.Cas()),
+//							InitTime: initTime,
+//							AckTime:  time.Now().UTC().Format(time.RFC850),
+//						}
+//					}
+//
+//					<-routineLimiter
+//					return nil
+//				})
+//			}
+//			_ = wg.Wait()
+//
+//			shiftErrToCompletedOnRetrying(exception, task.Result, errorOffsetListMap, errorOffsetMaps, completedOffsetMaps)
+//		}
+//	}
+//
+//	task.State.MakeCompleteKeyFromMap(completedOffsetMaps)
+//	task.State.MakeErrorKeyFromMap(errorOffsetMaps)
+//	task.Result.Failure = int64(len(task.State.KeyStates.Err))
+//	log.Println("completed retrying:- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//}
+//
+//func (task *SubDocDelete) MatchResultSeed(resultSeed string) (bool, error) {
+//	defer task.lock.Unlock()
+//	task.lock.Lock()
+//	if fmt.Sprintf("%d", task.ResultSeed) == resultSeed {
+//		if task.TaskPending {
+//			return true, err_sirius.TaskInPendingState
+//		}
+//		if task.Result == nil {
+//			task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
+//		}
+//		return true, nil
+//	}
+//	return false, nil
+//}
+//
+//func (task *SubDocDelete) GetCollectionObject() (*cb_sdk.CollectionObject, error) {
+//	return task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope,
+//		task.Collection)
+//}
+//
+//func (task *SubDocDelete) SetException(exceptions Exceptions) {
+//	task.OperationConfig.Exceptions = exceptions
+//}
+//
+//func (task *SubDocDelete) GetOperationConfig() (*OperationConfig, *task_state.TaskState) {
+//	return task.OperationConfig, task.State
+//}
diff --git a/internal/tasks/bulk_loading/task_sub_doc_insert.go b/internal/tasks/bulk_loading/task_sub_doc_insert.go
new file mode 100644
index 0000000..d2825fe
--- /dev/null
+++ b/internal/tasks/bulk_loading/task_sub_doc_insert.go
@@ -0,0 +1,416 @@
+package bulk_loading
+
+//
+//import (
+//	"errors"
+//	"fmt"
+//	"github.com/couchbase/gocb/v2"
+//	"github.com/couchbaselabs/sirius/internal/cb_sdk"
+//	"github.com/couchbaselabs/sirius/internal/docgenerator"
+//	"github.com/couchbaselabs/sirius/internal/err_sirius"
+//	"github.com/couchbaselabs/sirius/internal/meta_data"
+//	"github.com/couchbaselabs/sirius/internal/task_result"
+//	"github.com/couchbaselabs/sirius/internal/task_state"
+//	"github.com/couchbaselabs/sirius/internal/tasks"
+//	"github.com/couchbaselabs/sirius/internal/template"
+//	"github.com/jaswdr/faker"
+//	"golang.org/x/sync/errgroup"
+//	"log"
+//	"math"
+//	"math/rand"
+//	"strings"
+//	"sync"
+//	"time"
+//)
+//
+//type SubDocInsert struct {
+//	IdentifierToken   string                        `json:"identifierToken" doc:"true"`
+//	ClusterConfig     *cb_sdk.ClusterConfig         `json:"clusterConfig" doc:"true"`
+//	Bucket            string                        `json:"bucket" doc:"true"`
+//	Scope             string                        `json:"scope,omitempty" doc:"true"`
+//	Collection        string                        `json:"collection,omitempty" doc:"true"`
+//	OperationConfig   *OperationConfig              `json:"operationConfig" doc:"true"`
+//	InsertSpecOptions *cb_sdk.InsertSpecOptions     `json:"insertSpecOptions" doc:"true"`
+//	MutateInOptions   *cb_sdk.MutateInOptions       `json:"mutateInOptions" doc:"true"`
+//	Operation         string                        `json:"operation" doc:"false"`
+//	ResultSeed        int64                         `json:"resultSeed" doc:"false"`
+//	TaskPending       bool                          `json:"taskPending" doc:"false"`
+//	State             *task_state.TaskState         `json:"State" doc:"false"`
+//	MetaData          *meta_data.CollectionMetaData `json:"metaData" doc:"false"`
+//	Result            *task_result.TaskResult       `json:"-" doc:"false"`
+//	gen               *docgenerator.Generator       `json:"-" doc:"false"`
+//	req               *tasks.Request                `json:"-" doc:"false"`
+//	rerun             bool                          `json:"-" doc:"false"`
+//	lock              sync.Mutex                    `json:"lock" doc:"false"`
+//}
+//
+//func (task *SubDocInsert) Describe() string {
+//	return " SubDocInsert inserts a Sub-Document"
+//}
+//
+//func (task *SubDocInsert) MetaDataIdentifier() string {
+//	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
+//	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
+//		task.Collection}, ":")
+//}
+//
+//func (task *SubDocInsert) CheckIfPending() bool {
+//	return task.TaskPending
+//}
+//
+//// Config configures  the insert task
+//func (task *SubDocInsert) Config(req *tasks.Request, reRun bool) (int64, error) {
+//	task.TaskPending = true
+//	task.req = req
+//
+//	if task.req == nil {
+//		task.TaskPending = false
+//		return 0, err_sirius.RequestIsNil
+//	}
+//
+//	task.req.ReconnectionManager()
+//	if _, err_sirius := task.req.GetCluster(task.ClusterConfig); err_sirius != nil {
+//		task.TaskPending = false
+//		return 0, err_sirius
+//	}
+//
+//	task.lock = sync.Mutex{}
+//	task.rerun = reRun
+//
+//	if !reRun {
+//		task.ResultSeed = int64(time.Now().UnixNano())
+//		task.Operation = tasks.SubDocInsertOperation
+//
+//		if task.Bucket == "" {
+//			task.Bucket = cb_sdk.DefaultBucket
+//		}
+//		if task.Scope == "" {
+//			task.Scope = cb_sdk.DefaultScope
+//		}
+//		if task.Collection == "" {
+//			task.Collection = cb_sdk.DefaultCollection
+//		}
+//
+//		if err_sirius := ConfigureOperationConfig(task.OperationConfig); err_sirius != nil {
+//			task.TaskPending = false
+//			return 0, err_sirius
+//		}
+//
+//		if err_sirius := cb_sdk.ConfigInsertSpecOptions(task.InsertSpecOptions); err_sirius != nil {
+//			task.TaskPending = false
+//			return 0, err_sirius
+//		}
+//
+//		if err_sirius := cb_sdk.ConfigMutateInOptions(task.MutateInOptions); err_sirius != nil {
+//			task.TaskPending = false
+//			return 0, err_sirius
+//		}
+//
+//		task.MetaData = task.req.MetaData.GetCollectionMetadata(task.MetaDataIdentifier())
+//
+//		task.req.Lock()
+//		if task.OperationConfig.End+task.MetaData.Seed > task.MetaData.SeedEnd {
+//			task.req.AddToSeedEnd(task.MetaData, (task.OperationConfig.End+task.MetaData.Seed)-(task.MetaData.SeedEnd))
+//		}
+//		log.Println(task.MetaData)
+//		task.State = task_state.ConfigTaskState(task.MetaData.Seed, task.MetaData.SeedEnd, task.ResultSeed)
+//		task.req.Unlock()
+//
+//	} else {
+//		if task.State == nil {
+//			return task.ResultSeed, err_sirius.TaskStateIsNil
+//		}
+//		task.State.SetupStoringKeys()
+//		_ = task_result.DeleteResultFile(task.ResultSeed)
+//		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//	}
+//	return task.ResultSeed, nil
+//}
+//
+//func (task *SubDocInsert) TearUp() error {
+//	task.Result.StopStoringResult()
+//	//Use this case to store task's state on disk when required
+//	//if err_sirius := task.State.SaveTaskSateOnDisk(); err_sirius != nil {
+//	//	log.Println("Error in storing TASK State on DISK")
+//	//}
+//	if err_sirius := task.Result.SaveResultIntoFile(); err_sirius != nil {
+//		log.Println("not able to save Result into ", task.ResultSeed, task.Operation)
+//	}
+//	task.Result = nil
+//	task.State.StopStoringState()
+//	task.TaskPending = false
+//	return task.req.SaveRequestIntoFile()
+//}
+//
+//func (task *SubDocInsert) Do() error {
+//
+//	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
+//
+//	collectionObject, err1 := task.GetCollectionObject()
+//
+//	task.gen = docgenerator.ConfigGenerator(
+//		task.OperationConfig.KeySize,
+//		task.OperationConfig.DocSize,
+//		task.OperationConfig.DocType,
+//		task.OperationConfig.KeyPrefix,
+//		task.OperationConfig.KeySuffix,
+//		template.InitialiseTemplate(task.OperationConfig.TemplateName))
+//
+//	if err1 != nil {
+//		task.Result.ErrorOther = err1.Error()
+//		task.Result.FailWholeBulkOperation(task.OperationConfig.Start, task.OperationConfig.End,
+//			err1, task.State, task.gen, task.MetaData.Seed)
+//		return task.TearUp()
+//	}
+//
+//	insertSubDocuments(task, collectionObject)
+//	task.Result.Success = (task.OperationConfig.End - task.OperationConfig.Start) - task.Result.Failure
+//
+//	return task.TearUp()
+//}
+//
+//// insertDocuments uploads new documents in a bucket.scope.collection in a defined batch size at multiple iterations.
+//func insertSubDocuments(task *SubDocInsert, collectionObject *cb_sdk.CollectionObject) {
+//
+//	if task.req.ContextClosed() {
+//		return
+//	}
+//
+//	routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
+//	dataChannel := make(chan int64, tasks.MaxConcurrentRoutines)
+//
+//	skip := make(map[int64]struct{})
+//	for _, offset := range task.State.KeyStates.Completed {
+//		skip[offset] = struct{}{}
+//	}
+//	for _, offset := range task.State.KeyStates.Err {
+//		skip[offset] = struct{}{}
+//	}
+//	group := errgroup.Group{}
+//	for iteration := task.OperationConfig.Start; iteration < task.OperationConfig.End; iteration++ {
+//
+//		if task.req.ContextClosed() {
+//			close(routineLimiter)
+//			close(dataChannel)
+//			return
+//		}
+//
+//		routineLimiter <- struct{}{}
+//		dataChannel <- iteration
+//		group.Go(func() error {
+//			offset := <-dataChannel
+//			key := offset + task.MetaData.Seed
+//			docId := task.gen.BuildKey(key)
+//
+//			if _, ok := skip[offset]; ok {
+//				<-routineLimiter
+//				return fmt.Errorf("alreday performed operation on " + docId)
+//			}
+//			fake := faker.NewWithSeed(rand.NewSource(int64(key)))
+//
+//			var err_sirius error
+//			initTime := time.Now().UTC().Format(time.RFC850)
+//			for retry := 0; retry < int(math.Max(float64(1), float64(task.OperationConfig.Exceptions.
+//				RetryAttempts))); retry++ {
+//
+//				var iOps []gocb.MutateInSpec
+//				for path, value := range task.gen.Template.GenerateSubPathAndValue(&fake, task.OperationConfig.DocSize) {
+//					iOps = append(iOps, gocb.InsertSpec(path, value, &gocb.InsertSpecOptions{
+//						CreatePath: task.InsertSpecOptions.CreatePath,
+//						IsXattr:    task.InsertSpecOptions.IsXattr,
+//					}))
+//				}
+//
+//				if !task.InsertSpecOptions.IsXattr {
+//					iOps = append(iOps, gocb.IncrementSpec(template.MutatedPath,
+//						int64(template.MutateFieldIncrement), &gocb.CounterSpecOptions{
+//							CreatePath: true,
+//							IsXattr:    false,
+//						}))
+//				}
+//
+//				initTime = time.Now().UTC().Format(time.RFC850)
+//				_, err_sirius = collectionObject.Collection.MutateIn(docId, iOps, &gocb.MutateInOptions{
+//					Expiry:          time.Duration(task.MutateInOptions.Expiry) * time.Second,
+//					PersistTo:       task.MutateInOptions.PersistTo,
+//					ReplicateTo:     task.MutateInOptions.ReplicateTo,
+//					DurabilityLevel: cb_sdk.GetDurability(task.MutateInOptions.Durability),
+//					StoreSemantic:   cb_sdk.GetStoreSemantic(task.MutateInOptions.StoreSemantic),
+//					Timeout:         time.Duration(task.MutateInOptions.Expiry) * time.Second,
+//					PreserveExpiry:  task.MutateInOptions.PreserveExpiry,
+//				})
+//
+//				if err_sirius == nil {
+//					break
+//				}
+//			}
+//			if err_sirius != nil {
+//				if errors.Is(err_sirius, gocb.ErrPathExists) && task.rerun {
+//					task.State.StateChannel <- task_state.StateHelper{Status: task_state.COMPLETED, Offset: offset}
+//					<-routineLimiter
+//					return nil
+//				} else {
+//					task.Result.IncrementFailure(initTime, docId, err_sirius, false, 0, offset)
+//					task.State.StateChannel <- task_state.StateHelper{Status: task_state.ERR, Offset: offset}
+//					<-routineLimiter
+//					return err_sirius
+//				}
+//			}
+//
+//			task.State.StateChannel <- task_state.StateHelper{Status: task_state.COMPLETED, Offset: offset}
+//			<-routineLimiter
+//			return nil
+//		})
+//	}
+//
+//	_ = group.Wait()
+//	close(routineLimiter)
+//	close(dataChannel)
+//	task.PostTaskExceptionHandling(collectionObject)
+//	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//}
+//
+//func (task *SubDocInsert) PostTaskExceptionHandling(collectionObject *cb_sdk.CollectionObject) {
+//	task.Result.StopStoringResult()
+//	task.State.StopStoringState()
+//	if task.OperationConfig.Exceptions.RetryAttempts <= 0 {
+//		return
+//	}
+//
+//	// Get all the errorOffset
+//	errorOffsetMaps := task.State.ReturnErrOffset()
+//	// Get all the completed offset
+//	completedOffsetMaps := task.State.ReturnCompletedOffset()
+//
+//	// For the offset in ignore exceptions :-> move them from error to completed
+//	shiftErrToCompletedOnIgnore(task.OperationConfig.Exceptions.IgnoreExceptions, task.Result, errorOffsetMaps, completedOffsetMaps)
+//
+//	if task.OperationConfig.Exceptions.RetryAttempts > 0 {
+//
+//		exceptionList := GetExceptions(task.Result, task.OperationConfig.Exceptions.RetryExceptions)
+//
+//		// For the retry exceptions :-> move them on success after retrying from err_sirius to completed
+//		for _, exception := range exceptionList {
+//
+//			errorOffsetListMap := make([]map[int64]RetriedResult, 0)
+//			for _, failedDocs := range task.Result.BulkError[exception] {
+//				m := make(map[int64]RetriedResult)
+//				m[failedDocs.Offset] = RetriedResult{}
+//				errorOffsetListMap = append(errorOffsetListMap, m)
+//			}
+//
+//			routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
+//			dataChannel := make(chan map[int64]RetriedResult, tasks.MaxConcurrentRoutines)
+//			wg := errgroup.Group{}
+//			for _, x := range errorOffsetListMap {
+//				dataChannel <- x
+//				routineLimiter <- struct{}{}
+//				wg.Go(func() error {
+//					m := <-dataChannel
+//					var offset = int64(-1)
+//					for k, _ := range m {
+//						offset = k
+//					}
+//					key := offset + task.MetaData.Seed
+//					docId := task.gen.BuildKey(key)
+//					fake := faker.NewWithSeed(rand.NewSource(int64(key)))
+//					retry := 0
+//					var err_sirius error
+//
+//					result := &gocb.MutateInResult{}
+//					initTime := time.Now().UTC().Format(time.RFC850)
+//					for retry = 0; retry <= task.OperationConfig.Exceptions.RetryAttempts; retry++ {
+//
+//						var iOps []gocb.MutateInSpec
+//						for path, value := range task.gen.Template.GenerateSubPathAndValue(&fake,
+//							task.OperationConfig.DocSize) {
+//							iOps = append(iOps, gocb.InsertSpec(path, value, &gocb.InsertSpecOptions{
+//								CreatePath: task.InsertSpecOptions.CreatePath,
+//								IsXattr:    task.InsertSpecOptions.IsXattr,
+//							}))
+//						}
+//
+//						if !task.InsertSpecOptions.IsXattr {
+//							iOps = append(iOps, gocb.IncrementSpec(template.MutatedPath,
+//								int64(template.MutateFieldIncrement), &gocb.CounterSpecOptions{
+//									CreatePath: true,
+//									IsXattr:    false,
+//								}))
+//						}
+//
+//						initTime = time.Now().UTC().Format(time.RFC850)
+//						result, err_sirius = collectionObject.Collection.MutateIn(docId, iOps, &gocb.MutateInOptions{
+//							Expiry:          time.Duration(task.MutateInOptions.Expiry) * time.Second,
+//							PersistTo:       task.MutateInOptions.PersistTo,
+//							ReplicateTo:     task.MutateInOptions.ReplicateTo,
+//							DurabilityLevel: cb_sdk.GetDurability(task.MutateInOptions.Durability),
+//							StoreSemantic:   cb_sdk.GetStoreSemantic(task.MutateInOptions.StoreSemantic),
+//							Timeout:         time.Duration(task.MutateInOptions.Expiry) * time.Second,
+//							PreserveExpiry:  task.MutateInOptions.PreserveExpiry,
+//						})
+//
+//						if err_sirius == nil {
+//							break
+//						}
+//					}
+//
+//					if err_sirius != nil {
+//						m[offset] = RetriedResult{
+//							Status:   false,
+//							CAS:      0,
+//							InitTime: initTime,
+//							AckTime:  time.Now().UTC().Format(time.RFC850),
+//						}
+//					} else {
+//						m[offset] = RetriedResult{
+//							Status:   true,
+//							CAS:      uint64(result.Cas()),
+//							InitTime: initTime,
+//							AckTime:  time.Now().UTC().Format(time.RFC850),
+//						}
+//					}
+//
+//					<-routineLimiter
+//					return nil
+//				})
+//			}
+//			_ = wg.Wait()
+//
+//			shiftErrToCompletedOnRetrying(exception, task.Result, errorOffsetListMap, errorOffsetMaps, completedOffsetMaps)
+//		}
+//	}
+//
+//	task.State.MakeCompleteKeyFromMap(completedOffsetMaps)
+//	task.State.MakeErrorKeyFromMap(errorOffsetMaps)
+//	task.Result.Failure = int64(len(task.State.KeyStates.Err))
+//	log.Println("completed retrying:- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//
+//}
+//
+//func (task *SubDocInsert) MatchResultSeed(resultSeed string) (bool, error) {
+//	defer task.lock.Unlock()
+//	task.lock.Lock()
+//	if fmt.Sprintf("%d", task.ResultSeed) == resultSeed {
+//		if task.TaskPending {
+//			return true, err_sirius.TaskInPendingState
+//		}
+//		if task.Result == nil {
+//			task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
+//		}
+//		return true, nil
+//	}
+//	return false, nil
+//}
+//
+//func (task *SubDocInsert) GetCollectionObject() (*cb_sdk.CollectionObject, error) {
+//	return task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope,
+//		task.Collection)
+//}
+//
+//func (task *SubDocInsert) SetException(exceptions Exceptions) {
+//	task.OperationConfig.Exceptions = exceptions
+//}
+//
+//func (task *SubDocInsert) GetOperationConfig() (*OperationConfig, *task_state.TaskState) {
+//	return task.OperationConfig, task.State
+//}
diff --git a/internal/tasks/bulk_loading/task_sub_doc_read.go b/internal/tasks/bulk_loading/task_sub_doc_read.go
new file mode 100644
index 0000000..e88962f
--- /dev/null
+++ b/internal/tasks/bulk_loading/task_sub_doc_read.go
@@ -0,0 +1,390 @@
+package bulk_loading
+
+//
+//import (
+//	"fmt"
+//	"github.com/couchbase/gocb/v2"
+//	"github.com/couchbaselabs/sirius/internal/cb_sdk"
+//	"github.com/couchbaselabs/sirius/internal/docgenerator"
+//	"github.com/couchbaselabs/sirius/internal/err_sirius"
+//	"github.com/couchbaselabs/sirius/internal/meta_data"
+//	"github.com/couchbaselabs/sirius/internal/task_result"
+//	"github.com/couchbaselabs/sirius/internal/task_state"
+//	"github.com/couchbaselabs/sirius/internal/tasks"
+//	"github.com/couchbaselabs/sirius/internal/template"
+//	"github.com/jaswdr/faker"
+//	"golang.org/x/sync/errgroup"
+//	"log"
+//	"math"
+//	"math/rand"
+//	"strings"
+//	"sync"
+//	"time"
+//)
+//
+//type SubDocRead struct {
+//	IdentifierToken string                        `json:"identifierToken" doc:"true"`
+//	ClusterConfig   *cb_sdk.ClusterConfig         `json:"clusterConfig" doc:"true"`
+//	Bucket          string                        `json:"bucket" doc:"true"`
+//	Scope           string                        `json:"scope,omitempty" doc:"true"`
+//	Collection      string                        `json:"collection,omitempty" doc:"true"`
+//	OperationConfig *OperationConfig              `json:"operationConfig" doc:"true"`
+//	GetSpecOptions  *cb_sdk.GetSpecOptions        `json:"getSpecOptions" doc:"true"`
+//	LookupInOptions *cb_sdk.LookupInOptions       `json:"lookupInOptions" doc:"true"`
+//	Operation       string                        `json:"operation" doc:"false"`
+//	ResultSeed      int64                         `json:"resultSeed" doc:"false"`
+//	TaskPending     bool                          `json:"taskPending" doc:"false"`
+//	State           *task_state.TaskState         `json:"State" doc:"false"`
+//	MetaData        *meta_data.CollectionMetaData `json:"metaData" doc:"false"`
+//	Result          *task_result.TaskResult       `json:"-" doc:"false"`
+//	gen             *docgenerator.Generator       `json:"-" doc:"false"`
+//	req             *tasks.Request                `json:"-" doc:"false"`
+//	rerun           bool                          `json:"-" doc:"false"`
+//	lock            sync.Mutex                    `json:"-" doc:"false"`
+//}
+//
+//func (task *SubDocRead) Describe() string {
+//	return " SubDocRead reads sub-document in bulk"
+//}
+//
+//func (task *SubDocRead) MetaDataIdentifier() string {
+//	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
+//	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
+//		task.Collection}, ":")
+//}
+//
+//func (task *SubDocRead) CheckIfPending() bool {
+//	return task.TaskPending
+//}
+//
+//// Config configures  the insert task
+//func (task *SubDocRead) Config(req *tasks.Request, reRun bool) (int64, error) {
+//	task.TaskPending = true
+//	task.req = req
+//
+//	if task.req == nil {
+//		task.TaskPending = false
+//		return 0, err_sirius.RequestIsNil
+//	}
+//
+//	task.req.ReconnectionManager()
+//	if _, err_sirius := task.req.GetCluster(task.ClusterConfig); err_sirius != nil {
+//		task.TaskPending = false
+//		return 0, err_sirius
+//	}
+//
+//	task.lock = sync.Mutex{}
+//	task.rerun = reRun
+//
+//	if !reRun {
+//		task.ResultSeed = int64(time.Now().UnixNano())
+//		task.Operation = tasks.SubDocReadOperation
+//
+//		if task.Bucket == "" {
+//			task.Bucket = cb_sdk.DefaultBucket
+//		}
+//		if task.Scope == "" {
+//			task.Scope = cb_sdk.DefaultScope
+//		}
+//		if task.Collection == "" {
+//			task.Collection = cb_sdk.DefaultCollection
+//		}
+//
+//		if err_sirius := ConfigureOperationConfig(task.OperationConfig); err_sirius != nil {
+//			task.TaskPending = false
+//			return 0, err_sirius
+//		}
+//
+//		task.MetaData = task.req.MetaData.GetCollectionMetadata(task.MetaDataIdentifier())
+//
+//		task.req.Lock()
+//		task.State = task_state.ConfigTaskState(task.MetaData.Seed, task.MetaData.SeedEnd, task.ResultSeed)
+//		task.req.Unlock()
+//
+//	} else {
+//		if task.State == nil {
+//			return task.ResultSeed, err_sirius.TaskStateIsNil
+//		}
+//		task.State.SetupStoringKeys()
+//		_ = task_result.DeleteResultFile(task.ResultSeed)
+//		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//	}
+//	return task.ResultSeed, nil
+//}
+//
+//func (task *SubDocRead) TearUp() error {
+//	//Use this case to store task's state on disk when required
+//	//if err_sirius := task.State.SaveTaskSateOnDisk(); err_sirius != nil {
+//	//	log.Println("Error in storing TASK State on DISK")
+//	//}
+//	task.Result.StopStoringResult()
+//	if err_sirius := task.Result.SaveResultIntoFile(); err_sirius != nil {
+//		log.Println("not able to save Result into ", task.ResultSeed, task.Operation)
+//	}
+//	task.Result = nil
+//	task.TaskPending = false
+//	return task.req.SaveRequestIntoFile()
+//}
+//
+//func (task *SubDocRead) Do() error {
+//
+//	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
+//
+//	collectionObject, err1 := task.GetCollectionObject()
+//
+//	task.gen = docgenerator.ConfigGenerator(
+//		task.OperationConfig.KeySize,
+//		task.OperationConfig.DocSize,
+//		task.OperationConfig.DocType,
+//		task.OperationConfig.KeyPrefix,
+//		task.OperationConfig.KeySuffix,
+//		template.InitialiseTemplate(task.OperationConfig.TemplateName))
+//
+//	if err1 != nil {
+//		task.Result.ErrorOther = err1.Error()
+//		task.Result.FailWholeBulkOperation(task.OperationConfig.Start, task.OperationConfig.End,
+//			err1, task.State, task.gen, task.MetaData.Seed)
+//		return task.TearUp()
+//	}
+//
+//	readSubDocuments(task, collectionObject)
+//	task.Result.Success = (task.OperationConfig.End - task.OperationConfig.Start) - task.Result.Failure
+//
+//	return task.TearUp()
+//}
+//
+//// insertDocuments uploads new documents in a bucket.scope.collection in a defined batch size at multiple iterations.
+//func readSubDocuments(task *SubDocRead, collectionObject *cb_sdk.CollectionObject) {
+//
+//	if task.req.ContextClosed() {
+//		return
+//	}
+//
+//	routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
+//	dataChannel := make(chan int64, tasks.MaxConcurrentRoutines)
+//
+//	skip := make(map[int64]struct{})
+//	for _, offset := range task.State.KeyStates.Completed {
+//		skip[offset] = struct{}{}
+//	}
+//	for _, offset := range task.State.KeyStates.Err {
+//		skip[offset] = struct{}{}
+//	}
+//	group := errgroup.Group{}
+//	for iteration := task.OperationConfig.Start; iteration < task.OperationConfig.End; iteration++ {
+//
+//		if task.req.ContextClosed() {
+//			close(routineLimiter)
+//			close(dataChannel)
+//			return
+//		}
+//
+//		routineLimiter <- struct{}{}
+//		dataChannel <- iteration
+//		group.Go(func() error {
+//			offset := <-dataChannel
+//			key := offset + task.MetaData.Seed
+//			docId := task.gen.BuildKey(key)
+//
+//			if _, ok := skip[offset]; ok {
+//				<-routineLimiter
+//				return fmt.Errorf("alreday performed operation on " + docId)
+//			}
+//
+//			fake := faker.NewWithSeed(rand.NewSource(int64(key)))
+//
+//			var err_sirius error
+//			result := &gocb.LookupInResult{}
+//			var paths []string
+//			initTime := time.Now().UTC().Format(time.RFC850)
+//			for retry := 0; retry < int(math.Max(float64(1), float64(task.OperationConfig.Exceptions.
+//				RetryAttempts))); retry++ {
+//
+//				var iOps []gocb.LookupInSpec
+//				for path, _ := range task.gen.Template.GenerateSubPathAndValue(&fake, task.OperationConfig.DocSize) {
+//					paths = append(paths, path)
+//					iOps = append(iOps, gocb.GetSpec(path, &gocb.GetSpecOptions{
+//						IsXattr: task.GetSpecOptions.IsXattr,
+//					}))
+//				}
+//
+//				initTime = time.Now().UTC().Format(time.RFC850)
+//				result, err_sirius = collectionObject.Collection.LookupIn(docId, iOps, &gocb.LookupInOptions{
+//					Timeout: time.Duration(task.LookupInOptions.Timeout) * time.Second,
+//				})
+//
+//				if err_sirius == nil {
+//					break
+//				}
+//			}
+//			if err_sirius != nil {
+//				task.Result.IncrementFailure(initTime, docId, err_sirius, false, 0, offset)
+//				task.State.StateChannel <- task_state.StateHelper{Status: task_state.ERR, Offset: offset}
+//				<-routineLimiter
+//				return err_sirius
+//			}
+//
+//			for index, _ := range paths {
+//				var val interface{}
+//				if err_sirius := result.ContentAt(uint(index), &val); err_sirius != nil {
+//					task.Result.IncrementFailure(initTime, docId, err_sirius, false, 0, offset)
+//					task.State.StateChannel <- task_state.StateHelper{Status: task_state.ERR, Offset: offset}
+//					<-routineLimiter
+//					return err_sirius
+//				}
+//			}
+//
+//			task.State.StateChannel <- task_state.StateHelper{Status: task_state.COMPLETED, Offset: offset}
+//			<-routineLimiter
+//			return nil
+//		})
+//	}
+//
+//	_ = group.Wait()
+//	close(routineLimiter)
+//	close(dataChannel)
+//	task.PostTaskExceptionHandling(collectionObject)
+//	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//}
+//
+//func (task *SubDocRead) PostTaskExceptionHandling(collectionObject *cb_sdk.CollectionObject) {
+//	task.Result.StopStoringResult()
+//	task.State.StopStoringState()
+//
+//	if task.OperationConfig.Exceptions.RetryAttempts <= 0 {
+//		return
+//	}
+//
+//	// Get all the errorOffset
+//	errorOffsetMaps := task.State.ReturnErrOffset()
+//	// Get all the completed offset
+//	completedOffsetMaps := task.State.ReturnCompletedOffset()
+//
+//	// For the offset in ignore exceptions :-> move them from error to completed
+//	shiftErrToCompletedOnIgnore(task.OperationConfig.Exceptions.IgnoreExceptions, task.Result, errorOffsetMaps, completedOffsetMaps)
+//
+//	if task.OperationConfig.Exceptions.RetryAttempts > 0 {
+//
+//		exceptionList := GetExceptions(task.Result, task.OperationConfig.Exceptions.RetryExceptions)
+//
+//		// For the retry exceptions :-> move them on success after retrying from err_sirius to completed
+//		for _, exception := range exceptionList {
+//
+//			errorOffsetListMap := make([]map[int64]RetriedResult, 0)
+//			for _, failedDocs := range task.Result.BulkError[exception] {
+//				m := make(map[int64]RetriedResult)
+//				m[failedDocs.Offset] = RetriedResult{}
+//				errorOffsetListMap = append(errorOffsetListMap, m)
+//			}
+//
+//			routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
+//			dataChannel := make(chan map[int64]RetriedResult, tasks.MaxConcurrentRoutines)
+//			wg := errgroup.Group{}
+//			for _, x := range errorOffsetListMap {
+//				dataChannel <- x
+//				routineLimiter <- struct{}{}
+//				wg.Go(func() error {
+//					m := <-dataChannel
+//					var offset = int64(-1)
+//					for k, _ := range m {
+//						offset = k
+//					}
+//					key := offset + task.MetaData.Seed
+//					docId := task.gen.BuildKey(key)
+//					fake := faker.NewWithSeed(rand.NewSource(int64(key)))
+//
+//					retry := 0
+//					var err_sirius error
+//					result := &gocb.LookupInResult{}
+//					var paths []string
+//
+//					for retry = 0; retry <= task.OperationConfig.Exceptions.RetryAttempts; retry++ {
+//
+//						var iOps []gocb.LookupInSpec
+//						for path, _ := range task.gen.Template.GenerateSubPathAndValue(&fake,
+//							task.OperationConfig.DocSize) {
+//
+//							paths = append(paths, path)
+//
+//							iOps = append(iOps, gocb.GetSpec(path, &gocb.GetSpecOptions{
+//								IsXattr: task.GetSpecOptions.IsXattr,
+//							}))
+//						}
+//
+//						_, err_sirius = collectionObject.Collection.LookupIn(docId, iOps, &gocb.LookupInOptions{
+//							Timeout: time.Duration(task.LookupInOptions.Timeout) * time.Second,
+//						})
+//
+//						if err_sirius == nil {
+//							break
+//						}
+//					}
+//
+//					if err_sirius != nil {
+//						m[offset] = RetriedResult{
+//							Status: false,
+//							CAS:    0,
+//						}
+//					} else {
+//						for index, _ := range paths {
+//							var val interface{}
+//							if err_sirius := result.ContentAt(uint(index), &val); err_sirius != nil {
+//								m[offset] = RetriedResult{
+//									Status: false,
+//									CAS:    0,
+//								}
+//								<-routineLimiter
+//								return nil
+//							}
+//						}
+//
+//						m[offset] = RetriedResult{
+//							Status: true,
+//							CAS:    uint64(result.Cas()),
+//						}
+//					}
+//
+//					<-routineLimiter
+//					return nil
+//				})
+//			}
+//			_ = wg.Wait()
+//
+//			shiftErrToCompletedOnRetrying(exception, task.Result, errorOffsetListMap, errorOffsetMaps, completedOffsetMaps)
+//		}
+//	}
+//
+//	task.State.MakeCompleteKeyFromMap(completedOffsetMaps)
+//	task.State.MakeErrorKeyFromMap(errorOffsetMaps)
+//	task.Result.Failure = int64(len(task.State.KeyStates.Err))
+//	log.Println("completed retrying:- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//
+//}
+//
+//func (task *SubDocRead) MatchResultSeed(resultSeed string) (bool, error) {
+//	defer task.lock.Unlock()
+//	task.lock.Lock()
+//	if fmt.Sprintf("%d", task.ResultSeed) == resultSeed {
+//		if task.TaskPending {
+//			return true, err_sirius.TaskInPendingState
+//		}
+//		if task.Result == nil {
+//			task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
+//		}
+//		return true, nil
+//	}
+//	return false, nil
+//}
+//
+//func (task *SubDocRead) GetCollectionObject() (*cb_sdk.CollectionObject, error) {
+//	return task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope,
+//		task.Collection)
+//}
+//
+//func (task *SubDocRead) SetException(exceptions Exceptions) {
+//	task.OperationConfig.Exceptions = exceptions
+//}
+//
+//func (task *SubDocRead) GetOperationConfig() (*OperationConfig, *task_state.TaskState) {
+//	return task.OperationConfig, task.State
+//}
diff --git a/internal/tasks/bulk_loading/task_sub_doc_replace.go b/internal/tasks/bulk_loading/task_sub_doc_replace.go
new file mode 100644
index 0000000..b7191e8
--- /dev/null
+++ b/internal/tasks/bulk_loading/task_sub_doc_replace.go
@@ -0,0 +1,407 @@
+package bulk_loading
+
+//
+//import (
+//	"fmt"
+//	"github.com/couchbase/gocb/v2"
+//	"github.com/couchbaselabs/sirius/internal/cb_sdk"
+//	"github.com/couchbaselabs/sirius/internal/docgenerator"
+//	"github.com/couchbaselabs/sirius/internal/err_sirius"
+//	"github.com/couchbaselabs/sirius/internal/meta_data"
+//	"github.com/couchbaselabs/sirius/internal/task_result"
+//	"github.com/couchbaselabs/sirius/internal/task_state"
+//	"github.com/couchbaselabs/sirius/internal/tasks"
+//	"github.com/couchbaselabs/sirius/internal/template"
+//	"github.com/jaswdr/faker"
+//	"golang.org/x/sync/errgroup"
+//	"log"
+//	"math"
+//	"math/rand"
+//	"strings"
+//	"sync"
+//	"time"
+//)
+//
+//type SubDocReplace struct {
+//	IdentifierToken    string                        `json:"identifierToken" doc:"true"`
+//	ClusterConfig      *cb_sdk.ClusterConfig         `json:"clusterConfig" doc:"true"`
+//	Bucket             string                        `json:"bucket" doc:"true"`
+//	Scope              string                        `json:"scope,omitempty" doc:"true"`
+//	Collection         string                        `json:"collection,omitempty" doc:"true"`
+//	OperationConfig    *OperationConfig              `json:"operationConfig" doc:"true"`
+//	ReplaceSpecOptions *cb_sdk.ReplaceSpecOptions    `json:"replaceSpecOptions" doc:"true"`
+//	MutateInOptions    *cb_sdk.MutateInOptions       `json:"mutateInOptions" doc:"true"`
+//	Operation          string                        `json:"operation" doc:"false"`
+//	ResultSeed         int64                         `json:"resultSeed" doc:"false"`
+//	TaskPending        bool                          `json:"taskPending" doc:"false"`
+//	State              *task_state.TaskState         `json:"State" doc:"false"`
+//	MetaData           *meta_data.CollectionMetaData `json:"metaData" doc:"false"`
+//	Result             *task_result.TaskResult       `json:"-" doc:"false"`
+//	gen                *docgenerator.Generator       `json:"-" doc:"false"`
+//	req                *tasks.Request                `json:"-" doc:"false"`
+//	rerun              bool                          `json:"-" doc:"false"`
+//	lock               sync.Mutex                    `json:"-" doc:"false"`
+//}
+//
+//func (task *SubDocReplace) Describe() string {
+//	return " SubDocReplace upserts a Sub-Document"
+//}
+//
+//func (task *SubDocReplace) MetaDataIdentifier() string {
+//	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
+//	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
+//		task.Collection}, ":")
+//}
+//
+//func (task *SubDocReplace) CheckIfPending() bool {
+//	return task.TaskPending
+//}
+//
+//// Config configures  the insert task
+//func (task *SubDocReplace) Config(req *tasks.Request, reRun bool) (int64, error) {
+//	task.TaskPending = true
+//	task.req = req
+//
+//	if task.req == nil {
+//		task.TaskPending = false
+//		return 0, err_sirius.RequestIsNil
+//	}
+//
+//	task.req.ReconnectionManager()
+//	if _, err_sirius := task.req.GetCluster(task.ClusterConfig); err_sirius != nil {
+//		task.TaskPending = false
+//		return 0, err_sirius
+//	}
+//
+//	task.lock = sync.Mutex{}
+//	task.rerun = reRun
+//
+//	if !reRun {
+//		task.ResultSeed = int64(time.Now().UnixNano())
+//		task.Operation = tasks.SubDocReplaceOperation
+//
+//		if task.Bucket == "" {
+//			task.Bucket = cb_sdk.DefaultBucket
+//		}
+//		if task.Scope == "" {
+//			task.Scope = cb_sdk.DefaultScope
+//		}
+//		if task.Collection == "" {
+//			task.Collection = cb_sdk.DefaultCollection
+//		}
+//
+//		if err_sirius := ConfigureOperationConfig(task.OperationConfig); err_sirius != nil {
+//			task.TaskPending = false
+//			return 0, err_sirius
+//		}
+//
+//		if err_sirius := cb_sdk.ConfigReplaceSpecOptions(task.ReplaceSpecOptions); err_sirius != nil {
+//			task.TaskPending = false
+//			return 0, err_sirius
+//		}
+//
+//		if err_sirius := cb_sdk.ConfigMutateInOptions(task.MutateInOptions); err_sirius != nil {
+//			task.TaskPending = false
+//			return 0, err_sirius
+//		}
+//
+//		task.MetaData = task.req.MetaData.GetCollectionMetadata(task.MetaDataIdentifier())
+//
+//		task.req.Lock()
+//		task.State = task_state.ConfigTaskState(task.MetaData.Seed, task.MetaData.SeedEnd, task.ResultSeed)
+//		task.req.Unlock()
+//
+//	} else {
+//		if task.State == nil {
+//			return task.ResultSeed, err_sirius.TaskStateIsNil
+//		}
+//		task.State.SetupStoringKeys()
+//		_ = task_result.DeleteResultFile(task.ResultSeed)
+//		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//	}
+//	return task.ResultSeed, nil
+//}
+//
+//func (task *SubDocReplace) TearUp() error {
+//	//Use this case to store task's state on disk when required
+//	//if err_sirius := task.State.SaveTaskSateOnDisk(); err_sirius != nil {
+//	//	log.Println("Error in storing TASK State on DISK")
+//	//}
+//	task.Result.StopStoringResult()
+//	if err_sirius := task.Result.SaveResultIntoFile(); err_sirius != nil {
+//		log.Println("not able to save Result into ", task.ResultSeed, task.Operation)
+//	}
+//	task.Result = nil
+//	task.State.StopStoringState()
+//	task.TaskPending = false
+//	return task.req.SaveRequestIntoFile()
+//}
+//
+//func (task *SubDocReplace) Do() error {
+//
+//	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
+//
+//	collectionObject, err1 := task.GetCollectionObject()
+//
+//	task.gen = docgenerator.ConfigGenerator(
+//		task.OperationConfig.KeySize,
+//		task.OperationConfig.DocSize,
+//		task.OperationConfig.DocType,
+//		task.OperationConfig.KeyPrefix,
+//		task.OperationConfig.KeySuffix,
+//		template.InitialiseTemplate(task.OperationConfig.TemplateName))
+//
+//	if err1 != nil {
+//		task.Result.ErrorOther = err1.Error()
+//		task.Result.FailWholeBulkOperation(task.OperationConfig.Start, task.OperationConfig.End,
+//			err1, task.State, task.gen, task.MetaData.Seed)
+//		return task.TearUp()
+//	}
+//
+//	replaceSubDocuments(task, collectionObject)
+//	task.Result.Success = (task.OperationConfig.End - task.OperationConfig.Start) - task.Result.Failure
+//
+//	return task.TearUp()
+//}
+//
+//// insertDocuments uploads new documents in a bucket.scope.collection in a defined batch size at multiple iterations.
+//func replaceSubDocuments(task *SubDocReplace, collectionObject *cb_sdk.CollectionObject) {
+//
+//	if task.req.ContextClosed() {
+//		return
+//	}
+//
+//	routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
+//	dataChannel := make(chan int64, tasks.MaxConcurrentRoutines)
+//
+//	skip := make(map[int64]struct{})
+//	for _, offset := range task.State.KeyStates.Completed {
+//		skip[offset] = struct{}{}
+//	}
+//	for _, offset := range task.State.KeyStates.Err {
+//		skip[offset] = struct{}{}
+//	}
+//	group := errgroup.Group{}
+//	for iteration := task.OperationConfig.Start; iteration < task.OperationConfig.End; iteration++ {
+//
+//		if task.req.ContextClosed() {
+//			close(routineLimiter)
+//			close(dataChannel)
+//			return
+//		}
+//
+//		routineLimiter <- struct{}{}
+//		dataChannel <- iteration
+//		group.Go(func() error {
+//			offset := <-dataChannel
+//			key := offset + task.MetaData.Seed
+//			docId := task.gen.BuildKey(key)
+//
+//			if _, ok := skip[offset]; ok {
+//				<-routineLimiter
+//				return fmt.Errorf("alreday performed operation on " + docId)
+//			}
+//			fake := faker.NewWithSeed(rand.NewSource(int64(key)))
+//
+//			subDocumentMap := task.gen.Template.GenerateSubPathAndValue(&fake, task.OperationConfig.DocSize)
+//			retracePreviousSubDocMutations(task.req, task.MetaDataIdentifier(), offset, *task.gen, &fake,
+//				task.ResultSeed, subDocumentMap)
+//
+//			var err_sirius error
+//			initTime := time.Now().UTC().Format(time.RFC850)
+//			for retry := 0; retry < int(math.Max(float64(1), float64(task.OperationConfig.Exceptions.
+//				RetryAttempts))); retry++ {
+//
+//				var iOps []gocb.MutateInSpec
+//				for path, value := range task.gen.Template.GenerateSubPathAndValue(&fake, task.OperationConfig.DocSize) {
+//					iOps = append(iOps, gocb.ReplaceSpec(path, value, &gocb.ReplaceSpecOptions{
+//						IsXattr: task.ReplaceSpecOptions.IsXattr,
+//					}))
+//				}
+//
+//				if !task.ReplaceSpecOptions.IsXattr {
+//					iOps = append(iOps, gocb.IncrementSpec(template.MutatedPath,
+//						int64(template.MutateFieldIncrement), &gocb.CounterSpecOptions{
+//							CreatePath: false,
+//							IsXattr:    false,
+//						}))
+//				}
+//
+//				initTime = time.Now().UTC().Format(time.RFC850)
+//				_, err_sirius = collectionObject.Collection.MutateIn(docId, iOps, &gocb.MutateInOptions{
+//					Expiry:          time.Duration(task.MutateInOptions.Expiry) * time.Second,
+//					PersistTo:       task.MutateInOptions.PersistTo,
+//					ReplicateTo:     task.MutateInOptions.ReplicateTo,
+//					DurabilityLevel: cb_sdk.GetDurability(task.MutateInOptions.Durability),
+//					StoreSemantic:   cb_sdk.GetStoreSemantic(task.MutateInOptions.StoreSemantic),
+//					Timeout:         time.Duration(task.MutateInOptions.Expiry) * time.Second,
+//					PreserveExpiry:  task.MutateInOptions.PreserveExpiry,
+//				})
+//
+//				if err_sirius == nil {
+//					break
+//				}
+//			}
+//			if err_sirius != nil {
+//				task.Result.IncrementFailure(initTime, docId, err_sirius, false, 0, offset)
+//				task.State.StateChannel <- task_state.StateHelper{Status: task_state.ERR, Offset: offset}
+//				<-routineLimiter
+//				return err_sirius
+//
+//			}
+//
+//			task.State.StateChannel <- task_state.StateHelper{Status: task_state.COMPLETED, Offset: offset}
+//			<-routineLimiter
+//			return nil
+//		})
+//	}
+//
+//	_ = group.Wait()
+//	close(routineLimiter)
+//	close(dataChannel)
+//	task.PostTaskExceptionHandling(collectionObject)
+//	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//}
+//
+//func (task *SubDocReplace) PostTaskExceptionHandling(collectionObject *cb_sdk.CollectionObject) {
+//	task.Result.StopStoringResult()
+//	task.State.StopStoringState()
+//	if task.OperationConfig.Exceptions.RetryAttempts <= 0 {
+//		return
+//	}
+//
+//	// Get all the errorOffset
+//	errorOffsetMaps := task.State.ReturnErrOffset()
+//	// Get all the completed offset
+//	completedOffsetMaps := task.State.ReturnCompletedOffset()
+//
+//	// For the offset in ignore exceptions :-> move them from error to completed
+//	shiftErrToCompletedOnIgnore(task.OperationConfig.Exceptions.IgnoreExceptions, task.Result, errorOffsetMaps, completedOffsetMaps)
+//
+//	if task.OperationConfig.Exceptions.RetryAttempts > 0 {
+//
+//		exceptionList := GetExceptions(task.Result, task.OperationConfig.Exceptions.RetryExceptions)
+//
+//		// For the retry exceptions :-> move them on success after retrying from err_sirius to completed
+//		for _, exception := range exceptionList {
+//
+//			errorOffsetListMap := make([]map[int64]RetriedResult, 0)
+//			for _, failedDocs := range task.Result.BulkError[exception] {
+//				m := make(map[int64]RetriedResult)
+//				m[failedDocs.Offset] = RetriedResult{}
+//				errorOffsetListMap = append(errorOffsetListMap, m)
+//			}
+//
+//			routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
+//			dataChannel := make(chan map[int64]RetriedResult, tasks.MaxConcurrentRoutines)
+//			wg := errgroup.Group{}
+//			for _, x := range errorOffsetListMap {
+//				dataChannel <- x
+//				routineLimiter <- struct{}{}
+//				wg.Go(func() error {
+//					m := <-dataChannel
+//					var offset = int64(-1)
+//					for k, _ := range m {
+//						offset = k
+//					}
+//					key := offset + task.MetaData.Seed
+//					docId := task.gen.BuildKey(key)
+//					fake := faker.NewWithSeed(rand.NewSource(int64(key)))
+//
+//					subDocumentMap := task.gen.Template.GenerateSubPathAndValue(&fake, task.OperationConfig.DocSize)
+//					retracePreviousSubDocMutations(task.req, task.MetaDataIdentifier(), offset, *task.gen, &fake,
+//						task.ResultSeed, subDocumentMap)
+//
+//					retry := 0
+//					var err_sirius error
+//					result := &gocb.MutateInResult{}
+//
+//					for retry = 0; retry <= task.OperationConfig.Exceptions.RetryAttempts; retry++ {
+//
+//						var iOps []gocb.MutateInSpec
+//						for path, value := range task.gen.Template.GenerateSubPathAndValue(&fake,
+//							task.OperationConfig.DocSize) {
+//							iOps = append(iOps, gocb.ReplaceSpec(path, value, &gocb.ReplaceSpecOptions{
+//								IsXattr: task.ReplaceSpecOptions.IsXattr,
+//							}))
+//						}
+//
+//						if !task.ReplaceSpecOptions.IsXattr {
+//							iOps = append(iOps, gocb.IncrementSpec(template.MutatedPath,
+//								int64(template.MutateFieldIncrement), &gocb.CounterSpecOptions{
+//									CreatePath: false,
+//									IsXattr:    false,
+//								}))
+//						}
+//
+//						result, err_sirius = collectionObject.Collection.MutateIn(docId, iOps, &gocb.MutateInOptions{
+//							Expiry:          time.Duration(task.MutateInOptions.Expiry) * time.Second,
+//							PersistTo:       task.MutateInOptions.PersistTo,
+//							ReplicateTo:     task.MutateInOptions.ReplicateTo,
+//							DurabilityLevel: cb_sdk.GetDurability(task.MutateInOptions.Durability),
+//							StoreSemantic:   cb_sdk.GetStoreSemantic(task.MutateInOptions.StoreSemantic),
+//							Timeout:         time.Duration(task.MutateInOptions.Expiry) * time.Second,
+//							PreserveExpiry:  task.MutateInOptions.PreserveExpiry,
+//						})
+//
+//						if err_sirius == nil {
+//							break
+//						}
+//					}
+//
+//					if err_sirius != nil {
+//						m[offset] = RetriedResult{
+//							Status: true,
+//							CAS:    0,
+//						}
+//					} else {
+//						m[offset] = RetriedResult{
+//							Status: true,
+//							CAS:    uint64(result.Cas()),
+//						}
+//					}
+//
+//					<-routineLimiter
+//					return nil
+//				})
+//			}
+//			_ = wg.Wait()
+//
+//			shiftErrToCompletedOnRetrying(exception, task.Result, errorOffsetListMap, errorOffsetMaps, completedOffsetMaps)
+//		}
+//	}
+//
+//	task.State.MakeCompleteKeyFromMap(completedOffsetMaps)
+//	task.State.MakeErrorKeyFromMap(errorOffsetMaps)
+//	task.Result.Failure = int64(len(task.State.KeyStates.Err))
+//	log.Println("completed retrying:- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//
+//}
+//
+//func (task *SubDocReplace) MatchResultSeed(resultSeed string) (bool, error) {
+//	defer task.lock.Unlock()
+//	task.lock.Lock()
+//	if fmt.Sprintf("%d", task.ResultSeed) == resultSeed {
+//		if task.TaskPending {
+//			return true, err_sirius.TaskInPendingState
+//		}
+//		if task.Result == nil {
+//			task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
+//		}
+//		return true, nil
+//	}
+//	return false, nil
+//}
+//
+//func (task *SubDocReplace) GetCollectionObject() (*cb_sdk.CollectionObject, error) {
+//	return task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope,
+//		task.Collection)
+//}
+//
+//func (task *SubDocReplace) SetException(exceptions Exceptions) {
+//	task.OperationConfig.Exceptions = exceptions
+//}
+//
+//func (task *SubDocReplace) GetOperationConfig() (*OperationConfig, *task_state.TaskState) {
+//	return task.OperationConfig, task.State
+//}
diff --git a/internal/tasks/bulk_loading/task_sub_doc_upsert.go b/internal/tasks/bulk_loading/task_sub_doc_upsert.go
new file mode 100644
index 0000000..b18eef4
--- /dev/null
+++ b/internal/tasks/bulk_loading/task_sub_doc_upsert.go
@@ -0,0 +1,413 @@
+package bulk_loading
+
+//
+//import (
+//	"fmt"
+//	"github.com/couchbase/gocb/v2"
+//	"github.com/couchbaselabs/sirius/internal/cb_sdk"
+//	"github.com/couchbaselabs/sirius/internal/docgenerator"
+//	"github.com/couchbaselabs/sirius/internal/err_sirius"
+//	"github.com/couchbaselabs/sirius/internal/meta_data"
+//	"github.com/couchbaselabs/sirius/internal/task_result"
+//	"github.com/couchbaselabs/sirius/internal/task_state"
+//	"github.com/couchbaselabs/sirius/internal/tasks"
+//	"github.com/couchbaselabs/sirius/internal/template"
+//	"github.com/jaswdr/faker"
+//	"golang.org/x/sync/errgroup"
+//	"log"
+//	"math"
+//	"math/rand"
+//	"strings"
+//	"sync"
+//	"time"
+//)
+//
+//type SubDocUpsert struct {
+//	IdentifierToken   string                        `json:"identifierToken" doc:"true"`
+//	ClusterConfig     *cb_sdk.ClusterConfig         `json:"clusterConfig" doc:"true"`
+//	Bucket            string                        `json:"bucket" doc:"true"`
+//	Scope             string                        `json:"scope,omitempty" doc:"true"`
+//	Collection        string                        `json:"collection,omitempty" doc:"true"`
+//	OperationConfig   *OperationConfig              `json:"operationConfig" doc:"true"`
+//	InsertSpecOptions *cb_sdk.InsertSpecOptions     `json:"insertSpecOptions" doc:"true"`
+//	MutateInOptions   *cb_sdk.MutateInOptions       `json:"mutateInOptions" doc:"true"`
+//	Operation         string                        `json:"operation" doc:"false"`
+//	ResultSeed        int64                         `json:"resultSeed" doc:"false"`
+//	TaskPending       bool                          `json:"taskPending" doc:"false"`
+//	State             *task_state.TaskState         `json:"State" doc:"false"`
+//	MetaData          *meta_data.CollectionMetaData `json:"metaData" doc:"false"`
+//	Result            *task_result.TaskResult       `json:"-" doc:"false"`
+//	gen               *docgenerator.Generator       `json:"-" doc:"false"`
+//	req               *tasks.Request                `json:"-" doc:"false"`
+//	rerun             bool                          `json:"-" doc:"false"`
+//	lock              sync.Mutex                    `json:"-" doc:"false"`
+//}
+//
+//func (task *SubDocUpsert) Describe() string {
+//	return " SubDocUpsert upserts a Sub-Document"
+//}
+//
+//func (task *SubDocUpsert) MetaDataIdentifier() string {
+//	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
+//	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
+//		task.Collection}, ":")
+//}
+//
+//func (task *SubDocUpsert) CheckIfPending() bool {
+//	return task.TaskPending
+//}
+//
+//// Config configures  the insert task
+//func (task *SubDocUpsert) Config(req *tasks.Request, reRun bool) (int64, error) {
+//	task.TaskPending = true
+//	task.req = req
+//
+//	if task.req == nil {
+//		task.TaskPending = false
+//		return 0, err_sirius.RequestIsNil
+//	}
+//
+//	task.req.ReconnectionManager()
+//	if _, err_sirius := task.req.GetCluster(task.ClusterConfig); err_sirius != nil {
+//		task.TaskPending = false
+//		return 0, err_sirius
+//	}
+//
+//	task.lock = sync.Mutex{}
+//	task.rerun = reRun
+//
+//	if !reRun {
+//		task.ResultSeed = int64(time.Now().UnixNano())
+//		task.Operation = tasks.SubDocUpsertOperation
+//
+//		if task.Bucket == "" {
+//			task.Bucket = cb_sdk.DefaultBucket
+//		}
+//		if task.Scope == "" {
+//			task.Scope = cb_sdk.DefaultScope
+//		}
+//		if task.Collection == "" {
+//			task.Collection = cb_sdk.DefaultCollection
+//		}
+//
+//		if err_sirius := ConfigureOperationConfig(task.OperationConfig); err_sirius != nil {
+//			task.TaskPending = false
+//			return 0, err_sirius
+//		}
+//
+//		if err_sirius := cb_sdk.ConfigInsertSpecOptions(task.InsertSpecOptions); err_sirius != nil {
+//			task.TaskPending = false
+//			return 0, err_sirius
+//		}
+//
+//		if err_sirius := cb_sdk.ConfigMutateInOptions(task.MutateInOptions); err_sirius != nil {
+//			task.TaskPending = false
+//			return 0, err_sirius
+//		}
+//
+//		task.MetaData = task.req.MetaData.GetCollectionMetadata(task.MetaDataIdentifier())
+//
+//		task.req.Lock()
+//		if task.OperationConfig.End+task.MetaData.Seed > task.MetaData.SeedEnd {
+//			task.req.AddToSeedEnd(task.MetaData, (task.OperationConfig.End+task.MetaData.Seed)-(task.MetaData.SeedEnd))
+//		}
+//		task.State = task_state.ConfigTaskState(task.MetaData.Seed, task.MetaData.SeedEnd, task.ResultSeed)
+//		task.req.Unlock()
+//
+//	} else {
+//		if task.State == nil {
+//			return task.ResultSeed, err_sirius.TaskStateIsNil
+//		}
+//		task.State.SetupStoringKeys()
+//		_ = task_result.DeleteResultFile(task.ResultSeed)
+//		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//	}
+//	return task.ResultSeed, nil
+//}
+//
+//func (task *SubDocUpsert) TearUp() error {
+//	//Use this case to store task's state on disk when required
+//	//if err_sirius := task.State.SaveTaskSateOnDisk(); err_sirius != nil {
+//	//	log.Println("Error in storing TASK State on DISK")
+//	//}
+//	task.Result.StopStoringResult()
+//	if err_sirius := task.Result.SaveResultIntoFile(); err_sirius != nil {
+//		log.Println("not able to save Result into ", task.ResultSeed, task.Operation)
+//	}
+//	task.Result = nil
+//	task.State.StopStoringState()
+//	task.TaskPending = false
+//	return task.req.SaveRequestIntoFile()
+//}
+//
+//func (task *SubDocUpsert) Do() error {
+//
+//	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
+//
+//	collectionObject, err1 := task.GetCollectionObject()
+//
+//	task.gen = docgenerator.ConfigGenerator(
+//		task.OperationConfig.KeySize,
+//		task.OperationConfig.DocSize,
+//		task.OperationConfig.DocType,
+//		task.OperationConfig.KeyPrefix,
+//		task.OperationConfig.KeySuffix,
+//		template.InitialiseTemplate(task.OperationConfig.TemplateName))
+//
+//	if err1 != nil {
+//		task.Result.ErrorOther = err1.Error()
+//		task.Result.FailWholeBulkOperation(task.OperationConfig.Start, task.OperationConfig.End,
+//			err1, task.State, task.gen, task.MetaData.Seed)
+//		return task.TearUp()
+//	}
+//
+//	upsertSubDocuments(task, collectionObject)
+//	task.Result.Success = (task.OperationConfig.End - task.OperationConfig.Start) - task.Result.Failure
+//
+//	return task.TearUp()
+//}
+//
+//// insertDocuments uploads new documents in a bucket.scope.collection in a defined batch size at multiple iterations.
+//func upsertSubDocuments(task *SubDocUpsert, collectionObject *cb_sdk.CollectionObject) {
+//
+//	if task.req.ContextClosed() {
+//		return
+//	}
+//
+//	routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
+//	dataChannel := make(chan int64, tasks.MaxConcurrentRoutines)
+//
+//	skip := make(map[int64]struct{})
+//	for _, offset := range task.State.KeyStates.Completed {
+//		skip[offset] = struct{}{}
+//	}
+//	for _, offset := range task.State.KeyStates.Err {
+//		skip[offset] = struct{}{}
+//	}
+//	group := errgroup.Group{}
+//	for iteration := task.OperationConfig.Start; iteration < task.OperationConfig.End; iteration++ {
+//
+//		if task.req.ContextClosed() {
+//			close(routineLimiter)
+//			close(dataChannel)
+//			return
+//		}
+//
+//		routineLimiter <- struct{}{}
+//		dataChannel <- iteration
+//		group.Go(func() error {
+//			offset := <-dataChannel
+//			key := offset + task.MetaData.Seed
+//			docId := task.gen.BuildKey(key)
+//
+//			if _, ok := skip[offset]; ok {
+//				<-routineLimiter
+//				return fmt.Errorf("alreday performed operation on " + docId)
+//			}
+//			fake := faker.NewWithSeed(rand.NewSource(int64(key)))
+//
+//			subDocumentMap := task.gen.Template.GenerateSubPathAndValue(&fake, task.OperationConfig.DocSize)
+//			retracePreviousSubDocMutations(task.req, task.MetaDataIdentifier(), offset, *task.gen, &fake,
+//				task.ResultSeed, subDocumentMap)
+//
+//			var err_sirius error
+//			initTime := time.Now().UTC().Format(time.RFC850)
+//			for retry := 0; retry < int(math.Max(float64(1), float64(task.OperationConfig.Exceptions.
+//				RetryAttempts))); retry++ {
+//
+//				var iOps []gocb.MutateInSpec
+//				for path, value := range task.gen.Template.GenerateSubPathAndValue(&fake, task.OperationConfig.DocSize) {
+//					iOps = append(iOps, gocb.UpsertSpec(path, value, &gocb.UpsertSpecOptions{
+//						CreatePath: task.InsertSpecOptions.CreatePath,
+//						IsXattr:    task.InsertSpecOptions.IsXattr,
+//					}))
+//				}
+//
+//				if !task.InsertSpecOptions.IsXattr {
+//					iOps = append(iOps, gocb.IncrementSpec(template.MutatedPath,
+//						int64(template.MutateFieldIncrement), &gocb.CounterSpecOptions{
+//							CreatePath: false,
+//							IsXattr:    false,
+//						}))
+//				}
+//
+//				initTime = time.Now().UTC().Format(time.RFC850)
+//				_, err_sirius = collectionObject.Collection.MutateIn(docId, iOps, &gocb.MutateInOptions{
+//					Expiry:          time.Duration(task.MutateInOptions.Expiry) * time.Second,
+//					PersistTo:       task.MutateInOptions.PersistTo,
+//					ReplicateTo:     task.MutateInOptions.ReplicateTo,
+//					DurabilityLevel: cb_sdk.GetDurability(task.MutateInOptions.Durability),
+//					StoreSemantic:   cb_sdk.GetStoreSemantic(task.MutateInOptions.StoreSemantic),
+//					Timeout:         time.Duration(task.MutateInOptions.Expiry) * time.Second,
+//					PreserveExpiry:  task.MutateInOptions.PreserveExpiry,
+//				})
+//
+//				if err_sirius == nil {
+//					break
+//				}
+//			}
+//			if err_sirius != nil {
+//				task.Result.IncrementFailure(initTime, docId, err_sirius, false, 0, offset)
+//				task.State.StateChannel <- task_state.StateHelper{Status: task_state.ERR, Offset: offset}
+//				<-routineLimiter
+//				return err_sirius
+//
+//			}
+//
+//			task.State.StateChannel <- task_state.StateHelper{Status: task_state.COMPLETED, Offset: offset}
+//			<-routineLimiter
+//			return nil
+//		})
+//	}
+//
+//	_ = group.Wait()
+//	close(routineLimiter)
+//	close(dataChannel)
+//	task.PostTaskExceptionHandling(collectionObject)
+//	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//}
+//
+//func (task *SubDocUpsert) PostTaskExceptionHandling(collectionObject *cb_sdk.CollectionObject) {
+//	task.Result.StopStoringResult()
+//	task.State.StopStoringState()
+//
+//	if task.OperationConfig.Exceptions.RetryAttempts <= 0 {
+//		return
+//	}
+//
+//	// Get all the errorOffset
+//	errorOffsetMaps := task.State.ReturnErrOffset()
+//	// Get all the completed offset
+//	completedOffsetMaps := task.State.ReturnCompletedOffset()
+//
+//	// For the offset in ignore exceptions :-> move them from error to completed
+//	shiftErrToCompletedOnIgnore(task.OperationConfig.Exceptions.IgnoreExceptions, task.Result, errorOffsetMaps, completedOffsetMaps)
+//
+//	if task.OperationConfig.Exceptions.RetryAttempts > 0 {
+//
+//		exceptionList := GetExceptions(task.Result, task.OperationConfig.Exceptions.RetryExceptions)
+//
+//		// For the retry exceptions :-> move them on success after retrying from err_sirius to completed
+//		for _, exception := range exceptionList {
+//
+//			errorOffsetListMap := make([]map[int64]RetriedResult, 0)
+//			for _, failedDocs := range task.Result.BulkError[exception] {
+//				m := make(map[int64]RetriedResult)
+//				m[failedDocs.Offset] = RetriedResult{}
+//				errorOffsetListMap = append(errorOffsetListMap, m)
+//			}
+//
+//			routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
+//			dataChannel := make(chan map[int64]RetriedResult, tasks.MaxConcurrentRoutines)
+//			wg := errgroup.Group{}
+//			for _, x := range errorOffsetListMap {
+//				dataChannel <- x
+//				routineLimiter <- struct{}{}
+//				wg.Go(func() error {
+//					m := <-dataChannel
+//					var offset = int64(-1)
+//					for k, _ := range m {
+//						offset = k
+//					}
+//					key := offset + task.MetaData.Seed
+//					docId := task.gen.BuildKey(key)
+//					fake := faker.NewWithSeed(rand.NewSource(int64(key)))
+//
+//					subDocumetMap := task.gen.Template.GenerateSubPathAndValue(&fake, task.OperationConfig.DocSize)
+//					retracePreviousSubDocMutations(task.req, task.MetaDataIdentifier(), offset, *task.gen, &fake,
+//						task.ResultSeed, subDocumetMap)
+//
+//					retry := 0
+//					var err_sirius error
+//					result := &gocb.MutateInResult{}
+//
+//					for retry = 0; retry <= task.OperationConfig.Exceptions.RetryAttempts; retry++ {
+//
+//						var iOps []gocb.MutateInSpec
+//						for path, value := range task.gen.Template.GenerateSubPathAndValue(&fake,
+//							task.OperationConfig.DocSize) {
+//							iOps = append(iOps, gocb.UpsertSpec(path, value, &gocb.UpsertSpecOptions{
+//								CreatePath: task.InsertSpecOptions.CreatePath,
+//								IsXattr:    task.InsertSpecOptions.IsXattr,
+//							}))
+//						}
+//
+//						if !task.InsertSpecOptions.IsXattr {
+//							iOps = append(iOps, gocb.IncrementSpec(template.MutatedPath,
+//								int64(template.MutateFieldIncrement), &gocb.CounterSpecOptions{
+//									CreatePath: true,
+//									IsXattr:    false,
+//								}))
+//						}
+//
+//						result, err_sirius = collectionObject.Collection.MutateIn(docId, iOps, &gocb.MutateInOptions{
+//							Expiry:          time.Duration(task.MutateInOptions.Expiry) * time.Second,
+//							PersistTo:       task.MutateInOptions.PersistTo,
+//							ReplicateTo:     task.MutateInOptions.ReplicateTo,
+//							DurabilityLevel: cb_sdk.GetDurability(task.MutateInOptions.Durability),
+//							StoreSemantic:   cb_sdk.GetStoreSemantic(task.MutateInOptions.StoreSemantic),
+//							Timeout:         time.Duration(task.MutateInOptions.Expiry) * time.Second,
+//							PreserveExpiry:  task.MutateInOptions.PreserveExpiry,
+//						})
+//
+//						if err_sirius == nil {
+//							break
+//						}
+//					}
+//
+//					if err_sirius != nil {
+//						m[offset] = RetriedResult{
+//							Status: true,
+//							CAS:    0,
+//						}
+//					} else {
+//						m[offset] = RetriedResult{
+//							Status: true,
+//							CAS:    uint64(result.Cas()),
+//						}
+//					}
+//
+//					<-routineLimiter
+//					return nil
+//				})
+//			}
+//			_ = wg.Wait()
+//
+//			shiftErrToCompletedOnRetrying(exception, task.Result, errorOffsetListMap, errorOffsetMaps, completedOffsetMaps)
+//		}
+//	}
+//
+//	task.State.MakeCompleteKeyFromMap(completedOffsetMaps)
+//	task.State.MakeErrorKeyFromMap(errorOffsetMaps)
+//	task.Result.Failure = int64(len(task.State.KeyStates.Err))
+//	log.Println("completed retrying:- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//
+//}
+//
+//func (task *SubDocUpsert) MatchResultSeed(resultSeed string) (bool, error) {
+//	defer task.lock.Unlock()
+//	task.lock.Lock()
+//	if fmt.Sprintf("%d", task.ResultSeed) == resultSeed {
+//		if task.TaskPending {
+//			return true, err_sirius.TaskInPendingState
+//		}
+//		if task.Result == nil {
+//			task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
+//		}
+//		return true, nil
+//	}
+//	return false, nil
+//}
+//
+//func (task *SubDocUpsert) GetCollectionObject() (*cb_sdk.CollectionObject, error) {
+//	return task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope,
+//		task.Collection)
+//}
+//
+//func (task *SubDocUpsert) SetException(exceptions Exceptions) {
+//	task.OperationConfig.Exceptions = exceptions
+//}
+//
+//func (task *SubDocUpsert) GetOperationConfig() (*OperationConfig, *task_state.TaskState) {
+//	return task.OperationConfig, task.State
+//}
diff --git a/internal/tasks/bulk_loading/task_validate.go b/internal/tasks/bulk_loading/task_validate.go
new file mode 100644
index 0000000..6151cb2
--- /dev/null
+++ b/internal/tasks/bulk_loading/task_validate.go
@@ -0,0 +1,375 @@
+package bulk_loading
+
+//
+//import (
+//	"encoding/json"
+//	"errors"
+//	"fmt"
+//	"github.com/couchbase/gocb/v2"
+//	"github.com/couchbaselabs/sirius/internal/cb_sdk"
+//	"github.com/couchbaselabs/sirius/internal/docgenerator"
+//	"github.com/couchbaselabs/sirius/internal/err_sirius"
+//	"github.com/couchbaselabs/sirius/internal/meta_data"
+//	"github.com/couchbaselabs/sirius/internal/task_result"
+//	"github.com/couchbaselabs/sirius/internal/task_state"
+//	"github.com/couchbaselabs/sirius/internal/tasks"
+//	"github.com/couchbaselabs/sirius/internal/template"
+//	"github.com/jaswdr/faker"
+//	"golang.org/x/sync/errgroup"
+//	"log"
+//	"math"
+//	"math/rand"
+//	"strings"
+//	"sync"
+//	"time"
+//)
+//
+//type ValidateTask struct {
+//	IdentifierToken string                        `json:"identifierToken" doc:"true"`
+//	ClusterConfig   *cb_sdk.ClusterConfig         `json:"clusterConfig" doc:"true"`
+//	Bucket          string                        `json:"bucket" doc:"true"`
+//	Scope           string                        `json:"scope,omitempty" doc:"true"`
+//	Collection      string                        `json:"collection,omitempty" doc:"true"`
+//	Operation       string                        `json:"operation" doc:"false"`
+//	ResultSeed      int64                         `json:"resultSeed" doc:"false"`
+//	TaskPending     bool                          `json:"taskPending" doc:"false"`
+//	MetaData        *meta_data.CollectionMetaData `json:"metaData" doc:"false"`
+//	State           *task_state.TaskState         `json:"State" doc:"false"`
+//	Result          *task_result.TaskResult       `json:"-" doc:"false"`
+//	gen             *docgenerator.Generator       `json:"-" doc:"false"`
+//	req             *tasks.Request                `json:"-" doc:"false"`
+//	rerun           bool                          `json:"-" doc:"false"`
+//	lock            sync.Mutex                    `json:"" doc:"false"`
+//}
+//
+//func (task *ValidateTask) MetaDataIdentifier() string {
+//	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
+//	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
+//		task.Collection}, ":")
+//}
+//
+//func (task *ValidateTask) Describe() string {
+//	return "Validates every document in the cluster's bucket"
+//}
+//
+//func (task *ValidateTask) CheckIfPending() bool {
+//	return task.TaskPending
+//}
+//
+//func (task *ValidateTask) TearUp() error {
+//	if err_sirius := task.Result.SaveResultIntoFile(); err_sirius != nil {
+//		log.Println("not able to save Result into ", task.ResultSeed, task.Operation)
+//	}
+//	task.Result.StopStoringResult()
+//	task.Result = nil
+//	task.State.StopStoringState()
+//	task.TaskPending = false
+//	return task.req.SaveRequestIntoFile()
+//}
+//
+//func (task *ValidateTask) Config(req *tasks.Request, reRun bool) (int64, error) {
+//	task.TaskPending = true
+//	task.req = req
+//
+//	if task.req == nil {
+//		task.TaskPending = false
+//		return 0, err_sirius.RequestIsNil
+//	}
+//
+//	task.req.ReconnectionManager()
+//	if _, err_sirius := task.req.GetCluster(task.ClusterConfig); err_sirius != nil {
+//		task.TaskPending = false
+//		return 0, err_sirius
+//	}
+//
+//	task.lock = sync.Mutex{}
+//	task.rerun = false
+//
+//	if !reRun {
+//		task.ResultSeed = int64(time.Now().UnixNano())
+//		task.Operation = tasks.ValidateOperation
+//
+//		if task.Bucket == "" {
+//			task.Bucket = cb_sdk.DefaultBucket
+//		}
+//		if task.Scope == "" {
+//			task.Scope = cb_sdk.DefaultScope
+//		}
+//		if task.Collection == "" {
+//			task.Collection = cb_sdk.DefaultCollection
+//		}
+//
+//		task.MetaData = task.req.MetaData.GetCollectionMetadata(task.MetaDataIdentifier())
+//
+//		task.req.Lock()
+//		task.State = task_state.ConfigTaskState(task.MetaData.Seed, task.MetaData.SeedEnd, task.ResultSeed)
+//		task.req.Unlock()
+//
+//	} else {
+//		if task.State == nil {
+//			return task.ResultSeed, err_sirius.TaskStateIsNil
+//		}
+//		task.State.SetupStoringKeys()
+//		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//	}
+//	return task.ResultSeed, nil
+//}
+//
+//func (task *ValidateTask) Do() error {
+//
+//	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
+//
+//	collectionObject, err1 := task.GetCollectionObject()
+//
+//	task.gen = docgenerator.ConfigGenerator(
+//		docgenerator.DefaultKeySize,
+//		docgenerator.DefaultDocSize,
+//		docgenerator.JsonDocument,
+//		docgenerator.DefaultKeyPrefix,
+//		docgenerator.DefaultKeySuffix,
+//		template.InitialiseTemplate("person"))
+//
+//	if err1 != nil {
+//		task.Result.ErrorOther = err1.Error()
+//		task.Result.FailWholeBulkOperation(0, task.MetaData.Seed-task.MetaData.SeedEnd,
+//			err1, task.State, task.gen, task.MetaData.Seed)
+//		return task.TearUp()
+//	}
+//
+//	validateDocuments(task, collectionObject)
+//
+//	task.Result.Success = task.State.SeedEnd - task.State.SeedStart - task.Result.Failure
+//
+//	return task.TearUp()
+//}
+//
+//// ValidateDocuments return the validity of the collection using TaskState
+//func validateDocuments(task *ValidateTask, collectionObject *cb_sdk.CollectionObject) {
+//
+//	if task.req.ContextClosed() {
+//		return
+//	}
+//
+//	routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
+//	dataChannel := make(chan int64, tasks.MaxConcurrentRoutines)
+//	skip := make(map[int64]struct{})
+//	for _, offset := range task.State.KeyStates.Completed {
+//		skip[offset] = struct{}{}
+//	}
+//	for _, offset := range task.State.KeyStates.Err {
+//		skip[offset] = struct{}{}
+//	}
+//	deletedOffset, err1 := retracePreviousDeletions(task.req, task.MetaDataIdentifier(), task.ResultSeed)
+//	if err1 != nil {
+//		log.Println(err1)
+//		return
+//	}
+//
+//	deletedOffsetSubDoc, err2 := retracePreviousSubDocDeletions(task.req, task.MetaDataIdentifier(), task.ResultSeed)
+//	if err2 != nil {
+//		log.Println(err2)
+//		return
+//	}
+//
+//	group := errgroup.Group{}
+//	for offset := int64(0); offset < (task.MetaData.SeedEnd - task.MetaData.Seed); offset++ {
+//
+//		if task.req.ContextClosed() {
+//			close(routineLimiter)
+//			close(dataChannel)
+//			return
+//		}
+//
+//		routineLimiter <- struct{}{}
+//		dataChannel <- offset
+//		group.Go(func() error {
+//			offset := <-dataChannel
+//
+//			if _, ok := skip[offset]; ok {
+//				<-routineLimiter
+//				return nil
+//			}
+//
+//			operationConfigDoc, err_sirius := retrieveLastConfig(task.req, offset, false)
+//			if err_sirius != nil {
+//				<-routineLimiter
+//				return err_sirius
+//			}
+//			operationConfigSubDoc, err_sirius := retrieveLastConfig(task.req, offset, true)
+//
+//			/* Resetting the doc generator for the offset as per
+//			the last configuration of operation performed on offset.
+//			*/
+//			genDoc := docgenerator.Reset(
+//				operationConfigDoc.KeySize,
+//				operationConfigDoc.DocSize,
+//				operationConfigDoc.DocType,
+//				operationConfigDoc.KeyPrefix,
+//				operationConfigDoc.KeySuffix,
+//				operationConfigDoc.TemplateName,
+//			)
+//
+//			genSubDoc := docgenerator.Reset(
+//				operationConfigSubDoc.KeySize,
+//				operationConfigSubDoc.DocSize,
+//				operationConfigSubDoc.DocType,
+//				operationConfigSubDoc.KeyPrefix,
+//				operationConfigSubDoc.KeySuffix,
+//				operationConfigSubDoc.TemplateName,
+//			)
+//
+//			/* building Key and doc as per
+//			local config off the offset.
+//			*/
+//			key := task.MetaData.Seed + offset
+//			docId := genDoc.BuildKey(key)
+//
+//			fake := faker.NewWithSeed(rand.NewSource(int64(key)))
+//			fakeSub := faker.NewWithSeed(rand.NewSource(int64(key)))
+//			initTime := time.Now().UTC().Format(time.RFC850)
+//
+//			originalDocument, err_sirius := genDoc.Template.GenerateDocument(&fake, operationConfigDoc.DocSize)
+//			if err_sirius != nil {
+//				task.Result.IncrementFailure(initTime, docId, err_sirius, false, 0, offset)
+//				task.State.StateChannel <- task_state.StateHelper{Status: task_state.ERR, Offset: offset}
+//				<-routineLimiter
+//				return err_sirius
+//			}
+//			updatedDocument, err_sirius := retracePreviousMutations(task.req, task.MetaDataIdentifier(), offset,
+//				originalDocument, *genDoc,
+//				&fake,
+//				task.ResultSeed)
+//			if err_sirius != nil {
+//				task.Result.IncrementFailure(initTime, docId, err_sirius, false, 0, offset)
+//				task.State.StateChannel <- task_state.StateHelper{Status: task_state.ERR, Offset: offset}
+//				<-routineLimiter
+//				return err_sirius
+//			}
+//
+//			subDocumentMap := genSubDoc.Template.GenerateSubPathAndValue(&fakeSub, operationConfigSubDoc.DocSize)
+//			subDocumentMap, err_sirius = retracePreviousSubDocMutations(task.req, task.MetaDataIdentifier(), offset,
+//				*genSubDoc,
+//				&fakeSub,
+//				task.ResultSeed,
+//				subDocumentMap)
+//			if err_sirius != nil {
+//				task.Result.IncrementFailure(initTime, docId, err_sirius, false, 0, offset)
+//				task.State.StateChannel <- task_state.StateHelper{Status: task_state.ERR, Offset: offset}
+//				<-routineLimiter
+//				return err_sirius
+//			}
+//
+//			mutationCount, err_sirius := countMutation(task.req, task.MetaDataIdentifier(), offset, task.ResultSeed)
+//			if err_sirius != nil {
+//				task.Result.IncrementFailure(initTime, docId, err_sirius, false, 0, offset)
+//				task.State.StateChannel <- task_state.StateHelper{Status: task_state.ERR, Offset: offset}
+//				<-routineLimiter
+//				return err_sirius
+//			}
+//
+//			updatedDocumentBytes, err_sirius := json.Marshal(updatedDocument)
+//			if err_sirius != nil {
+//				log.Println(err_sirius)
+//				<-routineLimiter
+//				return err_sirius
+//			}
+//
+//			updatedDocumentMap := make(map[string]any)
+//			if err_sirius := json.Unmarshal(updatedDocumentBytes, &updatedDocumentMap); err_sirius != nil {
+//				log.Println(err_sirius)
+//				<-routineLimiter
+//				return err_sirius
+//			}
+//			updatedDocumentMap[template.MutatedPath] = float64(mutationCount)
+//
+//			result := &gocb.GetResult{}
+//			resultFromHost := make(map[string]any)
+//
+//			initTime = time.Now().UTC().Format(time.RFC850)
+//			for retry := 0; retry < int(math.Max(float64(1), float64(operationConfigDoc.Exceptions.
+//				RetryAttempts))); retry++ {
+//				result, err_sirius = collectionObject.Collection.Get(docId, nil)
+//				if err_sirius == nil {
+//					break
+//				}
+//			}
+//
+//			if err_sirius != nil {
+//				if errors.Is(err_sirius, gocb.ErrDocumentNotFound) {
+//					if _, ok := deletedOffset[offset]; ok {
+//						task.State.StateChannel <- task_state.StateHelper{Status: task_state.COMPLETED, Offset: offset}
+//						<-routineLimiter
+//						return nil
+//					}
+//					if _, ok := deletedOffsetSubDoc[offset]; ok {
+//						task.State.StateChannel <- task_state.StateHelper{Status: task_state.COMPLETED, Offset: offset}
+//						<-routineLimiter
+//						return nil
+//					}
+//				}
+//				task.Result.IncrementFailure(initTime, docId, err_sirius, false, 0, offset)
+//				task.State.StateChannel <- task_state.StateHelper{Status: task_state.ERR, Offset: offset}
+//				<-routineLimiter
+//				return err_sirius
+//			}
+//
+//			if err_sirius := result.Content(&resultFromHost); err_sirius != nil {
+//				task.Result.IncrementFailure(initTime, docId, err_sirius, false, 0, offset)
+//				task.State.StateChannel <- task_state.StateHelper{Status: task_state.ERR, Offset: offset}
+//				<-routineLimiter
+//				return err_sirius
+//			}
+//
+//			if !tasks.CompareDocumentsIsSame(resultFromHost, updatedDocumentMap, subDocumentMap) {
+//				task.Result.IncrementFailure(initTime, docId, errors.New("integrity Lost"),
+//					false, 0, offset)
+//				task.State.StateChannel <- task_state.StateHelper{Status: task_state.ERR, Offset: offset}
+//				<-routineLimiter
+//				return err_sirius
+//				//}
+//			}
+//
+//			task.State.StateChannel <- task_state.StateHelper{Status: task_state.COMPLETED, Offset: offset}
+//			<-routineLimiter
+//			return nil
+//		})
+//	}
+//	_ = group.Wait()
+//	close(routineLimiter)
+//	close(dataChannel)
+//	task.PostTaskExceptionHandling(collectionObject)
+//	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//
+//}
+//
+//func (task *ValidateTask) PostTaskExceptionHandling(collectionObject *cb_sdk.CollectionObject) {
+//	task.Result.StopStoringResult()
+//	task.State.StopStoringState()
+//}
+//
+//func (task *ValidateTask) MatchResultSeed(resultSeed string) (bool, error) {
+//	defer task.lock.Unlock()
+//	task.lock.Lock()
+//	if fmt.Sprintf("%d", task.ResultSeed) == resultSeed {
+//		if task.TaskPending {
+//			return true, err_sirius.TaskInPendingState
+//		}
+//		if task.Result == nil {
+//			task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
+//		}
+//		return true, nil
+//	}
+//	return false, nil
+//}
+//
+//func (task *ValidateTask) GetCollectionObject() (*cb_sdk.CollectionObject, error) {
+//	return task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope,
+//		task.Collection)
+//}
+//
+//func (task *ValidateTask) SetException(exceptions Exceptions) {
+//}
+//
+//func (task *ValidateTask) GetOperationConfig() (*OperationConfig, *task_state.TaskState) {
+//	return nil, task.State
+//}
diff --git a/internal/tasks/bulk_loading_cb/bulk_loading_task.go b/internal/tasks/bulk_loading_cb/bulk_loading_task.go
deleted file mode 100644
index 49f46a8..0000000
--- a/internal/tasks/bulk_loading_cb/bulk_loading_task.go
+++ /dev/null
@@ -1,17 +0,0 @@
-package bulk_loading_cb
-
-import (
-	"github.com/couchbaselabs/sirius/internal/cb_sdk"
-	"github.com/couchbaselabs/sirius/internal/task_state"
-	"github.com/couchbaselabs/sirius/internal/tasks"
-)
-
-type BulkTask interface {
-	tasks.Task
-	PostTaskExceptionHandling(collectionObject *cb_sdk.CollectionObject)
-	MatchResultSeed(resultSeed string) (bool, error)
-	GetCollectionObject() (*cb_sdk.CollectionObject, error)
-	SetException(exceptions Exceptions)
-	GetOperationConfig() (*OperationConfig, *task_state.TaskState)
-	CollectionIdentifier() string
-}
diff --git a/internal/tasks/bulk_loading_cb/helper.go b/internal/tasks/bulk_loading_cb/helper.go
deleted file mode 100644
index aacee99..0000000
--- a/internal/tasks/bulk_loading_cb/helper.go
+++ /dev/null
@@ -1,457 +0,0 @@
-package bulk_loading_cb
-
-import (
-	"fmt"
-	"github.com/couchbaselabs/sirius/internal/docgenerator"
-	"github.com/couchbaselabs/sirius/internal/task_errors"
-	"github.com/couchbaselabs/sirius/internal/task_result"
-	"github.com/couchbaselabs/sirius/internal/tasks"
-	"github.com/jaswdr/faker"
-	"golang.org/x/exp/slices"
-)
-
-// OperationConfig contains all the configuration for document operation.
-type OperationConfig struct {
-	Count            int64      `json:"count,omitempty" doc:"true"`
-	DocSize          int        `json:"docSize" doc:"true"`
-	DocType          string     `json:"docType,omitempty" doc:"true"`
-	KeySize          int        `json:"keySize,omitempty" doc:"true"`
-	KeyPrefix        string     `json:"keyPrefix" doc:"true"`
-	KeySuffix        string     `json:"keySuffix" doc:"true"`
-	ReadYourOwnWrite bool       `json:"readYourOwnWrite,omitempty" doc:"true"`
-	TemplateName     string     `json:"template" doc:"true"`
-	Start            int64      `json:"start" doc:"true"`
-	End              int64      `json:"end" doc:"true"`
-	FieldsToChange   []string   `json:"fieldsToChange" doc:"true"`
-	Exceptions       Exceptions `json:"exceptions,omitempty" doc:"true"`
-}
-
-// ConfigureOperationConfig configures and validate the OperationConfig
-func ConfigureOperationConfig(o *OperationConfig) error {
-	if o == nil {
-		return task_errors.ErrParsingOperatingConfig
-	}
-	if o.DocType == "" {
-		o.DocType = docgenerator.JsonDocument
-	}
-
-	if o.KeySize > docgenerator.DefaultKeySize {
-		o.KeySize = docgenerator.DefaultKeySize
-	}
-	if o.Count <= 0 {
-		o.Count = 1
-	}
-	if o.DocSize <= 0 {
-		o.DocSize = docgenerator.DefaultDocSize
-	}
-	if o.Start < 0 {
-		o.Start = 0
-		o.End = 0
-	}
-	if o.Start > o.End {
-		o.End = o.Start
-		return task_errors.ErrMalformedOperationRange
-	}
-	return nil
-}
-
-// checkBulkWriteOperation is used to check if the Write operation is on main doc or sub doc
-func checkBulkWriteOperation(operation string, subDocFlag bool) bool {
-	if subDocFlag {
-		switch operation {
-		case tasks.SubDocInsertOperation, tasks.SubDocUpsertOperation, tasks.SingleSubDocReplaceOperation:
-			return true
-		default:
-			return false
-		}
-	} else {
-		switch operation {
-		case tasks.InsertOperation, tasks.UpsertOperation, tasks.TouchOperation:
-			return true
-		default:
-			return false
-		}
-	}
-}
-
-// retrieveLastConfig retrieves the OperationConfig for the offset for a successful Sirius operation.
-func retrieveLastConfig(r *tasks.Request, offset int64, subDocFlag bool) (OperationConfig, error) {
-	if r == nil {
-		return OperationConfig{}, task_errors.ErrRequestIsNil
-	}
-	for i := range r.Tasks {
-		if checkBulkWriteOperation(r.Tasks[len(r.Tasks)-i-1].Operation, subDocFlag) {
-			task, ok := r.Tasks[len(r.Tasks)-i-1].Task.(BulkTask)
-			if ok {
-				operationConfig, taskState := task.GetOperationConfig()
-				if operationConfig == nil {
-					continue
-				} else {
-					if offset >= (operationConfig.Start) && (offset < operationConfig.End) {
-						if _, ok := taskState.ReturnCompletedOffset()[offset]; ok {
-							return *operationConfig, nil
-						}
-					}
-				}
-			}
-		}
-	}
-	return OperationConfig{}, task_errors.ErrNilOperationConfig
-}
-
-// retracePreviousFailedInsertions returns a lookup table representing the offsets which are not inserted properly..
-func retracePreviousFailedInsertions(r *tasks.Request, collectionIdentifier string,
-	resultSeed int64) (map[int64]struct{}, error) {
-	if r == nil {
-		return map[int64]struct{}{}, task_errors.ErrRequestIsNil
-	}
-	defer r.Unlock()
-	r.Lock()
-	result := make(map[int64]struct{})
-	for i := range r.Tasks {
-		td := r.Tasks[i]
-		if td.Operation == tasks.InsertOperation {
-			if task, ok := td.Task.(BulkTask); ok {
-				u, ok1 := task.(*InsertTask)
-				if ok1 {
-					if collectionIdentifier != u.CollectionIdentifier() {
-						continue
-					}
-					if resultSeed != u.ResultSeed {
-						errorOffSet := u.State.ReturnErrOffset()
-						for offSet, _ := range errorOffSet {
-							result[offSet] = struct{}{}
-						}
-					}
-				}
-			}
-		}
-	}
-	return result, nil
-}
-
-// retracePreviousDeletions returns a lookup table representing the offsets which are successfully deleted.
-func retracePreviousDeletions(r *tasks.Request, collectionIdentifier string, resultSeed int64) (map[int64]struct{},
-	error) {
-	if r == nil {
-		return map[int64]struct{}{}, task_errors.ErrRequestIsNil
-	}
-	defer r.Unlock()
-	r.Lock()
-	result := make(map[int64]struct{})
-	for i := range r.Tasks {
-		td := r.Tasks[i]
-		if td.Operation == tasks.DeleteOperation {
-			if task, ok := td.Task.(BulkTask); ok {
-				u, ok1 := task.(*DeleteTask)
-				if ok1 {
-					if collectionIdentifier != u.CollectionIdentifier() {
-						continue
-					}
-					if resultSeed != u.ResultSeed {
-						completedOffSet := u.State.ReturnCompletedOffset()
-						for deletedOffset, _ := range completedOffSet {
-							result[deletedOffset] = struct{}{}
-						}
-					}
-				}
-			}
-		}
-	}
-	return result, nil
-}
-
-// retracePreviousSubDocDeletions  returns a lookup table representing the offsets which are successfully deleted.
-func retracePreviousSubDocDeletions(r *tasks.Request, collectionIdentifier string,
-	resultSeed int64) (map[int64]struct{}, error) {
-	if r == nil {
-		return map[int64]struct{}{}, task_errors.ErrRequestIsNil
-	}
-	defer r.Unlock()
-	r.Lock()
-	result := make(map[int64]struct{})
-	if r == nil {
-		return result, task_errors.ErrRequestIsNil
-	}
-	for i := range r.Tasks {
-		td := r.Tasks[i]
-		if td.Operation == tasks.SubDocDeleteOperation {
-			if task, ok := td.Task.(BulkTask); ok {
-				u, ok1 := task.(*SubDocDelete)
-				if ok1 {
-					if collectionIdentifier != u.CollectionIdentifier() {
-						continue
-					}
-					if resultSeed != u.ResultSeed {
-						completedOffSet := u.State.ReturnCompletedOffset()
-						for deletedOffset, _ := range completedOffSet {
-							result[deletedOffset] = struct{}{}
-						}
-					}
-				}
-			}
-		}
-	}
-	return result, nil
-}
-
-// retracePreviousMutations returns an updated document after mutating the original documents.
-func retracePreviousMutations(r *tasks.Request, collectionIdentifier string, offset int64, doc interface{},
-	gen docgenerator.Generator, fake *faker.Faker, resultSeed int64) (interface{}, error) {
-	if r == nil {
-		return doc, task_errors.ErrRequestIsNil
-	}
-	defer r.Unlock()
-	r.Lock()
-	for i := range r.Tasks {
-		td := r.Tasks[i]
-		if td.Operation == tasks.UpsertOperation {
-
-			if tempX, ok := td.Task.(BulkTask); ok {
-				u, ok := tempX.(*UpsertTask)
-				if ok {
-					if collectionIdentifier != u.CollectionIdentifier() {
-						continue
-					}
-					if offset >= (u.OperationConfig.Start) && (offset < u.OperationConfig.End) && resultSeed != u.
-						ResultSeed {
-						if u.State == nil {
-							return doc, fmt.Errorf("Unable to retrace previous mutations on sirius for " + u.CollectionIdentifier())
-						}
-						errOffset := u.State.ReturnErrOffset()
-						if _, ok := errOffset[offset]; ok {
-							continue
-						} else {
-							doc, _ = gen.Template.UpdateDocument(u.OperationConfig.FieldsToChange, doc,
-								u.OperationConfig.DocSize, fake)
-						}
-					}
-
-				}
-			}
-
-		}
-	}
-	return doc, nil
-}
-
-func retracePreviousSubDocMutations(r *tasks.Request, collectionIdentifier string, offset int64,
-	gen docgenerator.Generator,
-	fake *faker.Faker, resultSeed int64,
-	subDocumentMap map[string]any) (map[string]any, error) {
-	if r == nil {
-		return map[string]any{}, task_errors.ErrRequestIsNil
-	}
-	defer r.Unlock()
-	r.Lock()
-	var result map[string]any = subDocumentMap
-	for i := range r.Tasks {
-		td := r.Tasks[i]
-		if td.Operation == tasks.SubDocUpsertOperation {
-			if task, ok := td.Task.(BulkTask); ok {
-				u, ok1 := task.(*SubDocUpsert)
-				if ok1 {
-					if collectionIdentifier != u.CollectionIdentifier() {
-						continue
-					}
-					if offset >= (u.OperationConfig.Start) && (offset < u.OperationConfig.End) && resultSeed != u.
-						ResultSeed {
-						errOffset := u.State.ReturnErrOffset()
-						if _, ok := errOffset[offset]; ok {
-							continue
-						} else {
-							result = gen.Template.GenerateSubPathAndValue(fake, u.OperationConfig.DocSize)
-						}
-					}
-				}
-			}
-		}
-	}
-	return result, nil
-}
-
-// countMutation return the number of mutation happened on an offset
-func countMutation(r *tasks.Request, collectionIdentifier string, offset int64, resultSeed int64) (int, error) {
-	if r == nil {
-		return 0, task_errors.ErrRequestIsNil
-	}
-	defer r.Unlock()
-	r.Lock()
-	var result int = 0
-	for i := range r.Tasks {
-		td := r.Tasks[i]
-		if td.Operation == tasks.SubDocUpsertOperation {
-			if task, ok := td.Task.(BulkTask); ok {
-				u, ok1 := task.(*SubDocUpsert)
-				if ok1 {
-					if collectionIdentifier != u.CollectionIdentifier() {
-						continue
-					}
-					if offset >= (u.OperationConfig.Start) && (offset < u.OperationConfig.End) && resultSeed != u.
-						ResultSeed {
-						completeOffset := u.State.ReturnCompletedOffset()
-						if _, ok := completeOffset[offset]; ok {
-							result++
-						}
-					}
-				}
-			}
-		} else if td.Operation == tasks.SubDocDeleteOperation {
-			if task, ok := td.Task.(BulkTask); ok {
-				u, ok1 := task.(*SubDocDelete)
-				if ok1 {
-					if collectionIdentifier != u.CollectionIdentifier() {
-						continue
-					}
-					if offset >= (u.OperationConfig.Start) && (offset < u.OperationConfig.End) && resultSeed != u.
-						ResultSeed {
-						completeOffset := u.State.ReturnCompletedOffset()
-						if _, ok := completeOffset[offset]; ok {
-							result++
-						}
-					}
-				}
-			}
-		} else if td.Operation == tasks.SubDocReplaceOperation {
-			if task, ok := td.Task.(BulkTask); ok {
-				u, ok1 := task.(*SubDocReplace)
-				if ok1 {
-					if collectionIdentifier != u.CollectionIdentifier() {
-						continue
-					}
-					if offset >= (u.OperationConfig.Start) && (offset < u.OperationConfig.End) && resultSeed != u.
-						ResultSeed {
-						completeOffset := u.State.ReturnCompletedOffset()
-						if _, ok := completeOffset[offset]; ok {
-							result++
-						}
-					}
-				}
-			}
-		} else if td.Operation == tasks.SubDocInsertOperation {
-			if task, ok := td.Task.(BulkTask); ok {
-				u, ok1 := task.(*SubDocInsert)
-				if ok1 {
-					if collectionIdentifier != u.CollectionIdentifier() {
-						continue
-					}
-					if offset >= (u.OperationConfig.Start) && (offset < u.OperationConfig.End) && resultSeed != u.
-						ResultSeed {
-						completeOffset := u.State.ReturnCompletedOffset()
-						if _, ok := completeOffset[offset]; ok {
-							result++
-						}
-					}
-				}
-			}
-		}
-	}
-	return result, nil
-
-}
-
-// shiftErrToCompletedOnRetrying will bring the offset which successfully completed their respective operation on
-// retrying
-func shiftErrToCompletedOnRetrying(exception string, result *task_result.TaskResult,
-	errorOffsetListMap []map[int64]RetriedResult, errorOffsetMaps, completedOffsetMaps map[int64]struct{}) {
-	if _, ok := result.BulkError[exception]; ok {
-		for _, x := range errorOffsetListMap {
-			for offset, retryResult := range x {
-				if retryResult.Status == true {
-					delete(errorOffsetMaps, offset)
-					completedOffsetMaps[offset] = struct{}{}
-					for index := range result.BulkError[exception] {
-						if result.BulkError[exception][index].Offset == offset {
-
-							offsetRetriedIndex := slices.IndexFunc(result.RetriedError[exception],
-								func(document task_result.FailedDocument) bool {
-									return document.Offset == offset
-								})
-
-							if offsetRetriedIndex == -1 {
-								result.RetriedError[exception] = append(result.RetriedError[exception], result.BulkError[exception][index])
-
-								result.RetriedError[exception][len(result.RetriedError[exception])-1].
-									Status = retryResult.Status
-
-								result.RetriedError[exception][len(result.RetriedError[exception])-1].
-									Cas = retryResult.CAS
-
-								result.RetriedError[exception][len(result.RetriedError[exception])-1].
-									SDKTiming.SendTime = retryResult.InitTime
-
-								result.RetriedError[exception][len(result.RetriedError[exception])-1].
-									SDKTiming.AckTime = retryResult.AckTime
-
-							} else {
-								result.RetriedError[exception][offsetRetriedIndex].Status = retryResult.Status
-								result.RetriedError[exception][offsetRetriedIndex].Cas = retryResult.CAS
-								result.RetriedError[exception][offsetRetriedIndex].SDKTiming.SendTime =
-									retryResult.InitTime
-								result.RetriedError[exception][offsetRetriedIndex].SDKTiming.AckTime =
-									retryResult.AckTime
-							}
-
-							result.BulkError[exception][index] = result.BulkError[exception][len(
-								result.BulkError[exception])-1]
-
-							result.BulkError[exception] = result.BulkError[exception][:len(
-								result.BulkError[exception])-1]
-
-							break
-						}
-					}
-				} else {
-					for index := range result.BulkError[exception] {
-						if result.BulkError[exception][index].Offset == offset {
-							result.BulkError[exception][index].SDKTiming.SendTime = retryResult.InitTime
-							result.BulkError[exception][index].SDKTiming.AckTime = retryResult.AckTime
-							result.RetriedError[exception] = append(result.RetriedError[exception],
-								result.BulkError[exception][index])
-							break
-						}
-					}
-				}
-			}
-		}
-	}
-}
-
-// shiftErrToCompletedOnIgnore will ignore retrying operation for offset lying in ignore exception category
-func shiftErrToCompletedOnIgnore(ignoreExceptions []string, result *task_result.TaskResult, errorOffsetMaps,
-	completedOffsetMaps map[int64]struct{}) {
-	for _, exception := range ignoreExceptions {
-		for _, failedDocs := range result.BulkError[exception] {
-			if _, ok := errorOffsetMaps[failedDocs.Offset]; ok {
-				delete(errorOffsetMaps, failedDocs.Offset)
-				completedOffsetMaps[failedDocs.Offset] = struct{}{}
-			}
-		}
-		delete(result.BulkError, exception)
-	}
-}
-
-type RetriedResult struct {
-	Status   bool   `json:"status" doc:"true"`
-	CAS      uint64 `json:"cas" doc:"true"`
-	InitTime string `json:"initTime" doc:"true"`
-	AckTime  string `json:"ackTime" doc:"true"`
-}
-
-type Exceptions struct {
-	IgnoreExceptions []string `json:"ignoreExceptions,omitempty" doc:"true"`
-	RetryExceptions  []string `json:"retryExceptions,omitempty" doc:"true"`
-	RetryAttempts    int      `json:"retryAttempts,omitempty" doc:"true"`
-}
-
-func GetExceptions(result *task_result.TaskResult, RetryExceptions []string) []string {
-	var exceptionList []string
-	if len(RetryExceptions) == 0 {
-		for exception, _ := range result.BulkError {
-			exceptionList = append(exceptionList, exception)
-		}
-	} else {
-		exceptionList = RetryExceptions
-	}
-	return exceptionList
-}
diff --git a/internal/tasks/bulk_loading_cb/task_bulk_delete.go b/internal/tasks/bulk_loading_cb/task_bulk_delete.go
deleted file mode 100644
index a5c4770..0000000
--- a/internal/tasks/bulk_loading_cb/task_bulk_delete.go
+++ /dev/null
@@ -1,366 +0,0 @@
-package bulk_loading_cb
-
-import (
-	"errors"
-	"fmt"
-	"github.com/couchbase/gocb/v2"
-	"github.com/couchbaselabs/sirius/internal/cb_sdk"
-	"github.com/couchbaselabs/sirius/internal/docgenerator"
-	"github.com/couchbaselabs/sirius/internal/meta_data"
-	"github.com/couchbaselabs/sirius/internal/task_errors"
-	"github.com/couchbaselabs/sirius/internal/task_result"
-	"github.com/couchbaselabs/sirius/internal/task_state"
-	"github.com/couchbaselabs/sirius/internal/tasks"
-	"github.com/couchbaselabs/sirius/internal/template"
-	"golang.org/x/sync/errgroup"
-	"log"
-	"math"
-	"strings"
-	"sync"
-	"time"
-)
-
-type DeleteTask struct {
-	IdentifierToken string                        `json:"identifierToken" doc:"true"`
-	ClusterConfig   *cb_sdk.ClusterConfig         `json:"clusterConfig" doc:"true"`
-	Bucket          string                        `json:"bucket" doc:"true"`
-	Scope           string                        `json:"scope,omitempty" doc:"true"`
-	Collection      string                        `json:"collection,omitempty" doc:"true"`
-	RemoveOptions   *cb_sdk.RemoveOptions         `json:"removeOptions,omitempty" doc:"true"`
-	OperationConfig *OperationConfig              `json:"operationConfig,omitempty" doc:"true"`
-	Operation       string                        `json:"operation" doc:"false"`
-	ResultSeed      int64                         `json:"resultSeed" doc:"false"`
-	TaskPending     bool                          `json:"taskPending" doc:"false"`
-	State           *task_state.TaskState         `json:"State" doc:"false"`
-	MetaData        *meta_data.CollectionMetaData `json:"metaData" doc:"false"`
-	Result          *task_result.TaskResult       `json:"-" doc:"false"`
-	gen             *docgenerator.Generator       `json:"-" doc:"false"`
-	req             *tasks.Request                `json:"-" doc:"false"`
-	rerun           bool                          `json:"-" doc:"false"`
-	lock            sync.Mutex                    `json:"-" doc:"false"`
-}
-
-func (task *DeleteTask) Describe() string {
-	return `Delete task deletes documents in bulk into a bucket.
-The task will delete documents from [start,end] inclusive.`
-}
-
-func (task *DeleteTask) CollectionIdentifier() string {
-	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
-	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
-		task.Collection}, ":")
-}
-
-func (task *DeleteTask) CheckIfPending() bool {
-	return task.TaskPending
-}
-
-// Config checks the validity of DeleteTask
-func (task *DeleteTask) Config(req *tasks.Request, reRun bool) (int64, error) {
-	task.TaskPending = true
-	task.req = req
-
-	if task.req == nil {
-		task.TaskPending = false
-		return 0, task_errors.ErrRequestIsNil
-	}
-
-	task.req.ReconnectionManager()
-	if _, err := task.req.GetCluster(task.ClusterConfig); err != nil {
-		task.TaskPending = false
-		return 0, err
-	}
-
-	task.lock = sync.Mutex{}
-	task.rerun = reRun
-
-	if !reRun {
-		task.ResultSeed = int64(time.Now().UnixNano())
-		task.Operation = tasks.DeleteOperation
-
-		if task.Bucket == "" {
-			task.Bucket = cb_sdk.DefaultBucket
-		}
-		if task.Scope == "" {
-			task.Scope = cb_sdk.DefaultScope
-		}
-		if task.Collection == "" {
-			task.Collection = cb_sdk.DefaultCollection
-		}
-
-		if err := cb_sdk.ConfigRemoveOptions(task.RemoveOptions); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-
-		if err := ConfigureOperationConfig(task.OperationConfig); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-
-		task.MetaData = task.req.MetaData.GetCollectionMetadata(task.CollectionIdentifier())
-
-		task.req.Lock()
-		task.State = task_state.ConfigTaskState(task.MetaData.Seed, task.MetaData.SeedEnd, task.ResultSeed)
-		task.req.Unlock()
-
-	} else {
-		if task.State == nil {
-			return task.ResultSeed, task_errors.ErrTaskStateIsNil
-		}
-
-		task.State.SetupStoringKeys()
-		_ = task_result.DeleteResultFile(task.ResultSeed)
-		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-	}
-	return task.ResultSeed, nil
-}
-
-func (task *DeleteTask) TearUp() error {
-	task.Result.StopStoringResult()
-	if err := task.Result.SaveResultIntoFile(); err != nil {
-		log.Println("not able to save Result into ", task.ResultSeed, task.Operation)
-	}
-	task.Result = nil
-	task.State.StopStoringState()
-	task.TaskPending = false
-	return task.req.SaveRequestIntoFile()
-}
-
-func (task *DeleteTask) Do() error {
-
-	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
-
-	collectionObject, err1 := task.GetCollectionObject()
-
-	task.gen = docgenerator.ConfigGenerator(
-		task.OperationConfig.KeySize,
-		task.OperationConfig.DocSize,
-		task.OperationConfig.DocType,
-		task.OperationConfig.KeyPrefix,
-		task.OperationConfig.KeySuffix,
-		template.InitialiseTemplate(task.OperationConfig.TemplateName))
-
-	if err1 != nil {
-		task.Result.ErrorOther = err1.Error()
-		task.Result.FailWholeBulkOperation(task.OperationConfig.Start, task.OperationConfig.End, err1, task.State,
-			task.gen, task.MetaData.Seed)
-		return task.TearUp()
-	}
-
-	deleteDocuments(task, collectionObject)
-	task.Result.Success = task.OperationConfig.End - task.OperationConfig.Start - task.Result.Failure
-
-	return task.TearUp()
-}
-
-// deleteDocuments delete the document stored on a host from start to end.
-func deleteDocuments(task *DeleteTask, collectionObject *cb_sdk.CollectionObject) {
-
-	if task.req.ContextClosed() {
-		return
-	}
-
-	skip := make(map[int64]struct{})
-	for _, offset := range task.State.KeyStates.Completed {
-		skip[offset] = struct{}{}
-	}
-	for _, offset := range task.State.KeyStates.Err {
-		skip[offset] = struct{}{}
-	}
-
-	routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
-	dataChannel := make(chan int64, tasks.MaxConcurrentRoutines)
-	group := errgroup.Group{}
-
-	for i := task.OperationConfig.Start; i < task.OperationConfig.End; i++ {
-
-		if task.req.ContextClosed() {
-			close(routineLimiter)
-			close(dataChannel)
-			return
-		}
-
-		routineLimiter <- struct{}{}
-		dataChannel <- i
-		group.Go(func() error {
-			offset := <-dataChannel
-			key := task.MetaData.Seed + offset
-			docId := task.gen.BuildKey(key)
-			if _, ok := skip[offset]; ok {
-				<-routineLimiter
-				return fmt.Errorf("alreday performed operation on " + docId)
-			}
-
-			var err error
-			initTime := time.Now().UTC().Format(time.RFC850)
-			for retry := 0; retry < int(math.Max(float64(1), float64(task.OperationConfig.Exceptions.
-				RetryAttempts))); retry++ {
-				initTime = time.Now().UTC().Format(time.RFC850)
-				_, err = collectionObject.Collection.Remove(docId, &gocb.RemoveOptions{
-					Cas:             gocb.Cas(task.RemoveOptions.Cas),
-					PersistTo:       task.RemoveOptions.PersistTo,
-					ReplicateTo:     task.RemoveOptions.ReplicateTo,
-					DurabilityLevel: cb_sdk.GetDurability(task.RemoveOptions.Durability),
-					Timeout:         time.Duration(task.RemoveOptions.Timeout) * time.Second,
-				})
-				if err == nil {
-					break
-				}
-			}
-			if err != nil {
-				if errors.Is(err, gocb.ErrDocumentNotFound) && task.rerun {
-					task.State.StateChannel <- task_state.StateHelper{Status: task_state.COMPLETED, Offset: offset}
-					<-routineLimiter
-					return nil
-				} else {
-					task.Result.IncrementFailure(initTime, docId, err, false, uint64(0), offset)
-					task.State.StateChannel <- task_state.StateHelper{Status: task_state.ERR, Offset: offset}
-					<-routineLimiter
-					return err
-				}
-			}
-
-			task.State.StateChannel <- task_state.StateHelper{Status: task_state.COMPLETED, Offset: offset}
-			<-routineLimiter
-			return nil
-		})
-	}
-	_ = group.Wait()
-	close(routineLimiter)
-	close(dataChannel)
-	task.PostTaskExceptionHandling(collectionObject)
-	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-}
-
-func (task *DeleteTask) PostTaskExceptionHandling(collectionObject *cb_sdk.CollectionObject) {
-	task.Result.StopStoringResult()
-	task.State.StopStoringState()
-	if task.OperationConfig.Exceptions.RetryAttempts <= 0 {
-		return
-	}
-
-	// Get all the errorOffset
-	errorOffsetMaps := task.State.ReturnErrOffset()
-	// Get all the completed offset
-	completedOffsetMaps := task.State.ReturnCompletedOffset()
-
-	// For the offset in ignore exceptions :-> move them from error to completed
-	shiftErrToCompletedOnIgnore(task.OperationConfig.Exceptions.IgnoreExceptions, task.Result, errorOffsetMaps, completedOffsetMaps)
-
-	if task.OperationConfig.Exceptions.RetryAttempts > 0 {
-
-		exceptionList := GetExceptions(task.Result, task.OperationConfig.Exceptions.RetryExceptions)
-
-		// For the retry exceptions :-> move them on success after retrying from err to completed
-		for _, exception := range exceptionList {
-
-			errorOffsetListMap := make([]map[int64]RetriedResult, 0)
-			for _, failedDocs := range task.Result.BulkError[exception] {
-				m := make(map[int64]RetriedResult)
-				m[failedDocs.Offset] = RetriedResult{}
-				errorOffsetListMap = append(errorOffsetListMap, m)
-			}
-
-			routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
-			dataChannel := make(chan map[int64]RetriedResult, tasks.MaxConcurrentRoutines)
-			wg := errgroup.Group{}
-			for _, x := range errorOffsetListMap {
-				dataChannel <- x
-				routineLimiter <- struct{}{}
-				wg.Go(func() error {
-					m := <-dataChannel
-					var offset = int64(-1)
-					for k, _ := range m {
-						offset = k
-					}
-					key := task.MetaData.Seed + offset
-					docId := task.gen.BuildKey(key)
-
-					retry := 0
-					var err error
-					result := &gocb.MutationResult{}
-
-					initTime := time.Now().UTC().Format(time.RFC850)
-					for retry = 0; retry <= task.OperationConfig.Exceptions.RetryAttempts; retry++ {
-						result, err = collectionObject.Collection.Remove(docId, &gocb.RemoveOptions{
-							Cas:             gocb.Cas(task.RemoveOptions.Cas),
-							PersistTo:       task.RemoveOptions.PersistTo,
-							ReplicateTo:     task.RemoveOptions.ReplicateTo,
-							DurabilityLevel: cb_sdk.GetDurability(task.RemoveOptions.Durability),
-							Timeout:         time.Duration(task.RemoveOptions.Timeout) * time.Second,
-						})
-
-						if err == nil {
-							break
-						}
-					}
-
-					if err != nil {
-						if errors.Is(err, gocb.ErrDocumentNotFound) {
-							m[offset] = RetriedResult{
-								Status:   true,
-								CAS:      0,
-								InitTime: initTime,
-								AckTime:  time.Now().UTC().Format(time.RFC850),
-							}
-						} else {
-							m[offset] = RetriedResult{
-								InitTime: initTime,
-								AckTime:  time.Now().UTC().Format(time.RFC850),
-							}
-						}
-					} else {
-						m[offset] = RetriedResult{
-							Status:   true,
-							CAS:      uint64(result.Cas()),
-							InitTime: initTime,
-							AckTime:  time.Now().UTC().Format(time.RFC850),
-						}
-					}
-
-					<-routineLimiter
-					return nil
-				})
-			}
-			_ = wg.Wait()
-
-			shiftErrToCompletedOnRetrying(exception, task.Result, errorOffsetListMap, errorOffsetMaps, completedOffsetMaps)
-		}
-	}
-
-	task.State.MakeCompleteKeyFromMap(completedOffsetMaps)
-	task.State.MakeErrorKeyFromMap(errorOffsetMaps)
-	task.Result.Failure = int64(len(task.State.KeyStates.Err))
-	task.Result.Success = task.OperationConfig.End - task.OperationConfig.Start - task.Result.Failure
-	log.Println("completed retrying:- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-
-}
-
-func (task *DeleteTask) MatchResultSeed(resultSeed string) (bool, error) {
-	defer task.lock.Unlock()
-	task.lock.Lock()
-	if fmt.Sprintf("%d", task.ResultSeed) == resultSeed {
-		if task.TaskPending {
-			return true, task_errors.ErrTaskInPendingState
-		}
-		if task.Result == nil {
-			task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
-		}
-		return true, nil
-	}
-	return false, nil
-}
-
-func (task *DeleteTask) GetCollectionObject() (*cb_sdk.CollectionObject, error) {
-	return task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope,
-		task.Collection)
-}
-
-func (task *DeleteTask) SetException(exceptions Exceptions) {
-	task.OperationConfig.Exceptions = exceptions
-}
-
-func (task *DeleteTask) GetOperationConfig() (*OperationConfig, *task_state.TaskState) {
-	return task.OperationConfig, task.State
-}
diff --git a/internal/tasks/bulk_loading_cb/task_bulk_insert.go b/internal/tasks/bulk_loading_cb/task_bulk_insert.go
deleted file mode 100644
index bfdbbca..0000000
--- a/internal/tasks/bulk_loading_cb/task_bulk_insert.go
+++ /dev/null
@@ -1,403 +0,0 @@
-package bulk_loading_cb
-
-import (
-	"errors"
-	"fmt"
-	"github.com/couchbase/gocb/v2"
-	"github.com/couchbaselabs/sirius/internal/cb_sdk"
-	"github.com/couchbaselabs/sirius/internal/docgenerator"
-	"github.com/couchbaselabs/sirius/internal/meta_data"
-	"github.com/couchbaselabs/sirius/internal/task_errors"
-	"github.com/couchbaselabs/sirius/internal/task_result"
-	"github.com/couchbaselabs/sirius/internal/task_state"
-	"github.com/couchbaselabs/sirius/internal/tasks"
-	"github.com/couchbaselabs/sirius/internal/template"
-	"github.com/jaswdr/faker"
-	"golang.org/x/sync/errgroup"
-	"log"
-	"math"
-	"math/rand"
-	"strings"
-	"sync"
-	"time"
-)
-
-type InsertTask struct {
-	IdentifierToken string                        `json:"identifierToken" doc:"true"`
-	ClusterConfig   *cb_sdk.ClusterConfig         `json:"clusterConfig" doc:"true"`
-	Bucket          string                        `json:"bucket" doc:"true"`
-	Scope           string                        `json:"scope,omitempty" doc:"true"`
-	Collection      string                        `json:"collection,omitempty" doc:"true"`
-	InsertOptions   *cb_sdk.InsertOptions         `json:"insertOptions,omitempty" doc:"true"`
-	OperationConfig *OperationConfig              `json:"operationConfig,omitempty" doc:"true"`
-	Operation       string                        `json:"operation" doc:"false"`
-	ResultSeed      int64                         `json:"resultSeed" doc:"false"`
-	TaskPending     bool                          `json:"taskPending" doc:"false"`
-	State           *task_state.TaskState         `json:"State" doc:"false"`
-	MetaData        *meta_data.CollectionMetaData `json:"metaData" doc:"false"`
-	Result          *task_result.TaskResult       `json:"-" doc:"false"`
-	gen             *docgenerator.Generator       `json:"-" doc:"false"`
-	req             *tasks.Request                `json:"-" doc:"false"`
-	rerun           bool                          `json:"-" doc:"false"`
-	lock            sync.Mutex                    `json:"-" doc:"false"`
-}
-
-func (task *InsertTask) Describe() string {
-	return " Insert task uploads documents in bulk into a bucket.\n" +
-		"The durability while inserting a document can be set using following values in the 'durability' JSON tag :-\n" +
-		"1. MAJORITY\n" +
-		"2. MAJORITY_AND_PERSIST_TO_ACTIVE\n" +
-		"3. PERSIST_TO_MAJORITY\n"
-}
-
-func (task *InsertTask) CollectionIdentifier() string {
-	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
-	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
-		task.Collection}, ":")
-}
-
-func (task *InsertTask) CheckIfPending() bool {
-	return task.TaskPending
-}
-
-// Config configures  the insert task
-func (task *InsertTask) Config(req *tasks.Request, reRun bool) (int64, error) {
-	task.TaskPending = true
-	task.req = req
-
-	if task.req == nil {
-		task.TaskPending = false
-		return 0, task_errors.ErrRequestIsNil
-	}
-
-	task.req.ReconnectionManager()
-	if _, err := task.req.GetCluster(task.ClusterConfig); err != nil {
-		task.TaskPending = false
-		return 0, err
-	}
-
-	task.lock = sync.Mutex{}
-	task.rerun = reRun
-
-	if !reRun {
-		task.ResultSeed = int64(time.Now().UnixNano())
-		task.Operation = tasks.InsertOperation
-
-		if task.Bucket == "" {
-			task.Bucket = cb_sdk.DefaultBucket
-		}
-		if task.Scope == "" {
-			task.Scope = cb_sdk.DefaultScope
-		}
-		if task.Collection == "" {
-			task.Collection = cb_sdk.DefaultCollection
-		}
-
-		if err := cb_sdk.ConfigInsertOptions(task.InsertOptions); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-
-		if err := ConfigureOperationConfig(task.OperationConfig); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-
-		task.MetaData = task.req.MetaData.GetCollectionMetadata(task.CollectionIdentifier())
-
-		task.req.Lock()
-		if task.OperationConfig.End+task.MetaData.Seed > task.MetaData.SeedEnd {
-			task.req.AddToSeedEnd(task.MetaData, (task.OperationConfig.End+task.MetaData.Seed)-(task.MetaData.SeedEnd))
-		}
-
-		task.State = task_state.ConfigTaskState(task.MetaData.Seed, task.MetaData.SeedEnd, task.ResultSeed)
-		task.req.Unlock()
-
-	} else {
-		if task.State == nil {
-			return task.ResultSeed, task_errors.ErrTaskStateIsNil
-		}
-		task.State.SetupStoringKeys()
-		_ = task_result.DeleteResultFile(task.ResultSeed)
-		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-	}
-	return task.ResultSeed, nil
-}
-
-func (task *InsertTask) TearUp() error {
-	//Use this case to store task's state on disk when required
-	//if err := task.State.SaveTaskSateOnDisk(); err != nil {
-	//	log.Println("Error in storing TASK State on DISK")
-	//}
-	task.Result.StopStoringResult()
-	if err := task.Result.SaveResultIntoFile(); err != nil {
-		log.Println("not able to save Result into ", task.ResultSeed, task.Operation)
-	}
-	task.Result = nil
-	task.State.StopStoringState()
-	task.TaskPending = false
-	return task.req.SaveRequestIntoFile()
-}
-
-func (task *InsertTask) Do() error {
-
-	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
-
-	collectionObject, err1 := task.GetCollectionObject()
-
-	task.gen = docgenerator.ConfigGenerator(
-		task.OperationConfig.KeySize,
-		task.OperationConfig.DocSize,
-		task.OperationConfig.DocType,
-		task.OperationConfig.KeyPrefix,
-		task.OperationConfig.KeySuffix,
-		template.InitialiseTemplate(task.OperationConfig.TemplateName))
-
-	if err1 != nil {
-		task.Result.ErrorOther = err1.Error()
-		task.Result.FailWholeBulkOperation(task.OperationConfig.Start, task.OperationConfig.End,
-			err1, task.State, task.gen, task.MetaData.Seed)
-		return task.TearUp()
-	}
-
-	insertDocuments(task, collectionObject)
-	task.Result.Success = task.OperationConfig.End - task.OperationConfig.Start - task.Result.Failure
-
-	return task.TearUp()
-}
-
-// insertDocuments uploads new documents in a bucket.scope.collection in a defined batch size at multiple iterations.
-func insertDocuments(task *InsertTask, collectionObject *cb_sdk.CollectionObject) {
-
-	if task.req.ContextClosed() {
-		return
-	}
-
-	routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
-	dataChannel := make(chan int64, tasks.MaxConcurrentRoutines)
-
-	skip := make(map[int64]struct{})
-	for _, offset := range task.State.KeyStates.Completed {
-		skip[offset] = struct{}{}
-	}
-	for _, offset := range task.State.KeyStates.Err {
-		skip[offset] = struct{}{}
-	}
-	group := errgroup.Group{}
-	for iteration := task.OperationConfig.Start; iteration < task.OperationConfig.End; iteration++ {
-
-		if task.req.ContextClosed() {
-			close(routineLimiter)
-			close(dataChannel)
-			return
-		}
-
-		routineLimiter <- struct{}{}
-		dataChannel <- iteration
-		group.Go(func() error {
-			offset := <-dataChannel
-			key := offset + task.MetaData.Seed
-			docId := task.gen.BuildKey(key)
-
-			if _, ok := skip[offset]; ok {
-				<-routineLimiter
-				return fmt.Errorf("alreday performed operation on " + docId)
-			}
-			fake := faker.NewWithSeed(rand.NewSource(int64(key)))
-
-			initTime := time.Now().UTC().Format(time.RFC850)
-			doc, err := task.gen.Template.GenerateDocument(&fake, task.OperationConfig.DocSize)
-			if err != nil {
-				task.Result.IncrementFailure(initTime, docId, err, false, 0, offset)
-				<-routineLimiter
-				return err
-			}
-
-			for retry := 0; retry < int(math.Max(float64(1), float64(task.OperationConfig.Exceptions.
-				RetryAttempts))); retry++ {
-				initTime = time.Now().UTC().Format(time.RFC850)
-				_, err = collectionObject.Collection.Insert(docId, doc, &gocb.InsertOptions{
-					DurabilityLevel: cb_sdk.GetDurability(task.InsertOptions.Durability),
-					PersistTo:       task.InsertOptions.PersistTo,
-					ReplicateTo:     task.InsertOptions.ReplicateTo,
-					Timeout:         time.Duration(task.InsertOptions.Timeout) * time.Second,
-					Expiry:          time.Duration(task.InsertOptions.Expiry) * time.Second,
-				})
-				if err == nil {
-					break
-				}
-			}
-
-			if err != nil {
-				if errors.Is(err, gocb.ErrDocumentExists) && task.rerun {
-					task.State.StateChannel <- task_state.StateHelper{Status: task_state.COMPLETED, Offset: offset}
-					<-routineLimiter
-					return nil
-				} else {
-					task.Result.IncrementFailure(initTime, docId, err, false, 0, offset)
-					task.State.StateChannel <- task_state.StateHelper{Status: task_state.ERR, Offset: offset}
-					<-routineLimiter
-					return err
-				}
-			}
-
-			task.State.StateChannel <- task_state.StateHelper{Status: task_state.COMPLETED, Offset: offset}
-			<-routineLimiter
-			return nil
-		})
-	}
-
-	_ = group.Wait()
-	close(routineLimiter)
-	close(dataChannel)
-	task.PostTaskExceptionHandling(collectionObject)
-	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-}
-
-func (task *InsertTask) PostTaskExceptionHandling(collectionObject *cb_sdk.CollectionObject) {
-	task.Result.StopStoringResult()
-	task.State.StopStoringState()
-
-	if task.OperationConfig.Exceptions.RetryAttempts <= 0 {
-		return
-	}
-
-	// Get all the errorOffset
-	errorOffsetMaps := task.State.ReturnErrOffset()
-	// Get all the completed offset
-	completedOffsetMaps := task.State.ReturnCompletedOffset()
-
-	// For the offset in ignore exceptions :-> move them from error to completed
-	shiftErrToCompletedOnIgnore(task.OperationConfig.Exceptions.IgnoreExceptions, task.Result, errorOffsetMaps,
-		completedOffsetMaps)
-
-	if task.OperationConfig.Exceptions.RetryAttempts > 0 {
-
-		exceptionList := GetExceptions(task.Result, task.OperationConfig.Exceptions.RetryExceptions)
-
-		// For the retry exceptions :-> move them on success after retrying from err to completed
-		for _, exception := range exceptionList {
-
-			errorOffsetListMap := make([]map[int64]RetriedResult, 0)
-			for _, failedDocs := range task.Result.BulkError[exception] {
-				m := make(map[int64]RetriedResult)
-				m[failedDocs.Offset] = RetriedResult{}
-				errorOffsetListMap = append(errorOffsetListMap, m)
-			}
-
-			routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
-			dataChannel := make(chan map[int64]RetriedResult, tasks.MaxConcurrentRoutines)
-			wg := errgroup.Group{}
-			for _, x := range errorOffsetListMap {
-				dataChannel <- x
-				routineLimiter <- struct{}{}
-				wg.Go(func() error {
-					m := <-dataChannel
-					var offset = int64(-1)
-					for k, _ := range m {
-						offset = k
-					}
-					key := offset + task.MetaData.Seed
-					docId := task.gen.BuildKey(key)
-
-					fake := faker.NewWithSeed(rand.NewSource(int64(key)))
-					doc, _ := task.gen.Template.GenerateDocument(&fake, task.OperationConfig.DocSize)
-
-					retry := 0
-					var err error
-					result := &gocb.MutationResult{}
-
-					initTime := time.Now().UTC().Format(time.RFC850)
-
-					for retry = 0; retry <= task.OperationConfig.Exceptions.RetryAttempts; retry++ {
-						result, err = collectionObject.Collection.Insert(docId, doc, &gocb.InsertOptions{
-							DurabilityLevel: cb_sdk.GetDurability(task.InsertOptions.Durability),
-							PersistTo:       task.InsertOptions.PersistTo,
-							ReplicateTo:     task.InsertOptions.ReplicateTo,
-							Timeout:         time.Duration(task.InsertOptions.Timeout) * time.Second,
-							Expiry:          time.Duration(task.InsertOptions.Expiry) * time.Second,
-						})
-
-						if err == nil {
-							break
-						}
-					}
-
-					if err != nil {
-						if errors.Is(err, gocb.ErrDocumentExists) {
-							if tempResult, err1 := collectionObject.Collection.Get(docId, &gocb.GetOptions{
-								Timeout: 5 * time.Second,
-							}); err1 == nil {
-								m[offset] = RetriedResult{
-									Status:   true,
-									CAS:      uint64(tempResult.Cas()),
-									InitTime: initTime,
-									AckTime:  time.Now().UTC().Format(time.RFC850),
-								}
-							} else {
-								m[offset] = RetriedResult{
-									Status:   true,
-									InitTime: initTime,
-									AckTime:  time.Now().UTC().Format(time.RFC850),
-								}
-							}
-						} else {
-							m[offset] = RetriedResult{
-								InitTime: initTime,
-								AckTime:  time.Now().UTC().Format(time.RFC850),
-							}
-						}
-					} else {
-						m[offset] = RetriedResult{
-							Status:   true,
-							CAS:      uint64(result.Cas()),
-							InitTime: initTime,
-							AckTime:  time.Now().UTC().Format(time.RFC850),
-						}
-					}
-
-					<-routineLimiter
-					return nil
-				})
-			}
-			_ = wg.Wait()
-
-			shiftErrToCompletedOnRetrying(exception, task.Result, errorOffsetListMap, errorOffsetMaps,
-				completedOffsetMaps)
-		}
-	}
-
-	task.State.MakeCompleteKeyFromMap(completedOffsetMaps)
-	task.State.MakeErrorKeyFromMap(errorOffsetMaps)
-	task.Result.Failure = int64(len(task.State.KeyStates.Err))
-	task.Result.Success = task.OperationConfig.End - task.OperationConfig.Start - task.Result.Failure
-	log.Println("completed retrying:- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-}
-
-func (task *InsertTask) MatchResultSeed(resultSeed string) (bool, error) {
-	defer task.lock.Unlock()
-	task.lock.Lock()
-	if fmt.Sprintf("%d", task.ResultSeed) == resultSeed {
-		if task.TaskPending {
-			return true, task_errors.ErrTaskInPendingState
-		}
-		if task.Result == nil {
-			task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
-		}
-		return true, nil
-	}
-	return false, nil
-}
-
-func (task *InsertTask) GetCollectionObject() (*cb_sdk.CollectionObject, error) {
-	return task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope,
-		task.Collection)
-}
-
-func (task *InsertTask) SetException(exceptions Exceptions) {
-	task.OperationConfig.Exceptions = exceptions
-}
-
-func (task *InsertTask) GetOperationConfig() (*OperationConfig, *task_state.TaskState) {
-	return task.OperationConfig, task.State
-}
diff --git a/internal/tasks/bulk_loading_cb/task_bulk_read.go b/internal/tasks/bulk_loading_cb/task_bulk_read.go
deleted file mode 100644
index 19d3129..0000000
--- a/internal/tasks/bulk_loading_cb/task_bulk_read.go
+++ /dev/null
@@ -1,329 +0,0 @@
-package bulk_loading_cb
-
-import (
-	"fmt"
-	"github.com/couchbase/gocb/v2"
-	"github.com/couchbaselabs/sirius/internal/cb_sdk"
-	"github.com/couchbaselabs/sirius/internal/docgenerator"
-	"github.com/couchbaselabs/sirius/internal/meta_data"
-	"github.com/couchbaselabs/sirius/internal/task_errors"
-	"github.com/couchbaselabs/sirius/internal/task_result"
-	"github.com/couchbaselabs/sirius/internal/task_state"
-	"github.com/couchbaselabs/sirius/internal/tasks"
-	"github.com/couchbaselabs/sirius/internal/template"
-	"golang.org/x/sync/errgroup"
-	"log"
-	"math"
-	"strings"
-	"sync"
-	"time"
-)
-
-type ReadTask struct {
-	IdentifierToken string                        `json:"identifierToken" doc:"true"`
-	ClusterConfig   *cb_sdk.ClusterConfig         `json:"clusterConfig" doc:"true"`
-	Bucket          string                        `json:"bucket" doc:"true"`
-	Scope           string                        `json:"scope,omitempty" doc:"true"`
-	Collection      string                        `json:"collection,omitempty" doc:"true"`
-	OperationConfig *OperationConfig              `json:"operationConfig,omitempty" doc:"true"`
-	Operation       string                        `json:"operation" doc:"false"`
-	ResultSeed      int64                         `json:"resultSeed" doc:"false"`
-	TaskPending     bool                          `json:"taskPending" doc:"false"`
-	State           *task_state.TaskState         `json:"State" doc:"false"`
-	MetaData        *meta_data.CollectionMetaData `json:"metaData" doc:"false"`
-	Result          *task_result.TaskResult       `json:"Result" doc:"false"`
-	gen             *docgenerator.Generator       `json:"-" doc:"false"`
-	req             *tasks.Request                `json:"-" doc:"false"`
-	rerun           bool                          `json:"-" doc:"false"`
-	lock            sync.Mutex                    `json:"-" doc:"false"`
-}
-
-func (task *ReadTask) Describe() string {
-	return "Read BulkTask get documents from bucket and validate them with the expected ones"
-}
-
-func (task *ReadTask) CollectionIdentifier() string {
-	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
-	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
-		task.Collection}, ":")
-}
-
-func (task *ReadTask) CheckIfPending() bool {
-	return task.TaskPending
-}
-
-func (task *ReadTask) TearUp() error {
-	task.Result.StopStoringResult()
-	if err := task.Result.SaveResultIntoFile(); err != nil {
-		log.Println("not able to save Result into ", task.ResultSeed, task.Operation)
-	}
-	task.Result = nil
-	task.State.StopStoringState()
-	task.TaskPending = false
-	return task.req.SaveRequestIntoFile()
-}
-
-func (task *ReadTask) Config(req *tasks.Request, reRun bool) (int64, error) {
-	task.TaskPending = true
-	task.req = req
-
-	if task.req == nil {
-		task.TaskPending = false
-		return 0, task_errors.ErrRequestIsNil
-	}
-
-	task.req.ReconnectionManager()
-	if _, err := task.req.GetCluster(task.ClusterConfig); err != nil {
-		task.TaskPending = false
-		return 0, err
-	}
-
-	task.lock = sync.Mutex{}
-	task.rerun = reRun
-
-	if !reRun {
-		task.ResultSeed = int64(time.Now().UnixNano())
-		task.Operation = tasks.ReadOperation
-
-		if task.Bucket == "" {
-			task.Bucket = cb_sdk.DefaultBucket
-		}
-		if task.Scope == "" {
-			task.Scope = cb_sdk.DefaultScope
-		}
-		if task.Collection == "" {
-			task.Collection = cb_sdk.DefaultCollection
-		}
-
-		if err := ConfigureOperationConfig(task.OperationConfig); err != nil {
-			task.TaskPending = false
-			return 0, fmt.Errorf(err.Error())
-		}
-
-		task.MetaData = task.req.MetaData.GetCollectionMetadata(task.CollectionIdentifier())
-		task.req.Lock()
-		task.State = task_state.ConfigTaskState(task.MetaData.Seed, task.MetaData.SeedEnd, task.ResultSeed)
-		task.req.Unlock()
-
-	} else {
-		if task.State == nil {
-			return task.ResultSeed, task_errors.ErrTaskStateIsNil
-		}
-
-		task.State.SetupStoringKeys()
-		_ = task_result.DeleteResultFile(task.ResultSeed)
-		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-	}
-	return task.ResultSeed, nil
-}
-
-func (task *ReadTask) Do() error {
-
-	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
-
-	collectionObject, err1 := task.GetCollectionObject()
-
-	task.gen = docgenerator.ConfigGenerator(
-		task.OperationConfig.KeySize,
-		task.OperationConfig.DocSize,
-		task.OperationConfig.DocType,
-		task.OperationConfig.KeyPrefix,
-		task.OperationConfig.KeySuffix,
-		template.InitialiseTemplate(task.OperationConfig.TemplateName))
-
-	if err1 != nil {
-		task.Result.ErrorOther = err1.Error()
-		task.Result.FailWholeBulkOperation(task.OperationConfig.Start, task.OperationConfig.End,
-			err1, task.State, task.gen, task.MetaData.Seed)
-		return task.TearUp()
-	}
-
-	getDocuments(task, collectionObject)
-	task.Result.Success = task.OperationConfig.End - task.OperationConfig.Start - task.Result.Failure
-
-	return task.TearUp()
-}
-
-// getDocuments reads the documents in the bucket
-func getDocuments(task *ReadTask, collectionObject *cb_sdk.CollectionObject) {
-
-	if task.req.ContextClosed() {
-		return
-	}
-
-	routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
-	dataChannel := make(chan int64, tasks.MaxConcurrentRoutines)
-	skip := make(map[int64]struct{})
-	for _, offset := range task.State.KeyStates.Completed {
-		skip[offset] = struct{}{}
-	}
-	for _, offset := range task.State.KeyStates.Err {
-		skip[offset] = struct{}{}
-	}
-
-	group := errgroup.Group{}
-	for i := task.OperationConfig.Start; i < task.OperationConfig.End; i++ {
-
-		if task.req.ContextClosed() {
-			close(routineLimiter)
-			close(dataChannel)
-			return
-		}
-
-		routineLimiter <- struct{}{}
-		dataChannel <- i
-		group.Go(func() error {
-			offset := <-dataChannel
-			key := task.MetaData.Seed + offset
-			docId := task.gen.BuildKey(key)
-			if _, ok := skip[offset]; ok {
-				<-routineLimiter
-				return fmt.Errorf("alreday performed operation on " + docId)
-			}
-
-			initTime := time.Now().UTC().Format(time.RFC850)
-			var err error
-			for retry := 0; retry < int(math.Max(float64(1), float64(task.OperationConfig.Exceptions.
-				RetryAttempts))); retry++ {
-				initTime = time.Now().UTC().Format(time.RFC850)
-				_, err = collectionObject.Collection.Get(docId, nil)
-				if err == nil {
-					break
-				}
-			}
-
-			if err != nil {
-				task.State.StateChannel <- task_state.StateHelper{Status: task_state.ERR, Offset: offset}
-				task.Result.IncrementFailure(initTime, docId, err, false, 0, offset)
-				<-routineLimiter
-				return err
-			}
-
-			task.State.StateChannel <- task_state.StateHelper{Status: task_state.COMPLETED, Offset: offset}
-			<-routineLimiter
-			return nil
-		})
-	}
-	_ = group.Wait()
-	close(routineLimiter)
-	close(dataChannel)
-	task.PostTaskExceptionHandling(collectionObject)
-	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-}
-
-func (task *ReadTask) PostTaskExceptionHandling(collectionObject *cb_sdk.CollectionObject) {
-	task.Result.StopStoringResult()
-	task.State.StopStoringState()
-	if task.OperationConfig.Exceptions.RetryAttempts <= 0 {
-		return
-	}
-
-	// Get all the errorOffset
-	errorOffsetMaps := task.State.ReturnErrOffset()
-	// Get all the completed offset
-	completedOffsetMaps := task.State.ReturnCompletedOffset()
-
-	// For the offset in ignore exceptions :-> move them from error to completed
-	shiftErrToCompletedOnIgnore(task.OperationConfig.Exceptions.IgnoreExceptions, task.Result, errorOffsetMaps,
-		completedOffsetMaps)
-
-	if task.OperationConfig.Exceptions.RetryAttempts > 0 {
-
-		exceptionList := GetExceptions(task.Result, task.OperationConfig.Exceptions.RetryExceptions)
-
-		// For the retry exceptions :-> move them on success after retrying from err to completed
-		for _, exception := range exceptionList {
-
-			errorOffsetListMap := make([]map[int64]RetriedResult, 0)
-			for _, failedDocs := range task.Result.BulkError[exception] {
-				m := make(map[int64]RetriedResult)
-				m[failedDocs.Offset] = RetriedResult{}
-				errorOffsetListMap = append(errorOffsetListMap, m)
-			}
-
-			routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
-			dataChannel := make(chan map[int64]RetriedResult, tasks.MaxConcurrentRoutines)
-			wg := errgroup.Group{}
-			for _, x := range errorOffsetListMap {
-				dataChannel <- x
-				routineLimiter <- struct{}{}
-				wg.Go(func() error {
-					m := <-dataChannel
-					var offset = int64(-1)
-					for k, _ := range m {
-						offset = k
-					}
-					key := task.MetaData.Seed + offset
-					docId := task.gen.BuildKey(key)
-
-					retry := 0
-					result := &gocb.GetResult{}
-					var err error
-					initTime := time.Now().UTC().Format(time.RFC850)
-					for retry = 0; retry <= task.OperationConfig.Exceptions.RetryAttempts; retry++ {
-						result, err = collectionObject.Collection.Get(docId, nil)
-
-						if err == nil {
-							break
-						}
-					}
-
-					if err == nil {
-						m[offset] = RetriedResult{
-							Status:   true,
-							CAS:      uint64(result.Cas()),
-							InitTime: initTime,
-							AckTime:  time.Now().UTC().Format(time.RFC850),
-						}
-					} else {
-						m[offset] = RetriedResult{
-							InitTime: initTime,
-							AckTime:  time.Now().UTC().Format(time.RFC850),
-						}
-					}
-
-					<-routineLimiter
-					return nil
-				})
-			}
-			_ = wg.Wait()
-
-			shiftErrToCompletedOnRetrying(exception, task.Result, errorOffsetListMap, errorOffsetMaps,
-				completedOffsetMaps)
-		}
-	}
-
-	task.State.MakeCompleteKeyFromMap(completedOffsetMaps)
-	task.State.MakeErrorKeyFromMap(errorOffsetMaps)
-	task.Result.Failure = int64(len(task.State.KeyStates.Err))
-	task.Result.Success = task.OperationConfig.End - task.OperationConfig.Start - task.Result.Failure
-	log.Println("completed retrying:- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-}
-
-func (task *ReadTask) MatchResultSeed(resultSeed string) (bool, error) {
-	defer task.lock.Unlock()
-	task.lock.Lock()
-	if fmt.Sprintf("%d", task.ResultSeed) == resultSeed {
-		if task.TaskPending {
-			return true, task_errors.ErrTaskInPendingState
-		}
-		if task.Result == nil {
-			task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
-		}
-		return true, nil
-	}
-	return false, nil
-}
-
-func (task *ReadTask) GetCollectionObject() (*cb_sdk.CollectionObject, error) {
-	return task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope,
-		task.Collection)
-}
-
-func (task *ReadTask) SetException(exceptions Exceptions) {
-	task.OperationConfig.Exceptions = exceptions
-}
-
-func (task *ReadTask) GetOperationConfig() (*OperationConfig, *task_state.TaskState) {
-	return task.OperationConfig, task.State
-}
diff --git a/internal/tasks/bulk_loading_cb/task_bulk_touch.go b/internal/tasks/bulk_loading_cb/task_bulk_touch.go
deleted file mode 100644
index a85c0f5..0000000
--- a/internal/tasks/bulk_loading_cb/task_bulk_touch.go
+++ /dev/null
@@ -1,351 +0,0 @@
-package bulk_loading_cb
-
-import (
-	"fmt"
-	"github.com/couchbase/gocb/v2"
-	"github.com/couchbaselabs/sirius/internal/cb_sdk"
-	"github.com/couchbaselabs/sirius/internal/docgenerator"
-	"github.com/couchbaselabs/sirius/internal/meta_data"
-	"github.com/couchbaselabs/sirius/internal/task_errors"
-	"github.com/couchbaselabs/sirius/internal/task_result"
-	"github.com/couchbaselabs/sirius/internal/task_state"
-	"github.com/couchbaselabs/sirius/internal/tasks"
-	"github.com/couchbaselabs/sirius/internal/template"
-	"golang.org/x/sync/errgroup"
-	"log"
-	"math"
-	"strings"
-	"sync"
-	"time"
-)
-
-type TouchTask struct {
-	IdentifierToken string                        `json:"identifierToken" doc:"true"`
-	ClusterConfig   *cb_sdk.ClusterConfig         `json:"clusterConfig" doc:"true"`
-	Bucket          string                        `json:"bucket" doc:"true"`
-	Scope           string                        `json:"scope,omitempty" doc:"true"`
-	Collection      string                        `json:"collection,omitempty" doc:"true"`
-	TouchOptions    *cb_sdk.TouchOptions          `json:"touchOptions,omitempty" doc:"true"`
-	Expiry          int64                         `json:"expiry" doc:"true"`
-	OperationConfig *OperationConfig              `json:"operationConfig,omitempty" doc:"true"`
-	Operation       string                        `json:"operation" doc:"false"`
-	ResultSeed      int64                         `json:"resultSeed" doc:"false"`
-	TaskPending     bool                          `json:"taskPending" doc:"false"`
-	State           *task_state.TaskState         `json:"State" doc:"false"`
-	MetaData        *meta_data.CollectionMetaData `json:"metaData" doc:"false"`
-	Result          *task_result.TaskResult       `json:"-" doc:"false"`
-	gen             *docgenerator.Generator       `json:"-" doc:"false"`
-	req             *tasks.Request                `json:"-" doc:"false"`
-	rerun           bool                          `json:"-" doc:"false"`
-	lock            sync.Mutex                    `json:"-" doc:"false"`
-}
-
-func (task *TouchTask) CollectionIdentifier() string {
-	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
-	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
-		task.Collection}, ":")
-}
-
-func (task *TouchTask) Describe() string {
-	return `Upsert task mutates documents in bulk into a bucket.
-The task will update the fields in a documents ranging from [start,end] inclusive.
-We need to share the fields we want to update in a json document using SQL++ syntax.`
-}
-
-func (task *TouchTask) CheckIfPending() bool {
-	return task.TaskPending
-}
-
-func (task *TouchTask) Config(req *tasks.Request, reRun bool) (int64, error) {
-	task.TaskPending = true
-	task.req = req
-
-	if task.req == nil {
-		task.TaskPending = false
-		return 0, task_errors.ErrRequestIsNil
-	}
-
-	task.req.ReconnectionManager()
-	if _, err := task.req.GetCluster(task.ClusterConfig); err != nil {
-		task.TaskPending = false
-		return 0, err
-	}
-
-	task.lock = sync.Mutex{}
-	task.rerun = reRun
-
-	if !reRun {
-		task.ResultSeed = int64(time.Now().UnixNano())
-		task.Operation = tasks.TouchOperation
-
-		if task.Bucket == "" {
-			task.Bucket = cb_sdk.DefaultBucket
-		}
-		if task.Scope == "" {
-			task.Scope = cb_sdk.DefaultScope
-		}
-		if task.Collection == "" {
-			task.Collection = cb_sdk.DefaultCollection
-		}
-
-		if err := cb_sdk.ConfigTouchOptions(task.TouchOptions); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-
-		if err := ConfigureOperationConfig(task.OperationConfig); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-
-		task.MetaData = task.req.MetaData.GetCollectionMetadata(task.CollectionIdentifier())
-
-		task.req.Lock()
-		if task.OperationConfig.End+task.MetaData.Seed > task.MetaData.SeedEnd {
-			task.req.AddToSeedEnd(task.MetaData, (task.OperationConfig.End+task.MetaData.Seed)-(task.MetaData.SeedEnd))
-		}
-		task.State = task_state.ConfigTaskState(task.MetaData.Seed, task.MetaData.SeedEnd, task.ResultSeed)
-		task.req.Unlock()
-
-	} else {
-		if task.State == nil {
-			return task.ResultSeed, task_errors.ErrTaskStateIsNil
-		}
-
-		task.State.SetupStoringKeys()
-		_ = task_result.DeleteResultFile(task.ResultSeed)
-		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-	}
-	return task.ResultSeed, nil
-}
-
-func (task *TouchTask) TearUp() error {
-	task.Result.StopStoringResult()
-	if err := task.Result.SaveResultIntoFile(); err != nil {
-		log.Println("not able to save Result into ", task.ResultSeed, task.Operation)
-	}
-	task.Result.StopStoringResult()
-	task.Result = nil
-	task.State.StopStoringState()
-	task.TaskPending = false
-	return task.req.SaveRequestIntoFile()
-}
-
-func (task *TouchTask) Do() error {
-
-	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
-
-	collectionObject, err1 := task.GetCollectionObject()
-
-	task.gen = docgenerator.ConfigGenerator(
-		task.OperationConfig.KeySize,
-		task.OperationConfig.DocSize,
-		task.OperationConfig.DocType,
-		task.OperationConfig.KeyPrefix,
-		task.OperationConfig.KeySuffix,
-		template.InitialiseTemplate(task.OperationConfig.TemplateName))
-
-	if err1 != nil {
-		task.Result.ErrorOther = err1.Error()
-		task.Result.FailWholeBulkOperation(task.OperationConfig.Start, task.OperationConfig.End,
-			err1, task.State, task.gen, task.MetaData.Seed)
-		return task.TearUp()
-	}
-
-	touchDocuments(task, collectionObject)
-	task.Result.Success = task.OperationConfig.End - task.OperationConfig.Start - task.Result.Failure
-
-	return task.TearUp()
-}
-
-func touchDocuments(task *TouchTask, collectionObject *cb_sdk.CollectionObject) {
-
-	if task.req.ContextClosed() {
-		return
-	}
-
-	routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
-	dataChannel := make(chan int64, tasks.MaxConcurrentRoutines)
-
-	skip := make(map[int64]struct{})
-	for _, offset := range task.State.KeyStates.Completed {
-		skip[offset] = struct{}{}
-	}
-	for _, offset := range task.State.KeyStates.Err {
-		skip[offset] = struct{}{}
-	}
-
-	group := errgroup.Group{}
-	for i := task.OperationConfig.Start; i < task.OperationConfig.End; i++ {
-
-		if task.req.ContextClosed() {
-			close(routineLimiter)
-			close(dataChannel)
-			return
-		}
-
-		routineLimiter <- struct{}{}
-		dataChannel <- i
-		group.Go(func() error {
-			var err error
-			offset := <-dataChannel
-			key := task.MetaData.Seed + offset
-			docId := task.gen.BuildKey(key)
-			if _, ok := skip[offset]; ok {
-				<-routineLimiter
-				return fmt.Errorf("alreday performed operation on " + docId)
-			}
-
-			initTime := time.Now().UTC().Format(time.RFC850)
-
-			for retry := 0; retry < int(math.Max(float64(1), float64(task.OperationConfig.Exceptions.
-				RetryAttempts))); retry++ {
-				initTime = time.Now().UTC().Format(time.RFC850)
-				_, err = collectionObject.Collection.Touch(docId, time.Duration(task.Expiry)*time.Second,
-					&gocb.TouchOptions{
-						Timeout: time.Duration(task.TouchOptions.Timeout) * time.Second,
-					})
-
-				if err == nil {
-					break
-				}
-			}
-
-			if err != nil {
-				task.Result.IncrementFailure(initTime, docId, err, false, 0, offset)
-				task.State.StateChannel <- task_state.StateHelper{Status: task_state.ERR, Offset: offset}
-				<-routineLimiter
-				return err
-			}
-
-			task.State.StateChannel <- task_state.StateHelper{Status: task_state.COMPLETED, Offset: offset}
-
-			<-routineLimiter
-			return nil
-		})
-	}
-	_ = group.Wait()
-	close(routineLimiter)
-	close(dataChannel)
-	task.PostTaskExceptionHandling(collectionObject)
-	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-}
-
-func (task *TouchTask) PostTaskExceptionHandling(collectionObject *cb_sdk.CollectionObject) {
-	task.Result.StopStoringResult()
-	task.State.StopStoringState()
-	if task.OperationConfig.Exceptions.RetryAttempts <= 0 {
-		return
-	}
-
-	// Get all the errorOffset
-	errorOffsetMaps := task.State.ReturnErrOffset()
-	// Get all the completed offset
-	completedOffsetMaps := task.State.ReturnCompletedOffset()
-
-	// For the offset in ignore exceptions :-> move them from error to completed
-	shiftErrToCompletedOnIgnore(task.OperationConfig.Exceptions.IgnoreExceptions, task.Result, errorOffsetMaps,
-		completedOffsetMaps)
-
-	if task.OperationConfig.Exceptions.RetryAttempts > 0 {
-
-		exceptionList := GetExceptions(task.Result, task.OperationConfig.Exceptions.RetryExceptions)
-
-		// For the retry exceptions :-> move them on success after retrying from err to completed
-		for _, exception := range exceptionList {
-
-			errorOffsetListMap := make([]map[int64]RetriedResult, 0)
-			for _, failedDocs := range task.Result.BulkError[exception] {
-				m := make(map[int64]RetriedResult)
-				m[failedDocs.Offset] = RetriedResult{}
-				errorOffsetListMap = append(errorOffsetListMap, m)
-			}
-
-			routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
-			dataChannel := make(chan map[int64]RetriedResult, tasks.MaxConcurrentRoutines)
-			wg := errgroup.Group{}
-			for _, x := range errorOffsetListMap {
-				dataChannel <- x
-				routineLimiter <- struct{}{}
-				wg.Go(func() error {
-					m := <-dataChannel
-					var offset = int64(-1)
-					for k, _ := range m {
-						offset = k
-					}
-					key := task.MetaData.Seed + offset
-					docId := task.gen.BuildKey(key)
-
-					result := &gocb.MutationResult{}
-					var err error
-					initTime := time.Now().UTC().Format(time.RFC850)
-					for retry := 0; retry <= task.OperationConfig.Exceptions.RetryAttempts; retry++ {
-						_, err := collectionObject.Collection.Touch(docId, time.Duration(task.Expiry)*time.Second,
-							&gocb.TouchOptions{
-								Timeout: time.Duration(task.TouchOptions.Timeout) * time.Second,
-							})
-
-						if err == nil {
-							break
-						}
-					}
-
-					if err == nil {
-						m[offset] = RetriedResult{
-							Status:   true,
-							CAS:      uint64(result.Cas()),
-							InitTime: initTime,
-							AckTime:  time.Now().UTC().Format(time.RFC850),
-						}
-					} else {
-						m[offset] = RetriedResult{
-							InitTime: initTime,
-							AckTime:  time.Now().UTC().Format(time.RFC850),
-						}
-					}
-
-					<-routineLimiter
-					return nil
-				})
-			}
-			_ = wg.Wait()
-
-			shiftErrToCompletedOnRetrying(exception, task.Result, errorOffsetListMap, errorOffsetMaps,
-				completedOffsetMaps)
-		}
-	}
-
-	task.State.MakeCompleteKeyFromMap(completedOffsetMaps)
-	task.State.MakeErrorKeyFromMap(errorOffsetMaps)
-	task.Result.Failure = int64(len(task.State.KeyStates.Err))
-	task.Result.Success = task.OperationConfig.End - task.OperationConfig.Start - task.Result.Failure
-	log.Println("completed retrying:- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-}
-
-func (task *TouchTask) MatchResultSeed(resultSeed string) (bool, error) {
-	defer task.lock.Unlock()
-	task.lock.Lock()
-	if fmt.Sprintf("%d", task.ResultSeed) == resultSeed {
-		if task.TaskPending {
-			return true, task_errors.ErrTaskInPendingState
-		}
-		if task.Result == nil {
-			task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
-		}
-		return true, nil
-	}
-	return false, nil
-}
-
-func (task *TouchTask) GetCollectionObject() (*cb_sdk.CollectionObject, error) {
-	return task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope,
-		task.Collection)
-}
-
-func (task *TouchTask) SetException(exceptions Exceptions) {
-	task.OperationConfig.Exceptions = exceptions
-}
-
-func (task *TouchTask) GetOperationConfig() (*OperationConfig, *task_state.TaskState) {
-	return task.OperationConfig, task.State
-}
diff --git a/internal/tasks/bulk_loading_cb/task_bulk_upsert.go b/internal/tasks/bulk_loading_cb/task_bulk_upsert.go
deleted file mode 100644
index 11d15bd..0000000
--- a/internal/tasks/bulk_loading_cb/task_bulk_upsert.go
+++ /dev/null
@@ -1,393 +0,0 @@
-package bulk_loading_cb
-
-import (
-	"fmt"
-	"github.com/couchbase/gocb/v2"
-	"github.com/couchbaselabs/sirius/internal/cb_sdk"
-	"github.com/couchbaselabs/sirius/internal/docgenerator"
-	"github.com/couchbaselabs/sirius/internal/meta_data"
-	"github.com/couchbaselabs/sirius/internal/task_errors"
-	"github.com/couchbaselabs/sirius/internal/task_result"
-	"github.com/couchbaselabs/sirius/internal/task_state"
-	"github.com/couchbaselabs/sirius/internal/tasks"
-	"github.com/couchbaselabs/sirius/internal/template"
-	"github.com/jaswdr/faker"
-	"golang.org/x/sync/errgroup"
-	"log"
-	"math"
-	"math/rand"
-	"strings"
-	"sync"
-	"time"
-)
-
-type UpsertTask struct {
-	IdentifierToken string                        `json:"identifierToken" doc:"true"`
-	ClusterConfig   *cb_sdk.ClusterConfig         `json:"clusterConfig" doc:"true"`
-	Bucket          string                        `json:"bucket" doc:"true"`
-	Scope           string                        `json:"scope,omitempty" doc:"true"`
-	Collection      string                        `json:"collection,omitempty" doc:"true"`
-	InsertOptions   *cb_sdk.InsertOptions         `json:"insertOptions,omitempty" doc:"true"`
-	OperationConfig *OperationConfig              `json:"operationConfig,omitempty" doc:"true"`
-	Operation       string                        `json:"operation" doc:"false"`
-	ResultSeed      int64                         `json:"resultSeed" doc:"false"`
-	TaskPending     bool                          `json:"taskPending" doc:"false"`
-	State           *task_state.TaskState         `json:"State" doc:"false"`
-	MetaData        *meta_data.CollectionMetaData `json:"metaData" doc:"false"`
-	Result          *task_result.TaskResult       `json:"-" doc:"false"`
-	gen             *docgenerator.Generator       `json:"-" doc:"false"`
-	req             *tasks.Request                `json:"-" doc:"false"`
-	rerun           bool                          `json:"-" doc:"false"`
-	lock            sync.Mutex                    `json:"-" doc:"false" `
-}
-
-func (task *UpsertTask) CollectionIdentifier() string {
-	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
-	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
-		task.Collection}, ":")
-}
-
-func (task *UpsertTask) Describe() string {
-	return `Upsert task mutates documents in bulk into a bucket.
-The task will update the fields in a documents ranging from [start,end] inclusive.
-We need to share the fields we want to update in a json document using SQL++ syntax.`
-}
-
-func (task *UpsertTask) CheckIfPending() bool {
-	return task.TaskPending
-}
-
-func (task *UpsertTask) Config(req *tasks.Request, reRun bool) (int64, error) {
-	task.TaskPending = true
-	task.req = req
-
-	if task.req == nil {
-		task.TaskPending = false
-		return 0, task_errors.ErrRequestIsNil
-	}
-
-	task.req.ReconnectionManager()
-	if _, err := task.req.GetCluster(task.ClusterConfig); err != nil {
-		task.TaskPending = false
-		return 0, err
-	}
-
-	task.lock = sync.Mutex{}
-	task.rerun = reRun
-
-	if !reRun {
-		task.ResultSeed = int64(time.Now().UnixNano())
-		task.Operation = tasks.UpsertOperation
-
-		if task.Bucket == "" {
-			task.Bucket = cb_sdk.DefaultBucket
-		}
-		if task.Scope == "" {
-			task.Scope = cb_sdk.DefaultScope
-		}
-		if task.Collection == "" {
-			task.Collection = cb_sdk.DefaultCollection
-		}
-
-		if err := cb_sdk.ConfigInsertOptions(task.InsertOptions); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-
-		if err := ConfigureOperationConfig(task.OperationConfig); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-
-		task.MetaData = task.req.MetaData.GetCollectionMetadata(task.CollectionIdentifier())
-
-		task.req.Lock()
-		if task.OperationConfig.End+task.MetaData.Seed > task.MetaData.SeedEnd {
-			task.req.AddToSeedEnd(task.MetaData, (task.OperationConfig.End+task.MetaData.Seed)-(task.MetaData.SeedEnd))
-		}
-		task.State = task_state.ConfigTaskState(task.MetaData.Seed, task.MetaData.SeedEnd, task.ResultSeed)
-		task.req.Unlock()
-
-	} else {
-		if task.State == nil {
-			return task.ResultSeed, task_errors.ErrTaskStateIsNil
-		}
-
-		task.State.SetupStoringKeys()
-		_ = task_result.DeleteResultFile(task.ResultSeed)
-		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-	}
-	return task.ResultSeed, nil
-}
-
-func (task *UpsertTask) TearUp() error {
-	task.Result.StopStoringResult()
-	if err := task.Result.SaveResultIntoFile(); err != nil {
-		log.Println("not able to save Result into ", task.ResultSeed, task.Operation)
-	}
-	task.Result = nil
-	task.State.StopStoringState()
-	task.TaskPending = false
-	return task.req.SaveRequestIntoFile()
-}
-
-func (task *UpsertTask) Do() error {
-
-	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
-
-	collectionObject, err1 := task.GetCollectionObject()
-
-	task.gen = docgenerator.ConfigGenerator(
-		task.OperationConfig.KeySize,
-		task.OperationConfig.DocSize,
-		task.OperationConfig.DocType,
-		task.OperationConfig.KeyPrefix,
-		task.OperationConfig.KeySuffix,
-		template.InitialiseTemplate(task.OperationConfig.TemplateName))
-
-	if err1 != nil {
-		task.Result.ErrorOther = err1.Error()
-		task.Result.FailWholeBulkOperation(task.OperationConfig.Start, task.OperationConfig.End, err1, task.State,
-			task.gen, task.MetaData.Seed)
-		return task.TearUp()
-	}
-
-	upsertDocuments(task, collectionObject)
-	task.Result.Success = task.OperationConfig.End - task.OperationConfig.Start - task.Result.Failure
-
-	return task.TearUp()
-}
-
-func upsertDocuments(task *UpsertTask, collectionObject *cb_sdk.CollectionObject) {
-
-	if task.req.ContextClosed() {
-		return
-	}
-
-	routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
-	dataChannel := make(chan int64, tasks.MaxConcurrentRoutines)
-
-	skip := make(map[int64]struct{})
-	for _, offset := range task.State.KeyStates.Completed {
-		skip[offset] = struct{}{}
-	}
-	for _, offset := range task.State.KeyStates.Err {
-		skip[offset] = struct{}{}
-	}
-
-	group := errgroup.Group{}
-	for i := task.OperationConfig.Start; i < task.OperationConfig.End; i++ {
-
-		if task.req.ContextClosed() {
-			close(routineLimiter)
-			close(dataChannel)
-			return
-		}
-
-		routineLimiter <- struct{}{}
-		dataChannel <- i
-		group.Go(func() error {
-			var err error
-			offset := <-dataChannel
-			key := task.MetaData.Seed + offset
-			docId := task.gen.BuildKey(key)
-			if _, ok := skip[offset]; ok {
-				<-routineLimiter
-				return fmt.Errorf("alreday performed operation on " + docId)
-			}
-
-			initTime := time.Now().UTC().Format(time.RFC850)
-			fake := faker.NewWithSeed(rand.NewSource(int64(key)))
-			originalDoc, err := task.gen.Template.GenerateDocument(&fake, task.OperationConfig.DocSize)
-			if err != nil {
-				<-routineLimiter
-				return err
-			}
-			originalDoc, err = retracePreviousMutations(task.req, task.CollectionIdentifier(), offset, originalDoc,
-				*task.gen, &fake, task.ResultSeed)
-			if err != nil {
-				task.Result.IncrementFailure(initTime, docId, err, false, 0, offset)
-				<-routineLimiter
-				return err
-			}
-			docUpdated, err := task.gen.Template.UpdateDocument(task.OperationConfig.FieldsToChange, originalDoc,
-				task.OperationConfig.DocSize, &fake)
-
-			for retry := 0; retry < int(math.Max(float64(1), float64(task.OperationConfig.Exceptions.
-				RetryAttempts))); retry++ {
-				initTime = time.Now().UTC().Format(time.RFC850)
-				_, err = collectionObject.Collection.Upsert(docId, docUpdated, &gocb.UpsertOptions{
-					DurabilityLevel: cb_sdk.GetDurability(task.InsertOptions.Durability),
-					PersistTo:       task.InsertOptions.PersistTo,
-					ReplicateTo:     task.InsertOptions.ReplicateTo,
-					Timeout:         time.Duration(task.InsertOptions.Timeout) * time.Second,
-					Expiry:          time.Duration(task.InsertOptions.Expiry) * time.Second,
-				})
-
-				if err == nil {
-					break
-				}
-			}
-
-			if err != nil {
-				task.Result.IncrementFailure(initTime, docId, err, false, 0, offset)
-				task.State.StateChannel <- task_state.StateHelper{Status: task_state.ERR, Offset: offset}
-				<-routineLimiter
-				return err
-			}
-
-			task.State.StateChannel <- task_state.StateHelper{Status: task_state.COMPLETED, Offset: offset}
-
-			<-routineLimiter
-			return nil
-		})
-	}
-	_ = group.Wait()
-	close(routineLimiter)
-	close(dataChannel)
-	task.PostTaskExceptionHandling(collectionObject)
-	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-}
-
-func (task *UpsertTask) PostTaskExceptionHandling(collectionObject *cb_sdk.CollectionObject) {
-	task.Result.StopStoringResult()
-	task.State.StopStoringState()
-	if task.OperationConfig.Exceptions.RetryAttempts <= 0 {
-		return
-	}
-
-	// Get all the errorOffset
-	errorOffsetMaps := task.State.ReturnErrOffset()
-	// Get all the completed offset
-	completedOffsetMaps := task.State.ReturnCompletedOffset()
-
-	// For the offset in ignore exceptions :-> move them from error to completed
-	shiftErrToCompletedOnIgnore(task.OperationConfig.Exceptions.IgnoreExceptions, task.Result, errorOffsetMaps,
-		completedOffsetMaps)
-
-	if task.OperationConfig.Exceptions.RetryAttempts > 0 {
-
-		exceptionList := GetExceptions(task.Result, task.OperationConfig.Exceptions.RetryExceptions)
-
-		// For the retry exceptions :-> move them on success after retrying from err to completed
-		for _, exception := range exceptionList {
-
-			errorOffsetListMap := make([]map[int64]RetriedResult, 0)
-			for _, failedDocs := range task.Result.BulkError[exception] {
-				m := make(map[int64]RetriedResult)
-				m[failedDocs.Offset] = RetriedResult{}
-				errorOffsetListMap = append(errorOffsetListMap, m)
-			}
-
-			routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
-			dataChannel := make(chan map[int64]RetriedResult, tasks.MaxConcurrentRoutines)
-			wg := errgroup.Group{}
-			for _, x := range errorOffsetListMap {
-				dataChannel <- x
-				routineLimiter <- struct{}{}
-				wg.Go(func() error {
-					m := <-dataChannel
-					var offset = int64(-1)
-					for k, _ := range m {
-						offset = k
-					}
-					key := task.MetaData.Seed + offset
-					docId := task.gen.BuildKey(key)
-					fake := faker.NewWithSeed(rand.NewSource(int64(key)))
-
-					originalDoc, err := task.gen.Template.GenerateDocument(&fake, task.OperationConfig.DocSize)
-					if err != nil {
-						<-routineLimiter
-						return err
-					}
-					originalDoc, err = retracePreviousMutations(task.req,
-						task.CollectionIdentifier(), offset,
-						originalDoc,
-						*task.gen, &fake,
-						task.ResultSeed)
-					if err != nil {
-						initTime := time.Now().UTC().Format(time.RFC850)
-						task.Result.IncrementFailure(initTime, docId, err, false, 0, offset)
-						<-routineLimiter
-						return err
-					}
-					docUpdated, err := task.gen.Template.UpdateDocument(task.OperationConfig.FieldsToChange,
-						originalDoc, task.OperationConfig.DocSize, &fake)
-
-					retry := 0
-					result := &gocb.MutationResult{}
-
-					initTime := time.Now().UTC().Format(time.RFC850)
-					for retry = 0; retry <= task.OperationConfig.Exceptions.RetryAttempts; retry++ {
-						result, err = collectionObject.Collection.Upsert(docId, docUpdated, &gocb.UpsertOptions{
-							DurabilityLevel: cb_sdk.GetDurability(task.InsertOptions.Durability),
-							PersistTo:       task.InsertOptions.PersistTo,
-							ReplicateTo:     task.InsertOptions.ReplicateTo,
-							Timeout:         time.Duration(task.InsertOptions.Timeout) * time.Second,
-							Expiry:          time.Duration(task.InsertOptions.Expiry) * time.Second,
-						})
-
-						if err == nil {
-							break
-						}
-					}
-
-					if err == nil {
-						m[offset] = RetriedResult{
-							Status:   true,
-							CAS:      uint64(result.Cas()),
-							InitTime: initTime,
-							AckTime:  time.Now().UTC().Format(time.RFC850),
-						}
-					} else {
-						m[offset] = RetriedResult{
-							InitTime: initTime,
-							AckTime:  time.Now().UTC().Format(time.RFC850),
-						}
-					}
-
-					<-routineLimiter
-					return nil
-				})
-			}
-			_ = wg.Wait()
-
-			shiftErrToCompletedOnRetrying(exception, task.Result, errorOffsetListMap, errorOffsetMaps,
-				completedOffsetMaps)
-		}
-	}
-
-	task.State.MakeCompleteKeyFromMap(completedOffsetMaps)
-	task.State.MakeErrorKeyFromMap(errorOffsetMaps)
-	task.Result.Failure = int64(len(task.State.KeyStates.Err))
-	task.Result.Success = task.OperationConfig.End - task.OperationConfig.Start - task.Result.Failure
-	log.Println("completed retrying:- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-}
-
-func (task *UpsertTask) MatchResultSeed(resultSeed string) (bool, error) {
-	defer task.lock.Unlock()
-	task.lock.Lock()
-	if fmt.Sprintf("%d", task.ResultSeed) == resultSeed {
-		if task.TaskPending {
-			return true, task_errors.ErrTaskInPendingState
-		}
-		if task.Result == nil {
-			task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
-		}
-		return true, nil
-	}
-	return false, nil
-}
-
-func (task *UpsertTask) GetCollectionObject() (*cb_sdk.CollectionObject, error) {
-	return task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope,
-		task.Collection)
-}
-
-func (task *UpsertTask) SetException(exceptions Exceptions) {
-	task.OperationConfig.Exceptions = exceptions
-}
-
-func (task *UpsertTask) GetOperationConfig() (*OperationConfig, *task_state.TaskState) {
-	return task.OperationConfig, task.State
-}
diff --git a/internal/tasks/bulk_loading_cb/task_retry_exceptions.go b/internal/tasks/bulk_loading_cb/task_retry_exceptions.go
deleted file mode 100644
index abeaa3e..0000000
--- a/internal/tasks/bulk_loading_cb/task_retry_exceptions.go
+++ /dev/null
@@ -1,105 +0,0 @@
-package bulk_loading_cb
-
-import (
-	"errors"
-	"fmt"
-	"github.com/couchbaselabs/sirius/internal/cb_sdk"
-	"github.com/couchbaselabs/sirius/internal/task_errors"
-	"github.com/couchbaselabs/sirius/internal/task_state"
-	"github.com/couchbaselabs/sirius/internal/tasks"
-)
-
-type RetryExceptions struct {
-	IdentifierToken string         `json:"identifierToken" doc:"true"`
-	ResultSeed      string         `json:"resultSeed" doc:"true"`
-	Exceptions      Exceptions     `json:"exceptions" doc:"true"`
-	Task            BulkTask       `json:"-" doc:"false"`
-	req             *tasks.Request `json:"-" doc:"false"`
-}
-
-func (r *RetryExceptions) Describe() string {
-	return "Retry Exception reties failed operations.\n" +
-		"IgnoreExceptions will ignore failed operation occurred in this category. \n" +
-		"RetryExceptions will retry failed operation occurred in this category. \n" +
-		"RetryAttempts is the number of retry attempts.\n"
-}
-
-func (r *RetryExceptions) Do() error {
-	if r.req.ContextClosed() {
-		return errors.New("req is cleared")
-	}
-
-	c, e := r.Task.GetCollectionObject()
-	if e != nil {
-		r.Task.TearUp()
-		return nil
-	}
-	r.Task.SetException(r.Exceptions)
-	r.Task.PostTaskExceptionHandling(c)
-	r.Task.TearUp()
-	return nil
-}
-
-func (r *RetryExceptions) Config(req *tasks.Request, reRun bool) (int64, error) {
-	r.req = req
-	if r.req == nil {
-		return 0, task_errors.ErrRequestIsNil
-	}
-
-	if r.req.Tasks == nil {
-		return 0, fmt.Errorf("request.Task struct is nil")
-	}
-	for i := range r.req.Tasks {
-		if bulkTask, ok := r.req.Tasks[i].Task.(BulkTask); ok {
-			if ok, err := bulkTask.MatchResultSeed(r.ResultSeed); ok {
-				if err != nil {
-					return 0, err
-				} else {
-					r.Task = bulkTask
-					break
-				}
-			}
-		}
-	}
-
-	if r.Task == nil {
-		return 0, fmt.Errorf("no bulk loading task found for %s : %s", r.req.Identifier, r.ResultSeed)
-	}
-
-	return r.Task.Config(req, true)
-
-}
-
-func (r *RetryExceptions) CollectionIdentifier() string {
-	return r.Task.CollectionIdentifier()
-}
-
-func (r *RetryExceptions) CheckIfPending() bool {
-	return r.Task.CheckIfPending()
-}
-
-func (r *RetryExceptions) PostTaskExceptionHandling(_ *cb_sdk.CollectionObject) {
-
-}
-
-func (r *RetryExceptions) TearUp() error {
-	return r.Task.TearUp()
-}
-func (r *RetryExceptions) MatchResultSeed(resultSeed string) (bool, error) {
-	return r.Task.MatchResultSeed(resultSeed)
-}
-
-func (r *RetryExceptions) GetCollectionObject() (*cb_sdk.CollectionObject, error) {
-	return r.Task.GetCollectionObject()
-}
-
-func (r *RetryExceptions) SetException(exceptions Exceptions) {
-	r.Task.SetException(r.Exceptions)
-}
-
-func (r *RetryExceptions) GetOperationConfig() (*OperationConfig, *task_state.TaskState) {
-	if r.Task != nil {
-		return r.Task.GetOperationConfig()
-	}
-	return nil, nil
-}
diff --git a/internal/tasks/bulk_loading_cb/task_sub_doc_delete.go b/internal/tasks/bulk_loading_cb/task_sub_doc_delete.go
deleted file mode 100644
index 587eaee..0000000
--- a/internal/tasks/bulk_loading_cb/task_sub_doc_delete.go
+++ /dev/null
@@ -1,402 +0,0 @@
-package bulk_loading_cb
-
-import (
-	"fmt"
-	"github.com/couchbase/gocb/v2"
-	"github.com/couchbaselabs/sirius/internal/cb_sdk"
-	"github.com/couchbaselabs/sirius/internal/docgenerator"
-	"github.com/couchbaselabs/sirius/internal/meta_data"
-	"github.com/couchbaselabs/sirius/internal/task_errors"
-	"github.com/couchbaselabs/sirius/internal/task_result"
-	"github.com/couchbaselabs/sirius/internal/task_state"
-	"github.com/couchbaselabs/sirius/internal/tasks"
-	"github.com/couchbaselabs/sirius/internal/template"
-	"github.com/jaswdr/faker"
-	"golang.org/x/sync/errgroup"
-	"log"
-	"math"
-	"math/rand"
-	"strings"
-	"sync"
-	"time"
-)
-
-type SubDocDelete struct {
-	IdentifierToken   string                        `json:"identifierToken" doc:"true"`
-	ClusterConfig     *cb_sdk.ClusterConfig         `json:"clusterConfig" doc:"true"`
-	Bucket            string                        `json:"bucket" doc:"true"`
-	Scope             string                        `json:"scope,omitempty" doc:"true"`
-	Collection        string                        `json:"collection,omitempty" doc:"true"`
-	OperationConfig   *OperationConfig              `json:"operationConfig" doc:"true"`
-	RemoveSpecOptions *cb_sdk.RemoveSpecOptions     `json:"removeSpecOptions" doc:"true"`
-	MutateInOptions   *cb_sdk.MutateInOptions       `json:"mutateInOptions" doc:"true"`
-	Operation         string                        `json:"operation" doc:"false"`
-	ResultSeed        int64                         `json:"resultSeed" doc:"false"`
-	TaskPending       bool                          `json:"taskPending" doc:"false"`
-	State             *task_state.TaskState         `json:"State" doc:"false"`
-	MetaData          *meta_data.CollectionMetaData `json:"metaData" doc:"false"`
-	Result            *task_result.TaskResult       `json:"-" doc:"false"`
-	gen               *docgenerator.Generator       `json:"-" doc:"false"`
-	req               *tasks.Request                `json:"-" doc:"false"`
-	rerun             bool                          `json:"-" doc:"false"`
-	lock              sync.Mutex                    `json:"lock" doc:"false"`
-}
-
-func (task *SubDocDelete) Describe() string {
-	return " SubDocDelete deletes sub-documents in bulk"
-}
-
-func (task *SubDocDelete) CollectionIdentifier() string {
-	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
-	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
-		task.Collection}, ":")
-}
-
-func (task *SubDocDelete) CheckIfPending() bool {
-	return task.TaskPending
-}
-
-// Config configures  the insert task
-func (task *SubDocDelete) Config(req *tasks.Request, reRun bool) (int64, error) {
-	task.TaskPending = true
-	task.req = req
-
-	if task.req == nil {
-		task.TaskPending = false
-		return 0, task_errors.ErrRequestIsNil
-	}
-
-	task.req.ReconnectionManager()
-	if _, err := task.req.GetCluster(task.ClusterConfig); err != nil {
-		task.TaskPending = false
-		return 0, err
-	}
-
-	task.lock = sync.Mutex{}
-	task.rerun = reRun
-
-	if !reRun {
-		task.ResultSeed = int64(time.Now().UnixNano())
-		task.Operation = tasks.SubDocDeleteOperation
-
-		if task.Bucket == "" {
-			task.Bucket = cb_sdk.DefaultBucket
-		}
-		if task.Scope == "" {
-			task.Scope = cb_sdk.DefaultScope
-		}
-		if task.Collection == "" {
-			task.Collection = cb_sdk.DefaultCollection
-		}
-
-		if err := ConfigureOperationConfig(task.OperationConfig); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-
-		if err := cb_sdk.ConfigRemoveSpecOptions(task.RemoveSpecOptions); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-
-		if err := cb_sdk.ConfigMutateInOptions(task.MutateInOptions); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-
-		task.MetaData = task.req.MetaData.GetCollectionMetadata(task.CollectionIdentifier())
-
-		task.req.Lock()
-		task.State = task_state.ConfigTaskState(task.MetaData.Seed, task.MetaData.SeedEnd, task.ResultSeed)
-		task.req.Unlock()
-
-	} else {
-		if task.State == nil {
-			return task.ResultSeed, task_errors.ErrTaskStateIsNil
-		}
-		task.State.SetupStoringKeys()
-		_ = task_result.DeleteResultFile(task.ResultSeed)
-		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-	}
-	return task.ResultSeed, nil
-}
-
-func (task *SubDocDelete) TearUp() error {
-	//Use this case to store task's state on disk when required
-	//if err := task.State.SaveTaskSateOnDisk(); err != nil {
-	//	log.Println("Error in storing TASK State on DISK")
-	//}
-	task.Result.StopStoringResult()
-	if err := task.Result.SaveResultIntoFile(); err != nil {
-		log.Println("not able to save Result into ", task.ResultSeed, task.Operation)
-	}
-	task.Result = nil
-	task.State.StopStoringState()
-	task.TaskPending = false
-	return task.req.SaveRequestIntoFile()
-}
-
-func (task *SubDocDelete) Do() error {
-
-	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
-
-	collectionObject, err1 := task.GetCollectionObject()
-
-	task.gen = docgenerator.ConfigGenerator(
-		task.OperationConfig.KeySize,
-		task.OperationConfig.DocSize,
-		task.OperationConfig.DocType,
-		task.OperationConfig.KeyPrefix,
-		task.OperationConfig.KeySuffix,
-		template.InitialiseTemplate(task.OperationConfig.TemplateName))
-
-	if err1 != nil {
-		task.Result.ErrorOther = err1.Error()
-		task.Result.FailWholeBulkOperation(task.OperationConfig.Start, task.OperationConfig.End,
-			err1, task.State, task.gen, task.MetaData.Seed)
-		return task.TearUp()
-	}
-
-	deleteSubDocuments(task, collectionObject)
-	task.Result.Success = (task.OperationConfig.End - task.OperationConfig.Start) - task.Result.Failure
-
-	return task.TearUp()
-}
-
-// insertDocuments uploads new documents in a bucket.scope.collection in a defined batch size at multiple iterations.
-func deleteSubDocuments(task *SubDocDelete, collectionObject *cb_sdk.CollectionObject) {
-
-	if task.req.ContextClosed() {
-		return
-	}
-
-	routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
-	dataChannel := make(chan int64, tasks.MaxConcurrentRoutines)
-
-	skip := make(map[int64]struct{})
-	for _, offset := range task.State.KeyStates.Completed {
-		skip[offset] = struct{}{}
-	}
-	for _, offset := range task.State.KeyStates.Err {
-		skip[offset] = struct{}{}
-	}
-	group := errgroup.Group{}
-	for iteration := task.OperationConfig.Start; iteration < task.OperationConfig.End; iteration++ {
-
-		if task.req.ContextClosed() {
-			close(routineLimiter)
-			close(dataChannel)
-			return
-		}
-
-		routineLimiter <- struct{}{}
-		dataChannel <- iteration
-		group.Go(func() error {
-			offset := <-dataChannel
-			key := offset + task.MetaData.Seed
-			docId := task.gen.BuildKey(key)
-
-			if _, ok := skip[offset]; ok {
-				<-routineLimiter
-				return fmt.Errorf("alreday performed operation on " + docId)
-			}
-			fake := faker.NewWithSeed(rand.NewSource(int64(key)))
-
-			var err error
-			initTime := time.Now().UTC().Format(time.RFC850)
-			for retry := 0; retry < int(math.Max(float64(1), float64(task.OperationConfig.Exceptions.
-				RetryAttempts))); retry++ {
-
-				var iOps []gocb.MutateInSpec
-				for path, _ := range task.gen.Template.GenerateSubPathAndValue(&fake, task.OperationConfig.DocSize) {
-					iOps = append(iOps, gocb.RemoveSpec(path, &gocb.RemoveSpecOptions{
-						IsXattr: task.RemoveSpecOptions.IsXattr,
-					}))
-				}
-
-				if !task.RemoveSpecOptions.IsXattr {
-					iOps = append(iOps, gocb.IncrementSpec(template.MutatedPath,
-						int64(template.MutateFieldIncrement), &gocb.CounterSpecOptions{
-							CreatePath: false,
-							IsXattr:    false,
-						}))
-				}
-
-				initTime = time.Now().UTC().Format(time.RFC850)
-				_, err = collectionObject.Collection.MutateIn(docId, iOps, &gocb.MutateInOptions{
-					Expiry:          time.Duration(task.MutateInOptions.Expiry) * time.Second,
-					PersistTo:       task.MutateInOptions.PersistTo,
-					ReplicateTo:     task.MutateInOptions.ReplicateTo,
-					DurabilityLevel: cb_sdk.GetDurability(task.MutateInOptions.Durability),
-					StoreSemantic:   cb_sdk.GetStoreSemantic(task.MutateInOptions.StoreSemantic),
-					Timeout:         time.Duration(task.MutateInOptions.Expiry) * time.Second,
-					PreserveExpiry:  task.MutateInOptions.PreserveExpiry,
-				})
-
-				if err == nil {
-					break
-				}
-			}
-			if err != nil {
-
-				task.Result.IncrementFailure(initTime, docId, err, false, 0, offset)
-				task.State.StateChannel <- task_state.StateHelper{Status: task_state.ERR, Offset: offset}
-				<-routineLimiter
-				return err
-
-			}
-
-			task.State.StateChannel <- task_state.StateHelper{Status: task_state.COMPLETED, Offset: offset}
-			<-routineLimiter
-			return nil
-		})
-	}
-
-	_ = group.Wait()
-	close(routineLimiter)
-	close(dataChannel)
-	task.PostTaskExceptionHandling(collectionObject)
-	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-}
-
-func (task *SubDocDelete) PostTaskExceptionHandling(collectionObject *cb_sdk.CollectionObject) {
-	task.Result.StopStoringResult()
-	task.State.StopStoringState()
-	if task.OperationConfig.Exceptions.RetryAttempts <= 0 {
-		return
-	}
-
-	// Get all the errorOffset
-	errorOffsetMaps := task.State.ReturnErrOffset()
-	// Get all the completed offset
-	completedOffsetMaps := task.State.ReturnCompletedOffset()
-
-	// For the offset in ignore exceptions :-> move them from error to completed
-	shiftErrToCompletedOnIgnore(task.OperationConfig.Exceptions.IgnoreExceptions, task.Result, errorOffsetMaps, completedOffsetMaps)
-
-	if task.OperationConfig.Exceptions.RetryAttempts > 0 {
-
-		exceptionList := GetExceptions(task.Result, task.OperationConfig.Exceptions.RetryExceptions)
-
-		// For the retry exceptions :-> move them on success after retrying from err to completed
-		for _, exception := range exceptionList {
-
-			errorOffsetListMap := make([]map[int64]RetriedResult, 0)
-			for _, failedDocs := range task.Result.BulkError[exception] {
-				m := make(map[int64]RetriedResult)
-				m[failedDocs.Offset] = RetriedResult{}
-				errorOffsetListMap = append(errorOffsetListMap, m)
-			}
-
-			routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
-			dataChannel := make(chan map[int64]RetriedResult, tasks.MaxConcurrentRoutines)
-			wg := errgroup.Group{}
-			for _, x := range errorOffsetListMap {
-				dataChannel <- x
-				routineLimiter <- struct{}{}
-				wg.Go(func() error {
-					m := <-dataChannel
-					var offset = int64(-1)
-					for k, _ := range m {
-						offset = k
-					}
-					key := offset + task.MetaData.Seed
-					docId := task.gen.BuildKey(key)
-					fake := faker.NewWithSeed(rand.NewSource(int64(key)))
-
-					retry := 0
-					var err error
-					result := &gocb.MutateInResult{}
-					initTime := time.Now().UTC().Format(time.RFC850)
-					for retry = 0; retry <= task.OperationConfig.Exceptions.RetryAttempts; retry++ {
-
-						var iOps []gocb.MutateInSpec
-						for path, _ := range task.gen.Template.GenerateSubPathAndValue(&fake, task.OperationConfig.DocSize) {
-							iOps = append(iOps, gocb.RemoveSpec(path, &gocb.RemoveSpecOptions{
-								IsXattr: task.RemoveSpecOptions.IsXattr,
-							}))
-						}
-
-						if !task.RemoveSpecOptions.IsXattr {
-							iOps = append(iOps, gocb.IncrementSpec(template.MutatedPath,
-								int64(template.MutateFieldIncrement), &gocb.CounterSpecOptions{
-									CreatePath: false,
-									IsXattr:    false,
-								}))
-						}
-
-						initTime = time.Now().UTC().Format(time.RFC850)
-						result, err = collectionObject.Collection.MutateIn(docId, iOps, &gocb.MutateInOptions{
-							Expiry:          time.Duration(task.MutateInOptions.Expiry) * time.Second,
-							PersistTo:       task.MutateInOptions.PersistTo,
-							ReplicateTo:     task.MutateInOptions.ReplicateTo,
-							DurabilityLevel: cb_sdk.GetDurability(task.MutateInOptions.Durability),
-							StoreSemantic:   cb_sdk.GetStoreSemantic(task.MutateInOptions.StoreSemantic),
-							Timeout:         time.Duration(task.MutateInOptions.Expiry) * time.Second,
-							PreserveExpiry:  task.MutateInOptions.PreserveExpiry,
-						})
-
-						if err == nil {
-							break
-						}
-					}
-
-					if err != nil {
-						m[offset] = RetriedResult{
-							Status:   true,
-							CAS:      0,
-							InitTime: initTime,
-							AckTime:  time.Now().UTC().Format(time.RFC850),
-						}
-					} else {
-						m[offset] = RetriedResult{
-							Status:   true,
-							CAS:      uint64(result.Cas()),
-							InitTime: initTime,
-							AckTime:  time.Now().UTC().Format(time.RFC850),
-						}
-					}
-
-					<-routineLimiter
-					return nil
-				})
-			}
-			_ = wg.Wait()
-
-			shiftErrToCompletedOnRetrying(exception, task.Result, errorOffsetListMap, errorOffsetMaps, completedOffsetMaps)
-		}
-	}
-
-	task.State.MakeCompleteKeyFromMap(completedOffsetMaps)
-	task.State.MakeErrorKeyFromMap(errorOffsetMaps)
-	task.Result.Failure = int64(len(task.State.KeyStates.Err))
-	log.Println("completed retrying:- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-}
-
-func (task *SubDocDelete) MatchResultSeed(resultSeed string) (bool, error) {
-	defer task.lock.Unlock()
-	task.lock.Lock()
-	if fmt.Sprintf("%d", task.ResultSeed) == resultSeed {
-		if task.TaskPending {
-			return true, task_errors.ErrTaskInPendingState
-		}
-		if task.Result == nil {
-			task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
-		}
-		return true, nil
-	}
-	return false, nil
-}
-
-func (task *SubDocDelete) GetCollectionObject() (*cb_sdk.CollectionObject, error) {
-	return task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope,
-		task.Collection)
-}
-
-func (task *SubDocDelete) SetException(exceptions Exceptions) {
-	task.OperationConfig.Exceptions = exceptions
-}
-
-func (task *SubDocDelete) GetOperationConfig() (*OperationConfig, *task_state.TaskState) {
-	return task.OperationConfig, task.State
-}
diff --git a/internal/tasks/bulk_loading_cb/task_sub_doc_insert.go b/internal/tasks/bulk_loading_cb/task_sub_doc_insert.go
deleted file mode 100644
index c6de118..0000000
--- a/internal/tasks/bulk_loading_cb/task_sub_doc_insert.go
+++ /dev/null
@@ -1,415 +0,0 @@
-package bulk_loading_cb
-
-import (
-	"errors"
-	"fmt"
-	"github.com/couchbase/gocb/v2"
-	"github.com/couchbaselabs/sirius/internal/cb_sdk"
-	"github.com/couchbaselabs/sirius/internal/docgenerator"
-	"github.com/couchbaselabs/sirius/internal/meta_data"
-	"github.com/couchbaselabs/sirius/internal/task_errors"
-	"github.com/couchbaselabs/sirius/internal/task_result"
-	"github.com/couchbaselabs/sirius/internal/task_state"
-	"github.com/couchbaselabs/sirius/internal/tasks"
-	"github.com/couchbaselabs/sirius/internal/template"
-	"github.com/jaswdr/faker"
-	"golang.org/x/sync/errgroup"
-	"log"
-	"math"
-	"math/rand"
-	"strings"
-	"sync"
-	"time"
-)
-
-type SubDocInsert struct {
-	IdentifierToken   string                        `json:"identifierToken" doc:"true"`
-	ClusterConfig     *cb_sdk.ClusterConfig         `json:"clusterConfig" doc:"true"`
-	Bucket            string                        `json:"bucket" doc:"true"`
-	Scope             string                        `json:"scope,omitempty" doc:"true"`
-	Collection        string                        `json:"collection,omitempty" doc:"true"`
-	OperationConfig   *OperationConfig              `json:"operationConfig" doc:"true"`
-	InsertSpecOptions *cb_sdk.InsertSpecOptions     `json:"insertSpecOptions" doc:"true"`
-	MutateInOptions   *cb_sdk.MutateInOptions       `json:"mutateInOptions" doc:"true"`
-	Operation         string                        `json:"operation" doc:"false"`
-	ResultSeed        int64                         `json:"resultSeed" doc:"false"`
-	TaskPending       bool                          `json:"taskPending" doc:"false"`
-	State             *task_state.TaskState         `json:"State" doc:"false"`
-	MetaData          *meta_data.CollectionMetaData `json:"metaData" doc:"false"`
-	Result            *task_result.TaskResult       `json:"-" doc:"false"`
-	gen               *docgenerator.Generator       `json:"-" doc:"false"`
-	req               *tasks.Request                `json:"-" doc:"false"`
-	rerun             bool                          `json:"-" doc:"false"`
-	lock              sync.Mutex                    `json:"lock" doc:"false"`
-}
-
-func (task *SubDocInsert) Describe() string {
-	return " SubDocInsert inserts a Sub-Document"
-}
-
-func (task *SubDocInsert) CollectionIdentifier() string {
-	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
-	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
-		task.Collection}, ":")
-}
-
-func (task *SubDocInsert) CheckIfPending() bool {
-	return task.TaskPending
-}
-
-// Config configures  the insert task
-func (task *SubDocInsert) Config(req *tasks.Request, reRun bool) (int64, error) {
-	task.TaskPending = true
-	task.req = req
-
-	if task.req == nil {
-		task.TaskPending = false
-		return 0, task_errors.ErrRequestIsNil
-	}
-
-	task.req.ReconnectionManager()
-	if _, err := task.req.GetCluster(task.ClusterConfig); err != nil {
-		task.TaskPending = false
-		return 0, err
-	}
-
-	task.lock = sync.Mutex{}
-	task.rerun = reRun
-
-	if !reRun {
-		task.ResultSeed = int64(time.Now().UnixNano())
-		task.Operation = tasks.SubDocInsertOperation
-
-		if task.Bucket == "" {
-			task.Bucket = cb_sdk.DefaultBucket
-		}
-		if task.Scope == "" {
-			task.Scope = cb_sdk.DefaultScope
-		}
-		if task.Collection == "" {
-			task.Collection = cb_sdk.DefaultCollection
-		}
-
-		if err := ConfigureOperationConfig(task.OperationConfig); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-
-		if err := cb_sdk.ConfigInsertSpecOptions(task.InsertSpecOptions); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-
-		if err := cb_sdk.ConfigMutateInOptions(task.MutateInOptions); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-
-		task.MetaData = task.req.MetaData.GetCollectionMetadata(task.CollectionIdentifier())
-
-		task.req.Lock()
-		if task.OperationConfig.End+task.MetaData.Seed > task.MetaData.SeedEnd {
-			task.req.AddToSeedEnd(task.MetaData, (task.OperationConfig.End+task.MetaData.Seed)-(task.MetaData.SeedEnd))
-		}
-		log.Println(task.MetaData)
-		task.State = task_state.ConfigTaskState(task.MetaData.Seed, task.MetaData.SeedEnd, task.ResultSeed)
-		task.req.Unlock()
-
-	} else {
-		if task.State == nil {
-			return task.ResultSeed, task_errors.ErrTaskStateIsNil
-		}
-		task.State.SetupStoringKeys()
-		_ = task_result.DeleteResultFile(task.ResultSeed)
-		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-	}
-	return task.ResultSeed, nil
-}
-
-func (task *SubDocInsert) TearUp() error {
-	task.Result.StopStoringResult()
-	//Use this case to store task's state on disk when required
-	//if err := task.State.SaveTaskSateOnDisk(); err != nil {
-	//	log.Println("Error in storing TASK State on DISK")
-	//}
-	if err := task.Result.SaveResultIntoFile(); err != nil {
-		log.Println("not able to save Result into ", task.ResultSeed, task.Operation)
-	}
-	task.Result = nil
-	task.State.StopStoringState()
-	task.TaskPending = false
-	return task.req.SaveRequestIntoFile()
-}
-
-func (task *SubDocInsert) Do() error {
-
-	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
-
-	collectionObject, err1 := task.GetCollectionObject()
-
-	task.gen = docgenerator.ConfigGenerator(
-		task.OperationConfig.KeySize,
-		task.OperationConfig.DocSize,
-		task.OperationConfig.DocType,
-		task.OperationConfig.KeyPrefix,
-		task.OperationConfig.KeySuffix,
-		template.InitialiseTemplate(task.OperationConfig.TemplateName))
-
-	if err1 != nil {
-		task.Result.ErrorOther = err1.Error()
-		task.Result.FailWholeBulkOperation(task.OperationConfig.Start, task.OperationConfig.End,
-			err1, task.State, task.gen, task.MetaData.Seed)
-		return task.TearUp()
-	}
-
-	insertSubDocuments(task, collectionObject)
-	task.Result.Success = (task.OperationConfig.End - task.OperationConfig.Start) - task.Result.Failure
-
-	return task.TearUp()
-}
-
-// insertDocuments uploads new documents in a bucket.scope.collection in a defined batch size at multiple iterations.
-func insertSubDocuments(task *SubDocInsert, collectionObject *cb_sdk.CollectionObject) {
-
-	if task.req.ContextClosed() {
-		return
-	}
-
-	routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
-	dataChannel := make(chan int64, tasks.MaxConcurrentRoutines)
-
-	skip := make(map[int64]struct{})
-	for _, offset := range task.State.KeyStates.Completed {
-		skip[offset] = struct{}{}
-	}
-	for _, offset := range task.State.KeyStates.Err {
-		skip[offset] = struct{}{}
-	}
-	group := errgroup.Group{}
-	for iteration := task.OperationConfig.Start; iteration < task.OperationConfig.End; iteration++ {
-
-		if task.req.ContextClosed() {
-			close(routineLimiter)
-			close(dataChannel)
-			return
-		}
-
-		routineLimiter <- struct{}{}
-		dataChannel <- iteration
-		group.Go(func() error {
-			offset := <-dataChannel
-			key := offset + task.MetaData.Seed
-			docId := task.gen.BuildKey(key)
-
-			if _, ok := skip[offset]; ok {
-				<-routineLimiter
-				return fmt.Errorf("alreday performed operation on " + docId)
-			}
-			fake := faker.NewWithSeed(rand.NewSource(int64(key)))
-
-			var err error
-			initTime := time.Now().UTC().Format(time.RFC850)
-			for retry := 0; retry < int(math.Max(float64(1), float64(task.OperationConfig.Exceptions.
-				RetryAttempts))); retry++ {
-
-				var iOps []gocb.MutateInSpec
-				for path, value := range task.gen.Template.GenerateSubPathAndValue(&fake, task.OperationConfig.DocSize) {
-					iOps = append(iOps, gocb.InsertSpec(path, value, &gocb.InsertSpecOptions{
-						CreatePath: task.InsertSpecOptions.CreatePath,
-						IsXattr:    task.InsertSpecOptions.IsXattr,
-					}))
-				}
-
-				if !task.InsertSpecOptions.IsXattr {
-					iOps = append(iOps, gocb.IncrementSpec(template.MutatedPath,
-						int64(template.MutateFieldIncrement), &gocb.CounterSpecOptions{
-							CreatePath: true,
-							IsXattr:    false,
-						}))
-				}
-
-				initTime = time.Now().UTC().Format(time.RFC850)
-				_, err = collectionObject.Collection.MutateIn(docId, iOps, &gocb.MutateInOptions{
-					Expiry:          time.Duration(task.MutateInOptions.Expiry) * time.Second,
-					PersistTo:       task.MutateInOptions.PersistTo,
-					ReplicateTo:     task.MutateInOptions.ReplicateTo,
-					DurabilityLevel: cb_sdk.GetDurability(task.MutateInOptions.Durability),
-					StoreSemantic:   cb_sdk.GetStoreSemantic(task.MutateInOptions.StoreSemantic),
-					Timeout:         time.Duration(task.MutateInOptions.Expiry) * time.Second,
-					PreserveExpiry:  task.MutateInOptions.PreserveExpiry,
-				})
-
-				if err == nil {
-					break
-				}
-			}
-			if err != nil {
-				if errors.Is(err, gocb.ErrPathExists) {
-					task.State.StateChannel <- task_state.StateHelper{Status: task_state.COMPLETED, Offset: offset}
-					<-routineLimiter
-					return nil
-				} else {
-					task.Result.IncrementFailure(initTime, docId, err, false, 0, offset)
-					task.State.StateChannel <- task_state.StateHelper{Status: task_state.ERR, Offset: offset}
-					<-routineLimiter
-					return err
-				}
-			}
-
-			task.State.StateChannel <- task_state.StateHelper{Status: task_state.COMPLETED, Offset: offset}
-			<-routineLimiter
-			return nil
-		})
-	}
-
-	_ = group.Wait()
-	close(routineLimiter)
-	close(dataChannel)
-	task.PostTaskExceptionHandling(collectionObject)
-	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-}
-
-func (task *SubDocInsert) PostTaskExceptionHandling(collectionObject *cb_sdk.CollectionObject) {
-	task.Result.StopStoringResult()
-	task.State.StopStoringState()
-	if task.OperationConfig.Exceptions.RetryAttempts <= 0 {
-		return
-	}
-
-	// Get all the errorOffset
-	errorOffsetMaps := task.State.ReturnErrOffset()
-	// Get all the completed offset
-	completedOffsetMaps := task.State.ReturnCompletedOffset()
-
-	// For the offset in ignore exceptions :-> move them from error to completed
-	shiftErrToCompletedOnIgnore(task.OperationConfig.Exceptions.IgnoreExceptions, task.Result, errorOffsetMaps, completedOffsetMaps)
-
-	if task.OperationConfig.Exceptions.RetryAttempts > 0 {
-
-		exceptionList := GetExceptions(task.Result, task.OperationConfig.Exceptions.RetryExceptions)
-
-		// For the retry exceptions :-> move them on success after retrying from err to completed
-		for _, exception := range exceptionList {
-
-			errorOffsetListMap := make([]map[int64]RetriedResult, 0)
-			for _, failedDocs := range task.Result.BulkError[exception] {
-				m := make(map[int64]RetriedResult)
-				m[failedDocs.Offset] = RetriedResult{}
-				errorOffsetListMap = append(errorOffsetListMap, m)
-			}
-
-			routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
-			dataChannel := make(chan map[int64]RetriedResult, tasks.MaxConcurrentRoutines)
-			wg := errgroup.Group{}
-			for _, x := range errorOffsetListMap {
-				dataChannel <- x
-				routineLimiter <- struct{}{}
-				wg.Go(func() error {
-					m := <-dataChannel
-					var offset = int64(-1)
-					for k, _ := range m {
-						offset = k
-					}
-					key := offset + task.MetaData.Seed
-					docId := task.gen.BuildKey(key)
-					fake := faker.NewWithSeed(rand.NewSource(int64(key)))
-					retry := 0
-					var err error
-
-					result := &gocb.MutateInResult{}
-					initTime := time.Now().UTC().Format(time.RFC850)
-					for retry = 0; retry <= task.OperationConfig.Exceptions.RetryAttempts; retry++ {
-
-						var iOps []gocb.MutateInSpec
-						for path, value := range task.gen.Template.GenerateSubPathAndValue(&fake,
-							task.OperationConfig.DocSize) {
-							iOps = append(iOps, gocb.InsertSpec(path, value, &gocb.InsertSpecOptions{
-								CreatePath: task.InsertSpecOptions.CreatePath,
-								IsXattr:    task.InsertSpecOptions.IsXattr,
-							}))
-						}
-
-						if !task.InsertSpecOptions.IsXattr {
-							iOps = append(iOps, gocb.IncrementSpec(template.MutatedPath,
-								int64(template.MutateFieldIncrement), &gocb.CounterSpecOptions{
-									CreatePath: true,
-									IsXattr:    false,
-								}))
-						}
-
-						initTime = time.Now().UTC().Format(time.RFC850)
-						result, err = collectionObject.Collection.MutateIn(docId, iOps, &gocb.MutateInOptions{
-							Expiry:          time.Duration(task.MutateInOptions.Expiry) * time.Second,
-							PersistTo:       task.MutateInOptions.PersistTo,
-							ReplicateTo:     task.MutateInOptions.ReplicateTo,
-							DurabilityLevel: cb_sdk.GetDurability(task.MutateInOptions.Durability),
-							StoreSemantic:   cb_sdk.GetStoreSemantic(task.MutateInOptions.StoreSemantic),
-							Timeout:         time.Duration(task.MutateInOptions.Expiry) * time.Second,
-							PreserveExpiry:  task.MutateInOptions.PreserveExpiry,
-						})
-
-						if err == nil {
-							break
-						}
-					}
-
-					if err != nil {
-						m[offset] = RetriedResult{
-							Status:   false,
-							CAS:      0,
-							InitTime: initTime,
-							AckTime:  time.Now().UTC().Format(time.RFC850),
-						}
-					} else {
-						m[offset] = RetriedResult{
-							Status:   true,
-							CAS:      uint64(result.Cas()),
-							InitTime: initTime,
-							AckTime:  time.Now().UTC().Format(time.RFC850),
-						}
-					}
-
-					<-routineLimiter
-					return nil
-				})
-			}
-			_ = wg.Wait()
-
-			shiftErrToCompletedOnRetrying(exception, task.Result, errorOffsetListMap, errorOffsetMaps, completedOffsetMaps)
-		}
-	}
-
-	task.State.MakeCompleteKeyFromMap(completedOffsetMaps)
-	task.State.MakeErrorKeyFromMap(errorOffsetMaps)
-	task.Result.Failure = int64(len(task.State.KeyStates.Err))
-	log.Println("completed retrying:- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-
-}
-
-func (task *SubDocInsert) MatchResultSeed(resultSeed string) (bool, error) {
-	defer task.lock.Unlock()
-	task.lock.Lock()
-	if fmt.Sprintf("%d", task.ResultSeed) == resultSeed {
-		if task.TaskPending {
-			return true, task_errors.ErrTaskInPendingState
-		}
-		if task.Result == nil {
-			task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
-		}
-		return true, nil
-	}
-	return false, nil
-}
-
-func (task *SubDocInsert) GetCollectionObject() (*cb_sdk.CollectionObject, error) {
-	return task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope,
-		task.Collection)
-}
-
-func (task *SubDocInsert) SetException(exceptions Exceptions) {
-	task.OperationConfig.Exceptions = exceptions
-}
-
-func (task *SubDocInsert) GetOperationConfig() (*OperationConfig, *task_state.TaskState) {
-	return task.OperationConfig, task.State
-}
diff --git a/internal/tasks/bulk_loading_cb/task_sub_doc_read.go b/internal/tasks/bulk_loading_cb/task_sub_doc_read.go
deleted file mode 100644
index cf4039b..0000000
--- a/internal/tasks/bulk_loading_cb/task_sub_doc_read.go
+++ /dev/null
@@ -1,389 +0,0 @@
-package bulk_loading_cb
-
-import (
-	"fmt"
-	"github.com/couchbase/gocb/v2"
-	"github.com/couchbaselabs/sirius/internal/cb_sdk"
-	"github.com/couchbaselabs/sirius/internal/docgenerator"
-	"github.com/couchbaselabs/sirius/internal/meta_data"
-	"github.com/couchbaselabs/sirius/internal/task_errors"
-	"github.com/couchbaselabs/sirius/internal/task_result"
-	"github.com/couchbaselabs/sirius/internal/task_state"
-	"github.com/couchbaselabs/sirius/internal/tasks"
-	"github.com/couchbaselabs/sirius/internal/template"
-	"github.com/jaswdr/faker"
-	"golang.org/x/sync/errgroup"
-	"log"
-	"math"
-	"math/rand"
-	"strings"
-	"sync"
-	"time"
-)
-
-type SubDocRead struct {
-	IdentifierToken string                        `json:"identifierToken" doc:"true"`
-	ClusterConfig   *cb_sdk.ClusterConfig         `json:"clusterConfig" doc:"true"`
-	Bucket          string                        `json:"bucket" doc:"true"`
-	Scope           string                        `json:"scope,omitempty" doc:"true"`
-	Collection      string                        `json:"collection,omitempty" doc:"true"`
-	OperationConfig *OperationConfig              `json:"operationConfig" doc:"true"`
-	GetSpecOptions  *cb_sdk.GetSpecOptions        `json:"getSpecOptions" doc:"true"`
-	LookupInOptions *cb_sdk.LookupInOptions       `json:"lookupInOptions" doc:"true"`
-	Operation       string                        `json:"operation" doc:"false"`
-	ResultSeed      int64                         `json:"resultSeed" doc:"false"`
-	TaskPending     bool                          `json:"taskPending" doc:"false"`
-	State           *task_state.TaskState         `json:"State" doc:"false"`
-	MetaData        *meta_data.CollectionMetaData `json:"metaData" doc:"false"`
-	Result          *task_result.TaskResult       `json:"-" doc:"false"`
-	gen             *docgenerator.Generator       `json:"-" doc:"false"`
-	req             *tasks.Request                `json:"-" doc:"false"`
-	rerun           bool                          `json:"-" doc:"false"`
-	lock            sync.Mutex                    `json:"-" doc:"false"`
-}
-
-func (task *SubDocRead) Describe() string {
-	return " SubDocRead reads sub-document in bulk"
-}
-
-func (task *SubDocRead) CollectionIdentifier() string {
-	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
-	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
-		task.Collection}, ":")
-}
-
-func (task *SubDocRead) CheckIfPending() bool {
-	return task.TaskPending
-}
-
-// Config configures  the insert task
-func (task *SubDocRead) Config(req *tasks.Request, reRun bool) (int64, error) {
-	task.TaskPending = true
-	task.req = req
-
-	if task.req == nil {
-		task.TaskPending = false
-		return 0, task_errors.ErrRequestIsNil
-	}
-
-	task.req.ReconnectionManager()
-	if _, err := task.req.GetCluster(task.ClusterConfig); err != nil {
-		task.TaskPending = false
-		return 0, err
-	}
-
-	task.lock = sync.Mutex{}
-	task.rerun = reRun
-
-	if !reRun {
-		task.ResultSeed = int64(time.Now().UnixNano())
-		task.Operation = tasks.SubDocReadOperation
-
-		if task.Bucket == "" {
-			task.Bucket = cb_sdk.DefaultBucket
-		}
-		if task.Scope == "" {
-			task.Scope = cb_sdk.DefaultScope
-		}
-		if task.Collection == "" {
-			task.Collection = cb_sdk.DefaultCollection
-		}
-
-		if err := ConfigureOperationConfig(task.OperationConfig); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-
-		task.MetaData = task.req.MetaData.GetCollectionMetadata(task.CollectionIdentifier())
-
-		task.req.Lock()
-		task.State = task_state.ConfigTaskState(task.MetaData.Seed, task.MetaData.SeedEnd, task.ResultSeed)
-		task.req.Unlock()
-
-	} else {
-		if task.State == nil {
-			return task.ResultSeed, task_errors.ErrTaskStateIsNil
-		}
-		task.State.SetupStoringKeys()
-		_ = task_result.DeleteResultFile(task.ResultSeed)
-		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-	}
-	return task.ResultSeed, nil
-}
-
-func (task *SubDocRead) TearUp() error {
-	//Use this case to store task's state on disk when required
-	//if err := task.State.SaveTaskSateOnDisk(); err != nil {
-	//	log.Println("Error in storing TASK State on DISK")
-	//}
-	task.Result.StopStoringResult()
-	if err := task.Result.SaveResultIntoFile(); err != nil {
-		log.Println("not able to save Result into ", task.ResultSeed, task.Operation)
-	}
-	task.Result = nil
-	task.TaskPending = false
-	return task.req.SaveRequestIntoFile()
-}
-
-func (task *SubDocRead) Do() error {
-
-	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
-
-	collectionObject, err1 := task.GetCollectionObject()
-
-	task.gen = docgenerator.ConfigGenerator(
-		task.OperationConfig.KeySize,
-		task.OperationConfig.DocSize,
-		task.OperationConfig.DocType,
-		task.OperationConfig.KeyPrefix,
-		task.OperationConfig.KeySuffix,
-		template.InitialiseTemplate(task.OperationConfig.TemplateName))
-
-	if err1 != nil {
-		task.Result.ErrorOther = err1.Error()
-		task.Result.FailWholeBulkOperation(task.OperationConfig.Start, task.OperationConfig.End,
-			err1, task.State, task.gen, task.MetaData.Seed)
-		return task.TearUp()
-	}
-
-	readSubDocuments(task, collectionObject)
-	task.Result.Success = (task.OperationConfig.End - task.OperationConfig.Start) - task.Result.Failure
-
-	return task.TearUp()
-}
-
-// insertDocuments uploads new documents in a bucket.scope.collection in a defined batch size at multiple iterations.
-func readSubDocuments(task *SubDocRead, collectionObject *cb_sdk.CollectionObject) {
-
-	if task.req.ContextClosed() {
-		return
-	}
-
-	routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
-	dataChannel := make(chan int64, tasks.MaxConcurrentRoutines)
-
-	skip := make(map[int64]struct{})
-	for _, offset := range task.State.KeyStates.Completed {
-		skip[offset] = struct{}{}
-	}
-	for _, offset := range task.State.KeyStates.Err {
-		skip[offset] = struct{}{}
-	}
-	group := errgroup.Group{}
-	for iteration := task.OperationConfig.Start; iteration < task.OperationConfig.End; iteration++ {
-
-		if task.req.ContextClosed() {
-			close(routineLimiter)
-			close(dataChannel)
-			return
-		}
-
-		routineLimiter <- struct{}{}
-		dataChannel <- iteration
-		group.Go(func() error {
-			offset := <-dataChannel
-			key := offset + task.MetaData.Seed
-			docId := task.gen.BuildKey(key)
-
-			if _, ok := skip[offset]; ok {
-				<-routineLimiter
-				return fmt.Errorf("alreday performed operation on " + docId)
-			}
-
-			fake := faker.NewWithSeed(rand.NewSource(int64(key)))
-
-			var err error
-			result := &gocb.LookupInResult{}
-			var paths []string
-			initTime := time.Now().UTC().Format(time.RFC850)
-			for retry := 0; retry < int(math.Max(float64(1), float64(task.OperationConfig.Exceptions.
-				RetryAttempts))); retry++ {
-
-				var iOps []gocb.LookupInSpec
-				for path, _ := range task.gen.Template.GenerateSubPathAndValue(&fake, task.OperationConfig.DocSize) {
-					paths = append(paths, path)
-					iOps = append(iOps, gocb.GetSpec(path, &gocb.GetSpecOptions{
-						IsXattr: task.GetSpecOptions.IsXattr,
-					}))
-				}
-
-				initTime = time.Now().UTC().Format(time.RFC850)
-				result, err = collectionObject.Collection.LookupIn(docId, iOps, &gocb.LookupInOptions{
-					Timeout: time.Duration(task.LookupInOptions.Timeout) * time.Second,
-				})
-
-				if err == nil {
-					break
-				}
-			}
-			if err != nil {
-				task.Result.IncrementFailure(initTime, docId, err, false, 0, offset)
-				task.State.StateChannel <- task_state.StateHelper{Status: task_state.ERR, Offset: offset}
-				<-routineLimiter
-				return err
-			}
-
-			for index, _ := range paths {
-				var val interface{}
-				if err := result.ContentAt(uint(index), &val); err != nil {
-					task.Result.IncrementFailure(initTime, docId, err, false, 0, offset)
-					task.State.StateChannel <- task_state.StateHelper{Status: task_state.ERR, Offset: offset}
-					<-routineLimiter
-					return err
-				}
-			}
-
-			task.State.StateChannel <- task_state.StateHelper{Status: task_state.COMPLETED, Offset: offset}
-			<-routineLimiter
-			return nil
-		})
-	}
-
-	_ = group.Wait()
-	close(routineLimiter)
-	close(dataChannel)
-	task.PostTaskExceptionHandling(collectionObject)
-	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-}
-
-func (task *SubDocRead) PostTaskExceptionHandling(collectionObject *cb_sdk.CollectionObject) {
-	task.Result.StopStoringResult()
-	task.State.StopStoringState()
-
-	if task.OperationConfig.Exceptions.RetryAttempts <= 0 {
-		return
-	}
-
-	// Get all the errorOffset
-	errorOffsetMaps := task.State.ReturnErrOffset()
-	// Get all the completed offset
-	completedOffsetMaps := task.State.ReturnCompletedOffset()
-
-	// For the offset in ignore exceptions :-> move them from error to completed
-	shiftErrToCompletedOnIgnore(task.OperationConfig.Exceptions.IgnoreExceptions, task.Result, errorOffsetMaps, completedOffsetMaps)
-
-	if task.OperationConfig.Exceptions.RetryAttempts > 0 {
-
-		exceptionList := GetExceptions(task.Result, task.OperationConfig.Exceptions.RetryExceptions)
-
-		// For the retry exceptions :-> move them on success after retrying from err to completed
-		for _, exception := range exceptionList {
-
-			errorOffsetListMap := make([]map[int64]RetriedResult, 0)
-			for _, failedDocs := range task.Result.BulkError[exception] {
-				m := make(map[int64]RetriedResult)
-				m[failedDocs.Offset] = RetriedResult{}
-				errorOffsetListMap = append(errorOffsetListMap, m)
-			}
-
-			routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
-			dataChannel := make(chan map[int64]RetriedResult, tasks.MaxConcurrentRoutines)
-			wg := errgroup.Group{}
-			for _, x := range errorOffsetListMap {
-				dataChannel <- x
-				routineLimiter <- struct{}{}
-				wg.Go(func() error {
-					m := <-dataChannel
-					var offset = int64(-1)
-					for k, _ := range m {
-						offset = k
-					}
-					key := offset + task.MetaData.Seed
-					docId := task.gen.BuildKey(key)
-					fake := faker.NewWithSeed(rand.NewSource(int64(key)))
-
-					retry := 0
-					var err error
-					result := &gocb.LookupInResult{}
-					var paths []string
-
-					for retry = 0; retry <= task.OperationConfig.Exceptions.RetryAttempts; retry++ {
-
-						var iOps []gocb.LookupInSpec
-						for path, _ := range task.gen.Template.GenerateSubPathAndValue(&fake,
-							task.OperationConfig.DocSize) {
-
-							paths = append(paths, path)
-
-							iOps = append(iOps, gocb.GetSpec(path, &gocb.GetSpecOptions{
-								IsXattr: task.GetSpecOptions.IsXattr,
-							}))
-						}
-
-						_, err = collectionObject.Collection.LookupIn(docId, iOps, &gocb.LookupInOptions{
-							Timeout: time.Duration(task.LookupInOptions.Timeout) * time.Second,
-						})
-
-						if err == nil {
-							break
-						}
-					}
-
-					if err != nil {
-						m[offset] = RetriedResult{
-							Status: false,
-							CAS:    0,
-						}
-					} else {
-						for index, _ := range paths {
-							var val interface{}
-							if err := result.ContentAt(uint(index), &val); err != nil {
-								m[offset] = RetriedResult{
-									Status: false,
-									CAS:    0,
-								}
-								<-routineLimiter
-								return nil
-							}
-						}
-
-						m[offset] = RetriedResult{
-							Status: true,
-							CAS:    uint64(result.Cas()),
-						}
-					}
-
-					<-routineLimiter
-					return nil
-				})
-			}
-			_ = wg.Wait()
-
-			shiftErrToCompletedOnRetrying(exception, task.Result, errorOffsetListMap, errorOffsetMaps, completedOffsetMaps)
-		}
-	}
-
-	task.State.MakeCompleteKeyFromMap(completedOffsetMaps)
-	task.State.MakeErrorKeyFromMap(errorOffsetMaps)
-	task.Result.Failure = int64(len(task.State.KeyStates.Err))
-	log.Println("completed retrying:- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-
-}
-
-func (task *SubDocRead) MatchResultSeed(resultSeed string) (bool, error) {
-	defer task.lock.Unlock()
-	task.lock.Lock()
-	if fmt.Sprintf("%d", task.ResultSeed) == resultSeed {
-		if task.TaskPending {
-			return true, task_errors.ErrTaskInPendingState
-		}
-		if task.Result == nil {
-			task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
-		}
-		return true, nil
-	}
-	return false, nil
-}
-
-func (task *SubDocRead) GetCollectionObject() (*cb_sdk.CollectionObject, error) {
-	return task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope,
-		task.Collection)
-}
-
-func (task *SubDocRead) SetException(exceptions Exceptions) {
-	task.OperationConfig.Exceptions = exceptions
-}
-
-func (task *SubDocRead) GetOperationConfig() (*OperationConfig, *task_state.TaskState) {
-	return task.OperationConfig, task.State
-}
diff --git a/internal/tasks/bulk_loading_cb/task_sub_doc_replace.go b/internal/tasks/bulk_loading_cb/task_sub_doc_replace.go
deleted file mode 100644
index 92f3a49..0000000
--- a/internal/tasks/bulk_loading_cb/task_sub_doc_replace.go
+++ /dev/null
@@ -1,406 +0,0 @@
-package bulk_loading_cb
-
-import (
-	"fmt"
-	"github.com/couchbase/gocb/v2"
-	"github.com/couchbaselabs/sirius/internal/cb_sdk"
-	"github.com/couchbaselabs/sirius/internal/docgenerator"
-	"github.com/couchbaselabs/sirius/internal/meta_data"
-	"github.com/couchbaselabs/sirius/internal/task_errors"
-	"github.com/couchbaselabs/sirius/internal/task_result"
-	"github.com/couchbaselabs/sirius/internal/task_state"
-	"github.com/couchbaselabs/sirius/internal/tasks"
-	"github.com/couchbaselabs/sirius/internal/template"
-	"github.com/jaswdr/faker"
-	"golang.org/x/sync/errgroup"
-	"log"
-	"math"
-	"math/rand"
-	"strings"
-	"sync"
-	"time"
-)
-
-type SubDocReplace struct {
-	IdentifierToken    string                        `json:"identifierToken" doc:"true"`
-	ClusterConfig      *cb_sdk.ClusterConfig         `json:"clusterConfig" doc:"true"`
-	Bucket             string                        `json:"bucket" doc:"true"`
-	Scope              string                        `json:"scope,omitempty" doc:"true"`
-	Collection         string                        `json:"collection,omitempty" doc:"true"`
-	OperationConfig    *OperationConfig              `json:"operationConfig" doc:"true"`
-	ReplaceSpecOptions *cb_sdk.ReplaceSpecOptions    `json:"replaceSpecOptions" doc:"true"`
-	MutateInOptions    *cb_sdk.MutateInOptions       `json:"mutateInOptions" doc:"true"`
-	Operation          string                        `json:"operation" doc:"false"`
-	ResultSeed         int64                         `json:"resultSeed" doc:"false"`
-	TaskPending        bool                          `json:"taskPending" doc:"false"`
-	State              *task_state.TaskState         `json:"State" doc:"false"`
-	MetaData           *meta_data.CollectionMetaData `json:"metaData" doc:"false"`
-	Result             *task_result.TaskResult       `json:"-" doc:"false"`
-	gen                *docgenerator.Generator       `json:"-" doc:"false"`
-	req                *tasks.Request                `json:"-" doc:"false"`
-	rerun              bool                          `json:"-" doc:"false"`
-	lock               sync.Mutex                    `json:"-" doc:"false"`
-}
-
-func (task *SubDocReplace) Describe() string {
-	return " SubDocReplace upserts a Sub-Document"
-}
-
-func (task *SubDocReplace) CollectionIdentifier() string {
-	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
-	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
-		task.Collection}, ":")
-}
-
-func (task *SubDocReplace) CheckIfPending() bool {
-	return task.TaskPending
-}
-
-// Config configures  the insert task
-func (task *SubDocReplace) Config(req *tasks.Request, reRun bool) (int64, error) {
-	task.TaskPending = true
-	task.req = req
-
-	if task.req == nil {
-		task.TaskPending = false
-		return 0, task_errors.ErrRequestIsNil
-	}
-
-	task.req.ReconnectionManager()
-	if _, err := task.req.GetCluster(task.ClusterConfig); err != nil {
-		task.TaskPending = false
-		return 0, err
-	}
-
-	task.lock = sync.Mutex{}
-	task.rerun = reRun
-
-	if !reRun {
-		task.ResultSeed = int64(time.Now().UnixNano())
-		task.Operation = tasks.SubDocReplaceOperation
-
-		if task.Bucket == "" {
-			task.Bucket = cb_sdk.DefaultBucket
-		}
-		if task.Scope == "" {
-			task.Scope = cb_sdk.DefaultScope
-		}
-		if task.Collection == "" {
-			task.Collection = cb_sdk.DefaultCollection
-		}
-
-		if err := ConfigureOperationConfig(task.OperationConfig); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-
-		if err := cb_sdk.ConfigReplaceSpecOptions(task.ReplaceSpecOptions); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-
-		if err := cb_sdk.ConfigMutateInOptions(task.MutateInOptions); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-
-		task.MetaData = task.req.MetaData.GetCollectionMetadata(task.CollectionIdentifier())
-
-		task.req.Lock()
-		task.State = task_state.ConfigTaskState(task.MetaData.Seed, task.MetaData.SeedEnd, task.ResultSeed)
-		task.req.Unlock()
-
-	} else {
-		if task.State == nil {
-			return task.ResultSeed, task_errors.ErrTaskStateIsNil
-		}
-		task.State.SetupStoringKeys()
-		_ = task_result.DeleteResultFile(task.ResultSeed)
-		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-	}
-	return task.ResultSeed, nil
-}
-
-func (task *SubDocReplace) TearUp() error {
-	//Use this case to store task's state on disk when required
-	//if err := task.State.SaveTaskSateOnDisk(); err != nil {
-	//	log.Println("Error in storing TASK State on DISK")
-	//}
-	task.Result.StopStoringResult()
-	if err := task.Result.SaveResultIntoFile(); err != nil {
-		log.Println("not able to save Result into ", task.ResultSeed, task.Operation)
-	}
-	task.Result = nil
-	task.State.StopStoringState()
-	task.TaskPending = false
-	return task.req.SaveRequestIntoFile()
-}
-
-func (task *SubDocReplace) Do() error {
-
-	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
-
-	collectionObject, err1 := task.GetCollectionObject()
-
-	task.gen = docgenerator.ConfigGenerator(
-		task.OperationConfig.KeySize,
-		task.OperationConfig.DocSize,
-		task.OperationConfig.DocType,
-		task.OperationConfig.KeyPrefix,
-		task.OperationConfig.KeySuffix,
-		template.InitialiseTemplate(task.OperationConfig.TemplateName))
-
-	if err1 != nil {
-		task.Result.ErrorOther = err1.Error()
-		task.Result.FailWholeBulkOperation(task.OperationConfig.Start, task.OperationConfig.End,
-			err1, task.State, task.gen, task.MetaData.Seed)
-		return task.TearUp()
-	}
-
-	replaceSubDocuments(task, collectionObject)
-	task.Result.Success = (task.OperationConfig.End - task.OperationConfig.Start) - task.Result.Failure
-
-	return task.TearUp()
-}
-
-// insertDocuments uploads new documents in a bucket.scope.collection in a defined batch size at multiple iterations.
-func replaceSubDocuments(task *SubDocReplace, collectionObject *cb_sdk.CollectionObject) {
-
-	if task.req.ContextClosed() {
-		return
-	}
-
-	routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
-	dataChannel := make(chan int64, tasks.MaxConcurrentRoutines)
-
-	skip := make(map[int64]struct{})
-	for _, offset := range task.State.KeyStates.Completed {
-		skip[offset] = struct{}{}
-	}
-	for _, offset := range task.State.KeyStates.Err {
-		skip[offset] = struct{}{}
-	}
-	group := errgroup.Group{}
-	for iteration := task.OperationConfig.Start; iteration < task.OperationConfig.End; iteration++ {
-
-		if task.req.ContextClosed() {
-			close(routineLimiter)
-			close(dataChannel)
-			return
-		}
-
-		routineLimiter <- struct{}{}
-		dataChannel <- iteration
-		group.Go(func() error {
-			offset := <-dataChannel
-			key := offset + task.MetaData.Seed
-			docId := task.gen.BuildKey(key)
-
-			if _, ok := skip[offset]; ok {
-				<-routineLimiter
-				return fmt.Errorf("alreday performed operation on " + docId)
-			}
-			fake := faker.NewWithSeed(rand.NewSource(int64(key)))
-
-			subDocumentMap := task.gen.Template.GenerateSubPathAndValue(&fake, task.OperationConfig.DocSize)
-			retracePreviousSubDocMutations(task.req, task.CollectionIdentifier(), offset, *task.gen, &fake,
-				task.ResultSeed, subDocumentMap)
-
-			var err error
-			initTime := time.Now().UTC().Format(time.RFC850)
-			for retry := 0; retry < int(math.Max(float64(1), float64(task.OperationConfig.Exceptions.
-				RetryAttempts))); retry++ {
-
-				var iOps []gocb.MutateInSpec
-				for path, value := range task.gen.Template.GenerateSubPathAndValue(&fake, task.OperationConfig.DocSize) {
-					iOps = append(iOps, gocb.ReplaceSpec(path, value, &gocb.ReplaceSpecOptions{
-						IsXattr: task.ReplaceSpecOptions.IsXattr,
-					}))
-				}
-
-				if !task.ReplaceSpecOptions.IsXattr {
-					iOps = append(iOps, gocb.IncrementSpec(template.MutatedPath,
-						int64(template.MutateFieldIncrement), &gocb.CounterSpecOptions{
-							CreatePath: false,
-							IsXattr:    false,
-						}))
-				}
-
-				initTime = time.Now().UTC().Format(time.RFC850)
-				_, err = collectionObject.Collection.MutateIn(docId, iOps, &gocb.MutateInOptions{
-					Expiry:          time.Duration(task.MutateInOptions.Expiry) * time.Second,
-					PersistTo:       task.MutateInOptions.PersistTo,
-					ReplicateTo:     task.MutateInOptions.ReplicateTo,
-					DurabilityLevel: cb_sdk.GetDurability(task.MutateInOptions.Durability),
-					StoreSemantic:   cb_sdk.GetStoreSemantic(task.MutateInOptions.StoreSemantic),
-					Timeout:         time.Duration(task.MutateInOptions.Expiry) * time.Second,
-					PreserveExpiry:  task.MutateInOptions.PreserveExpiry,
-				})
-
-				if err == nil {
-					break
-				}
-			}
-			if err != nil {
-				task.Result.IncrementFailure(initTime, docId, err, false, 0, offset)
-				task.State.StateChannel <- task_state.StateHelper{Status: task_state.ERR, Offset: offset}
-				<-routineLimiter
-				return err
-
-			}
-
-			task.State.StateChannel <- task_state.StateHelper{Status: task_state.COMPLETED, Offset: offset}
-			<-routineLimiter
-			return nil
-		})
-	}
-
-	_ = group.Wait()
-	close(routineLimiter)
-	close(dataChannel)
-	task.PostTaskExceptionHandling(collectionObject)
-	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-}
-
-func (task *SubDocReplace) PostTaskExceptionHandling(collectionObject *cb_sdk.CollectionObject) {
-	task.Result.StopStoringResult()
-	task.State.StopStoringState()
-	if task.OperationConfig.Exceptions.RetryAttempts <= 0 {
-		return
-	}
-
-	// Get all the errorOffset
-	errorOffsetMaps := task.State.ReturnErrOffset()
-	// Get all the completed offset
-	completedOffsetMaps := task.State.ReturnCompletedOffset()
-
-	// For the offset in ignore exceptions :-> move them from error to completed
-	shiftErrToCompletedOnIgnore(task.OperationConfig.Exceptions.IgnoreExceptions, task.Result, errorOffsetMaps, completedOffsetMaps)
-
-	if task.OperationConfig.Exceptions.RetryAttempts > 0 {
-
-		exceptionList := GetExceptions(task.Result, task.OperationConfig.Exceptions.RetryExceptions)
-
-		// For the retry exceptions :-> move them on success after retrying from err to completed
-		for _, exception := range exceptionList {
-
-			errorOffsetListMap := make([]map[int64]RetriedResult, 0)
-			for _, failedDocs := range task.Result.BulkError[exception] {
-				m := make(map[int64]RetriedResult)
-				m[failedDocs.Offset] = RetriedResult{}
-				errorOffsetListMap = append(errorOffsetListMap, m)
-			}
-
-			routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
-			dataChannel := make(chan map[int64]RetriedResult, tasks.MaxConcurrentRoutines)
-			wg := errgroup.Group{}
-			for _, x := range errorOffsetListMap {
-				dataChannel <- x
-				routineLimiter <- struct{}{}
-				wg.Go(func() error {
-					m := <-dataChannel
-					var offset = int64(-1)
-					for k, _ := range m {
-						offset = k
-					}
-					key := offset + task.MetaData.Seed
-					docId := task.gen.BuildKey(key)
-					fake := faker.NewWithSeed(rand.NewSource(int64(key)))
-
-					subDocumentMap := task.gen.Template.GenerateSubPathAndValue(&fake, task.OperationConfig.DocSize)
-					retracePreviousSubDocMutations(task.req, task.CollectionIdentifier(), offset, *task.gen, &fake,
-						task.ResultSeed, subDocumentMap)
-
-					retry := 0
-					var err error
-					result := &gocb.MutateInResult{}
-
-					for retry = 0; retry <= task.OperationConfig.Exceptions.RetryAttempts; retry++ {
-
-						var iOps []gocb.MutateInSpec
-						for path, value := range task.gen.Template.GenerateSubPathAndValue(&fake,
-							task.OperationConfig.DocSize) {
-							iOps = append(iOps, gocb.ReplaceSpec(path, value, &gocb.ReplaceSpecOptions{
-								IsXattr: task.ReplaceSpecOptions.IsXattr,
-							}))
-						}
-
-						if !task.ReplaceSpecOptions.IsXattr {
-							iOps = append(iOps, gocb.IncrementSpec(template.MutatedPath,
-								int64(template.MutateFieldIncrement), &gocb.CounterSpecOptions{
-									CreatePath: false,
-									IsXattr:    false,
-								}))
-						}
-
-						result, err = collectionObject.Collection.MutateIn(docId, iOps, &gocb.MutateInOptions{
-							Expiry:          time.Duration(task.MutateInOptions.Expiry) * time.Second,
-							PersistTo:       task.MutateInOptions.PersistTo,
-							ReplicateTo:     task.MutateInOptions.ReplicateTo,
-							DurabilityLevel: cb_sdk.GetDurability(task.MutateInOptions.Durability),
-							StoreSemantic:   cb_sdk.GetStoreSemantic(task.MutateInOptions.StoreSemantic),
-							Timeout:         time.Duration(task.MutateInOptions.Expiry) * time.Second,
-							PreserveExpiry:  task.MutateInOptions.PreserveExpiry,
-						})
-
-						if err == nil {
-							break
-						}
-					}
-
-					if err != nil {
-						m[offset] = RetriedResult{
-							Status: true,
-							CAS:    0,
-						}
-					} else {
-						m[offset] = RetriedResult{
-							Status: true,
-							CAS:    uint64(result.Cas()),
-						}
-					}
-
-					<-routineLimiter
-					return nil
-				})
-			}
-			_ = wg.Wait()
-
-			shiftErrToCompletedOnRetrying(exception, task.Result, errorOffsetListMap, errorOffsetMaps, completedOffsetMaps)
-		}
-	}
-
-	task.State.MakeCompleteKeyFromMap(completedOffsetMaps)
-	task.State.MakeErrorKeyFromMap(errorOffsetMaps)
-	task.Result.Failure = int64(len(task.State.KeyStates.Err))
-	log.Println("completed retrying:- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-
-}
-
-func (task *SubDocReplace) MatchResultSeed(resultSeed string) (bool, error) {
-	defer task.lock.Unlock()
-	task.lock.Lock()
-	if fmt.Sprintf("%d", task.ResultSeed) == resultSeed {
-		if task.TaskPending {
-			return true, task_errors.ErrTaskInPendingState
-		}
-		if task.Result == nil {
-			task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
-		}
-		return true, nil
-	}
-	return false, nil
-}
-
-func (task *SubDocReplace) GetCollectionObject() (*cb_sdk.CollectionObject, error) {
-	return task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope,
-		task.Collection)
-}
-
-func (task *SubDocReplace) SetException(exceptions Exceptions) {
-	task.OperationConfig.Exceptions = exceptions
-}
-
-func (task *SubDocReplace) GetOperationConfig() (*OperationConfig, *task_state.TaskState) {
-	return task.OperationConfig, task.State
-}
diff --git a/internal/tasks/bulk_loading_cb/task_sub_doc_upsert.go b/internal/tasks/bulk_loading_cb/task_sub_doc_upsert.go
deleted file mode 100644
index 41e73e2..0000000
--- a/internal/tasks/bulk_loading_cb/task_sub_doc_upsert.go
+++ /dev/null
@@ -1,412 +0,0 @@
-package bulk_loading_cb
-
-import (
-	"fmt"
-	"github.com/couchbase/gocb/v2"
-	"github.com/couchbaselabs/sirius/internal/cb_sdk"
-	"github.com/couchbaselabs/sirius/internal/docgenerator"
-	"github.com/couchbaselabs/sirius/internal/meta_data"
-	"github.com/couchbaselabs/sirius/internal/task_errors"
-	"github.com/couchbaselabs/sirius/internal/task_result"
-	"github.com/couchbaselabs/sirius/internal/task_state"
-	"github.com/couchbaselabs/sirius/internal/tasks"
-	"github.com/couchbaselabs/sirius/internal/template"
-	"github.com/jaswdr/faker"
-	"golang.org/x/sync/errgroup"
-	"log"
-	"math"
-	"math/rand"
-	"strings"
-	"sync"
-	"time"
-)
-
-type SubDocUpsert struct {
-	IdentifierToken   string                        `json:"identifierToken" doc:"true"`
-	ClusterConfig     *cb_sdk.ClusterConfig         `json:"clusterConfig" doc:"true"`
-	Bucket            string                        `json:"bucket" doc:"true"`
-	Scope             string                        `json:"scope,omitempty" doc:"true"`
-	Collection        string                        `json:"collection,omitempty" doc:"true"`
-	OperationConfig   *OperationConfig              `json:"operationConfig" doc:"true"`
-	InsertSpecOptions *cb_sdk.InsertSpecOptions     `json:"insertSpecOptions" doc:"true"`
-	MutateInOptions   *cb_sdk.MutateInOptions       `json:"mutateInOptions" doc:"true"`
-	Operation         string                        `json:"operation" doc:"false"`
-	ResultSeed        int64                         `json:"resultSeed" doc:"false"`
-	TaskPending       bool                          `json:"taskPending" doc:"false"`
-	State             *task_state.TaskState         `json:"State" doc:"false"`
-	MetaData          *meta_data.CollectionMetaData `json:"metaData" doc:"false"`
-	Result            *task_result.TaskResult       `json:"-" doc:"false"`
-	gen               *docgenerator.Generator       `json:"-" doc:"false"`
-	req               *tasks.Request                `json:"-" doc:"false"`
-	rerun             bool                          `json:"-" doc:"false"`
-	lock              sync.Mutex                    `json:"-" doc:"false"`
-}
-
-func (task *SubDocUpsert) Describe() string {
-	return " SubDocUpsert upserts a Sub-Document"
-}
-
-func (task *SubDocUpsert) CollectionIdentifier() string {
-	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
-	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
-		task.Collection}, ":")
-}
-
-func (task *SubDocUpsert) CheckIfPending() bool {
-	return task.TaskPending
-}
-
-// Config configures  the insert task
-func (task *SubDocUpsert) Config(req *tasks.Request, reRun bool) (int64, error) {
-	task.TaskPending = true
-	task.req = req
-
-	if task.req == nil {
-		task.TaskPending = false
-		return 0, task_errors.ErrRequestIsNil
-	}
-
-	task.req.ReconnectionManager()
-	if _, err := task.req.GetCluster(task.ClusterConfig); err != nil {
-		task.TaskPending = false
-		return 0, err
-	}
-
-	task.lock = sync.Mutex{}
-	task.rerun = reRun
-
-	if !reRun {
-		task.ResultSeed = int64(time.Now().UnixNano())
-		task.Operation = tasks.SubDocUpsertOperation
-
-		if task.Bucket == "" {
-			task.Bucket = cb_sdk.DefaultBucket
-		}
-		if task.Scope == "" {
-			task.Scope = cb_sdk.DefaultScope
-		}
-		if task.Collection == "" {
-			task.Collection = cb_sdk.DefaultCollection
-		}
-
-		if err := ConfigureOperationConfig(task.OperationConfig); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-
-		if err := cb_sdk.ConfigInsertSpecOptions(task.InsertSpecOptions); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-
-		if err := cb_sdk.ConfigMutateInOptions(task.MutateInOptions); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-
-		task.MetaData = task.req.MetaData.GetCollectionMetadata(task.CollectionIdentifier())
-
-		task.req.Lock()
-		if task.OperationConfig.End+task.MetaData.Seed > task.MetaData.SeedEnd {
-			task.req.AddToSeedEnd(task.MetaData, (task.OperationConfig.End+task.MetaData.Seed)-(task.MetaData.SeedEnd))
-		}
-		task.State = task_state.ConfigTaskState(task.MetaData.Seed, task.MetaData.SeedEnd, task.ResultSeed)
-		task.req.Unlock()
-
-	} else {
-		if task.State == nil {
-			return task.ResultSeed, task_errors.ErrTaskStateIsNil
-		}
-		task.State.SetupStoringKeys()
-		_ = task_result.DeleteResultFile(task.ResultSeed)
-		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-	}
-	return task.ResultSeed, nil
-}
-
-func (task *SubDocUpsert) TearUp() error {
-	//Use this case to store task's state on disk when required
-	//if err := task.State.SaveTaskSateOnDisk(); err != nil {
-	//	log.Println("Error in storing TASK State on DISK")
-	//}
-	task.Result.StopStoringResult()
-	if err := task.Result.SaveResultIntoFile(); err != nil {
-		log.Println("not able to save Result into ", task.ResultSeed, task.Operation)
-	}
-	task.Result = nil
-	task.State.StopStoringState()
-	task.TaskPending = false
-	return task.req.SaveRequestIntoFile()
-}
-
-func (task *SubDocUpsert) Do() error {
-
-	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
-
-	collectionObject, err1 := task.GetCollectionObject()
-
-	task.gen = docgenerator.ConfigGenerator(
-		task.OperationConfig.KeySize,
-		task.OperationConfig.DocSize,
-		task.OperationConfig.DocType,
-		task.OperationConfig.KeyPrefix,
-		task.OperationConfig.KeySuffix,
-		template.InitialiseTemplate(task.OperationConfig.TemplateName))
-
-	if err1 != nil {
-		task.Result.ErrorOther = err1.Error()
-		task.Result.FailWholeBulkOperation(task.OperationConfig.Start, task.OperationConfig.End,
-			err1, task.State, task.gen, task.MetaData.Seed)
-		return task.TearUp()
-	}
-
-	upsertSubDocuments(task, collectionObject)
-	task.Result.Success = (task.OperationConfig.End - task.OperationConfig.Start) - task.Result.Failure
-
-	return task.TearUp()
-}
-
-// insertDocuments uploads new documents in a bucket.scope.collection in a defined batch size at multiple iterations.
-func upsertSubDocuments(task *SubDocUpsert, collectionObject *cb_sdk.CollectionObject) {
-
-	if task.req.ContextClosed() {
-		return
-	}
-
-	routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
-	dataChannel := make(chan int64, tasks.MaxConcurrentRoutines)
-
-	skip := make(map[int64]struct{})
-	for _, offset := range task.State.KeyStates.Completed {
-		skip[offset] = struct{}{}
-	}
-	for _, offset := range task.State.KeyStates.Err {
-		skip[offset] = struct{}{}
-	}
-	group := errgroup.Group{}
-	for iteration := task.OperationConfig.Start; iteration < task.OperationConfig.End; iteration++ {
-
-		if task.req.ContextClosed() {
-			close(routineLimiter)
-			close(dataChannel)
-			return
-		}
-
-		routineLimiter <- struct{}{}
-		dataChannel <- iteration
-		group.Go(func() error {
-			offset := <-dataChannel
-			key := offset + task.MetaData.Seed
-			docId := task.gen.BuildKey(key)
-
-			if _, ok := skip[offset]; ok {
-				<-routineLimiter
-				return fmt.Errorf("alreday performed operation on " + docId)
-			}
-			fake := faker.NewWithSeed(rand.NewSource(int64(key)))
-
-			subDocumentMap := task.gen.Template.GenerateSubPathAndValue(&fake, task.OperationConfig.DocSize)
-			retracePreviousSubDocMutations(task.req, task.CollectionIdentifier(), offset, *task.gen, &fake,
-				task.ResultSeed, subDocumentMap)
-
-			var err error
-			initTime := time.Now().UTC().Format(time.RFC850)
-			for retry := 0; retry < int(math.Max(float64(1), float64(task.OperationConfig.Exceptions.
-				RetryAttempts))); retry++ {
-
-				var iOps []gocb.MutateInSpec
-				for path, value := range task.gen.Template.GenerateSubPathAndValue(&fake, task.OperationConfig.DocSize) {
-					iOps = append(iOps, gocb.UpsertSpec(path, value, &gocb.UpsertSpecOptions{
-						CreatePath: task.InsertSpecOptions.CreatePath,
-						IsXattr:    task.InsertSpecOptions.IsXattr,
-					}))
-				}
-
-				if !task.InsertSpecOptions.IsXattr {
-					iOps = append(iOps, gocb.IncrementSpec(template.MutatedPath,
-						int64(template.MutateFieldIncrement), &gocb.CounterSpecOptions{
-							CreatePath: false,
-							IsXattr:    false,
-						}))
-				}
-
-				initTime = time.Now().UTC().Format(time.RFC850)
-				_, err = collectionObject.Collection.MutateIn(docId, iOps, &gocb.MutateInOptions{
-					Expiry:          time.Duration(task.MutateInOptions.Expiry) * time.Second,
-					PersistTo:       task.MutateInOptions.PersistTo,
-					ReplicateTo:     task.MutateInOptions.ReplicateTo,
-					DurabilityLevel: cb_sdk.GetDurability(task.MutateInOptions.Durability),
-					StoreSemantic:   cb_sdk.GetStoreSemantic(task.MutateInOptions.StoreSemantic),
-					Timeout:         time.Duration(task.MutateInOptions.Expiry) * time.Second,
-					PreserveExpiry:  task.MutateInOptions.PreserveExpiry,
-				})
-
-				if err == nil {
-					break
-				}
-			}
-			if err != nil {
-				task.Result.IncrementFailure(initTime, docId, err, false, 0, offset)
-				task.State.StateChannel <- task_state.StateHelper{Status: task_state.ERR, Offset: offset}
-				<-routineLimiter
-				return err
-
-			}
-
-			task.State.StateChannel <- task_state.StateHelper{Status: task_state.COMPLETED, Offset: offset}
-			<-routineLimiter
-			return nil
-		})
-	}
-
-	_ = group.Wait()
-	close(routineLimiter)
-	close(dataChannel)
-	task.PostTaskExceptionHandling(collectionObject)
-	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-}
-
-func (task *SubDocUpsert) PostTaskExceptionHandling(collectionObject *cb_sdk.CollectionObject) {
-	task.Result.StopStoringResult()
-	task.State.StopStoringState()
-
-	if task.OperationConfig.Exceptions.RetryAttempts <= 0 {
-		return
-	}
-
-	// Get all the errorOffset
-	errorOffsetMaps := task.State.ReturnErrOffset()
-	// Get all the completed offset
-	completedOffsetMaps := task.State.ReturnCompletedOffset()
-
-	// For the offset in ignore exceptions :-> move them from error to completed
-	shiftErrToCompletedOnIgnore(task.OperationConfig.Exceptions.IgnoreExceptions, task.Result, errorOffsetMaps, completedOffsetMaps)
-
-	if task.OperationConfig.Exceptions.RetryAttempts > 0 {
-
-		exceptionList := GetExceptions(task.Result, task.OperationConfig.Exceptions.RetryExceptions)
-
-		// For the retry exceptions :-> move them on success after retrying from err to completed
-		for _, exception := range exceptionList {
-
-			errorOffsetListMap := make([]map[int64]RetriedResult, 0)
-			for _, failedDocs := range task.Result.BulkError[exception] {
-				m := make(map[int64]RetriedResult)
-				m[failedDocs.Offset] = RetriedResult{}
-				errorOffsetListMap = append(errorOffsetListMap, m)
-			}
-
-			routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
-			dataChannel := make(chan map[int64]RetriedResult, tasks.MaxConcurrentRoutines)
-			wg := errgroup.Group{}
-			for _, x := range errorOffsetListMap {
-				dataChannel <- x
-				routineLimiter <- struct{}{}
-				wg.Go(func() error {
-					m := <-dataChannel
-					var offset = int64(-1)
-					for k, _ := range m {
-						offset = k
-					}
-					key := offset + task.MetaData.Seed
-					docId := task.gen.BuildKey(key)
-					fake := faker.NewWithSeed(rand.NewSource(int64(key)))
-
-					subDocumetMap := task.gen.Template.GenerateSubPathAndValue(&fake, task.OperationConfig.DocSize)
-					retracePreviousSubDocMutations(task.req, task.CollectionIdentifier(), offset, *task.gen, &fake,
-						task.ResultSeed, subDocumetMap)
-
-					retry := 0
-					var err error
-					result := &gocb.MutateInResult{}
-
-					for retry = 0; retry <= task.OperationConfig.Exceptions.RetryAttempts; retry++ {
-
-						var iOps []gocb.MutateInSpec
-						for path, value := range task.gen.Template.GenerateSubPathAndValue(&fake,
-							task.OperationConfig.DocSize) {
-							iOps = append(iOps, gocb.UpsertSpec(path, value, &gocb.UpsertSpecOptions{
-								CreatePath: task.InsertSpecOptions.CreatePath,
-								IsXattr:    task.InsertSpecOptions.IsXattr,
-							}))
-						}
-
-						if !task.InsertSpecOptions.IsXattr {
-							iOps = append(iOps, gocb.IncrementSpec(template.MutatedPath,
-								int64(template.MutateFieldIncrement), &gocb.CounterSpecOptions{
-									CreatePath: true,
-									IsXattr:    false,
-								}))
-						}
-
-						result, err = collectionObject.Collection.MutateIn(docId, iOps, &gocb.MutateInOptions{
-							Expiry:          time.Duration(task.MutateInOptions.Expiry) * time.Second,
-							PersistTo:       task.MutateInOptions.PersistTo,
-							ReplicateTo:     task.MutateInOptions.ReplicateTo,
-							DurabilityLevel: cb_sdk.GetDurability(task.MutateInOptions.Durability),
-							StoreSemantic:   cb_sdk.GetStoreSemantic(task.MutateInOptions.StoreSemantic),
-							Timeout:         time.Duration(task.MutateInOptions.Expiry) * time.Second,
-							PreserveExpiry:  task.MutateInOptions.PreserveExpiry,
-						})
-
-						if err == nil {
-							break
-						}
-					}
-
-					if err != nil {
-						m[offset] = RetriedResult{
-							Status: true,
-							CAS:    0,
-						}
-					} else {
-						m[offset] = RetriedResult{
-							Status: true,
-							CAS:    uint64(result.Cas()),
-						}
-					}
-
-					<-routineLimiter
-					return nil
-				})
-			}
-			_ = wg.Wait()
-
-			shiftErrToCompletedOnRetrying(exception, task.Result, errorOffsetListMap, errorOffsetMaps, completedOffsetMaps)
-		}
-	}
-
-	task.State.MakeCompleteKeyFromMap(completedOffsetMaps)
-	task.State.MakeErrorKeyFromMap(errorOffsetMaps)
-	task.Result.Failure = int64(len(task.State.KeyStates.Err))
-	log.Println("completed retrying:- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-
-}
-
-func (task *SubDocUpsert) MatchResultSeed(resultSeed string) (bool, error) {
-	defer task.lock.Unlock()
-	task.lock.Lock()
-	if fmt.Sprintf("%d", task.ResultSeed) == resultSeed {
-		if task.TaskPending {
-			return true, task_errors.ErrTaskInPendingState
-		}
-		if task.Result == nil {
-			task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
-		}
-		return true, nil
-	}
-	return false, nil
-}
-
-func (task *SubDocUpsert) GetCollectionObject() (*cb_sdk.CollectionObject, error) {
-	return task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope,
-		task.Collection)
-}
-
-func (task *SubDocUpsert) SetException(exceptions Exceptions) {
-	task.OperationConfig.Exceptions = exceptions
-}
-
-func (task *SubDocUpsert) GetOperationConfig() (*OperationConfig, *task_state.TaskState) {
-	return task.OperationConfig, task.State
-}
diff --git a/internal/tasks/bulk_loading_cb/task_validate.go b/internal/tasks/bulk_loading_cb/task_validate.go
deleted file mode 100644
index 9642499..0000000
--- a/internal/tasks/bulk_loading_cb/task_validate.go
+++ /dev/null
@@ -1,374 +0,0 @@
-package bulk_loading_cb
-
-import (
-	"encoding/json"
-	"errors"
-	"fmt"
-	"github.com/couchbase/gocb/v2"
-	"github.com/couchbaselabs/sirius/internal/cb_sdk"
-	"github.com/couchbaselabs/sirius/internal/docgenerator"
-	"github.com/couchbaselabs/sirius/internal/meta_data"
-	"github.com/couchbaselabs/sirius/internal/task_errors"
-	"github.com/couchbaselabs/sirius/internal/task_result"
-	"github.com/couchbaselabs/sirius/internal/task_state"
-	"github.com/couchbaselabs/sirius/internal/tasks"
-	"github.com/couchbaselabs/sirius/internal/template"
-	"github.com/jaswdr/faker"
-	"golang.org/x/sync/errgroup"
-	"log"
-	"math"
-	"math/rand"
-	"strings"
-	"sync"
-	"time"
-)
-
-type ValidateTask struct {
-	IdentifierToken string                        `json:"identifierToken" doc:"true"`
-	ClusterConfig   *cb_sdk.ClusterConfig         `json:"clusterConfig" doc:"true"`
-	Bucket          string                        `json:"bucket" doc:"true"`
-	Scope           string                        `json:"scope,omitempty" doc:"true"`
-	Collection      string                        `json:"collection,omitempty" doc:"true"`
-	Operation       string                        `json:"operation" doc:"false"`
-	ResultSeed      int64                         `json:"resultSeed" doc:"false"`
-	TaskPending     bool                          `json:"taskPending" doc:"false"`
-	MetaData        *meta_data.CollectionMetaData `json:"metaData" doc:"false"`
-	State           *task_state.TaskState         `json:"State" doc:"false"`
-	Result          *task_result.TaskResult       `json:"-" doc:"false"`
-	gen             *docgenerator.Generator       `json:"-" doc:"false"`
-	req             *tasks.Request                `json:"-" doc:"false"`
-	rerun           bool                          `json:"-" doc:"false"`
-	lock            sync.Mutex                    `json:"" doc:"false"`
-}
-
-func (task *ValidateTask) CollectionIdentifier() string {
-	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
-	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
-		task.Collection}, ":")
-}
-
-func (task *ValidateTask) Describe() string {
-	return "Validates every document in the cluster's bucket"
-}
-
-func (task *ValidateTask) CheckIfPending() bool {
-	return task.TaskPending
-}
-
-func (task *ValidateTask) TearUp() error {
-	if err := task.Result.SaveResultIntoFile(); err != nil {
-		log.Println("not able to save Result into ", task.ResultSeed, task.Operation)
-	}
-	task.Result.StopStoringResult()
-	task.Result = nil
-	task.State.StopStoringState()
-	task.TaskPending = false
-	return task.req.SaveRequestIntoFile()
-}
-
-func (task *ValidateTask) Config(req *tasks.Request, reRun bool) (int64, error) {
-	task.TaskPending = true
-	task.req = req
-
-	if task.req == nil {
-		task.TaskPending = false
-		return 0, task_errors.ErrRequestIsNil
-	}
-
-	task.req.ReconnectionManager()
-	if _, err := task.req.GetCluster(task.ClusterConfig); err != nil {
-		task.TaskPending = false
-		return 0, err
-	}
-
-	task.lock = sync.Mutex{}
-	task.rerun = false
-
-	if !reRun {
-		task.ResultSeed = int64(time.Now().UnixNano())
-		task.Operation = tasks.ValidateOperation
-
-		if task.Bucket == "" {
-			task.Bucket = cb_sdk.DefaultBucket
-		}
-		if task.Scope == "" {
-			task.Scope = cb_sdk.DefaultScope
-		}
-		if task.Collection == "" {
-			task.Collection = cb_sdk.DefaultCollection
-		}
-
-		task.MetaData = task.req.MetaData.GetCollectionMetadata(task.CollectionIdentifier())
-
-		task.req.Lock()
-		task.State = task_state.ConfigTaskState(task.MetaData.Seed, task.MetaData.SeedEnd, task.ResultSeed)
-		task.req.Unlock()
-
-	} else {
-		if task.State == nil {
-			return task.ResultSeed, task_errors.ErrTaskStateIsNil
-		}
-		task.State.SetupStoringKeys()
-		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-	}
-	return task.ResultSeed, nil
-}
-
-func (task *ValidateTask) Do() error {
-
-	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
-
-	collectionObject, err1 := task.GetCollectionObject()
-
-	task.gen = docgenerator.ConfigGenerator(
-		docgenerator.DefaultKeySize,
-		docgenerator.DefaultDocSize,
-		docgenerator.JsonDocument,
-		docgenerator.DefaultKeyPrefix,
-		docgenerator.DefaultKeySuffix,
-		template.InitialiseTemplate("person"))
-
-	if err1 != nil {
-		task.Result.ErrorOther = err1.Error()
-		task.Result.FailWholeBulkOperation(0, task.MetaData.Seed-task.MetaData.SeedEnd,
-			err1, task.State, task.gen, task.MetaData.Seed)
-		return task.TearUp()
-	}
-
-	validateDocuments(task, collectionObject)
-
-	task.Result.Success = task.State.SeedEnd - task.State.SeedStart - task.Result.Failure
-
-	return task.TearUp()
-}
-
-// ValidateDocuments return the validity of the collection using TaskState
-func validateDocuments(task *ValidateTask, collectionObject *cb_sdk.CollectionObject) {
-
-	if task.req.ContextClosed() {
-		return
-	}
-
-	routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
-	dataChannel := make(chan int64, tasks.MaxConcurrentRoutines)
-	skip := make(map[int64]struct{})
-	for _, offset := range task.State.KeyStates.Completed {
-		skip[offset] = struct{}{}
-	}
-	for _, offset := range task.State.KeyStates.Err {
-		skip[offset] = struct{}{}
-	}
-	deletedOffset, err1 := retracePreviousDeletions(task.req, task.CollectionIdentifier(), task.ResultSeed)
-	if err1 != nil {
-		log.Println(err1)
-		return
-	}
-
-	deletedOffsetSubDoc, err2 := retracePreviousSubDocDeletions(task.req, task.CollectionIdentifier(), task.ResultSeed)
-	if err2 != nil {
-		log.Println(err2)
-		return
-	}
-
-	group := errgroup.Group{}
-	for offset := int64(0); offset < (task.MetaData.SeedEnd - task.MetaData.Seed); offset++ {
-
-		if task.req.ContextClosed() {
-			close(routineLimiter)
-			close(dataChannel)
-			return
-		}
-
-		routineLimiter <- struct{}{}
-		dataChannel <- offset
-		group.Go(func() error {
-			offset := <-dataChannel
-
-			if _, ok := skip[offset]; ok {
-				<-routineLimiter
-				return nil
-			}
-
-			operationConfigDoc, err := retrieveLastConfig(task.req, offset, false)
-			if err != nil {
-				<-routineLimiter
-				return err
-			}
-			operationConfigSubDoc, err := retrieveLastConfig(task.req, offset, true)
-
-			/* Resetting the doc generator for the offset as per
-			the last configuration of operation performed on offset.
-			*/
-			genDoc := docgenerator.Reset(
-				operationConfigDoc.KeySize,
-				operationConfigDoc.DocSize,
-				operationConfigDoc.DocType,
-				operationConfigDoc.KeyPrefix,
-				operationConfigDoc.KeySuffix,
-				operationConfigDoc.TemplateName,
-			)
-
-			genSubDoc := docgenerator.Reset(
-				operationConfigSubDoc.KeySize,
-				operationConfigSubDoc.DocSize,
-				operationConfigSubDoc.DocType,
-				operationConfigSubDoc.KeyPrefix,
-				operationConfigSubDoc.KeySuffix,
-				operationConfigSubDoc.TemplateName,
-			)
-
-			/* building Key and doc as per
-			local config off the offset.
-			*/
-			key := task.MetaData.Seed + offset
-			docId := genDoc.BuildKey(key)
-
-			fake := faker.NewWithSeed(rand.NewSource(int64(key)))
-			fakeSub := faker.NewWithSeed(rand.NewSource(int64(key)))
-			initTime := time.Now().UTC().Format(time.RFC850)
-
-			originalDocument, err := genDoc.Template.GenerateDocument(&fake, operationConfigDoc.DocSize)
-			if err != nil {
-				task.Result.IncrementFailure(initTime, docId, err, false, 0, offset)
-				task.State.StateChannel <- task_state.StateHelper{Status: task_state.ERR, Offset: offset}
-				<-routineLimiter
-				return err
-			}
-			updatedDocument, err := retracePreviousMutations(task.req, task.CollectionIdentifier(), offset,
-				originalDocument, *genDoc,
-				&fake,
-				task.ResultSeed)
-			if err != nil {
-				task.Result.IncrementFailure(initTime, docId, err, false, 0, offset)
-				task.State.StateChannel <- task_state.StateHelper{Status: task_state.ERR, Offset: offset}
-				<-routineLimiter
-				return err
-			}
-
-			subDocumentMap := genSubDoc.Template.GenerateSubPathAndValue(&fakeSub, operationConfigSubDoc.DocSize)
-			subDocumentMap, err = retracePreviousSubDocMutations(task.req, task.CollectionIdentifier(), offset,
-				*genSubDoc,
-				&fakeSub,
-				task.ResultSeed,
-				subDocumentMap)
-			if err != nil {
-				task.Result.IncrementFailure(initTime, docId, err, false, 0, offset)
-				task.State.StateChannel <- task_state.StateHelper{Status: task_state.ERR, Offset: offset}
-				<-routineLimiter
-				return err
-			}
-
-			mutationCount, err := countMutation(task.req, task.CollectionIdentifier(), offset, task.ResultSeed)
-			if err != nil {
-				task.Result.IncrementFailure(initTime, docId, err, false, 0, offset)
-				task.State.StateChannel <- task_state.StateHelper{Status: task_state.ERR, Offset: offset}
-				<-routineLimiter
-				return err
-			}
-
-			updatedDocumentBytes, err := json.Marshal(updatedDocument)
-			if err != nil {
-				log.Println(err)
-				<-routineLimiter
-				return err
-			}
-
-			updatedDocumentMap := make(map[string]any)
-			if err := json.Unmarshal(updatedDocumentBytes, &updatedDocumentMap); err != nil {
-				log.Println(err)
-				<-routineLimiter
-				return err
-			}
-			updatedDocumentMap[template.MutatedPath] = float64(mutationCount)
-
-			result := &gocb.GetResult{}
-			resultFromHost := make(map[string]any)
-
-			initTime = time.Now().UTC().Format(time.RFC850)
-			for retry := 0; retry < int(math.Max(float64(1), float64(operationConfigDoc.Exceptions.
-				RetryAttempts))); retry++ {
-				result, err = collectionObject.Collection.Get(docId, nil)
-				if err == nil {
-					break
-				}
-			}
-
-			if err != nil {
-				if errors.Is(err, gocb.ErrDocumentNotFound) {
-					if _, ok := deletedOffset[offset]; ok {
-						task.State.StateChannel <- task_state.StateHelper{Status: task_state.COMPLETED, Offset: offset}
-						<-routineLimiter
-						return nil
-					}
-					if _, ok := deletedOffsetSubDoc[offset]; ok {
-						task.State.StateChannel <- task_state.StateHelper{Status: task_state.COMPLETED, Offset: offset}
-						<-routineLimiter
-						return nil
-					}
-				}
-				task.Result.IncrementFailure(initTime, docId, err, false, 0, offset)
-				task.State.StateChannel <- task_state.StateHelper{Status: task_state.ERR, Offset: offset}
-				<-routineLimiter
-				return err
-			}
-
-			if err := result.Content(&resultFromHost); err != nil {
-				task.Result.IncrementFailure(initTime, docId, err, false, 0, offset)
-				task.State.StateChannel <- task_state.StateHelper{Status: task_state.ERR, Offset: offset}
-				<-routineLimiter
-				return err
-			}
-
-			if !tasks.CompareDocumentsIsSame(resultFromHost, updatedDocumentMap, subDocumentMap) {
-				task.Result.IncrementFailure(initTime, docId, errors.New("integrity Lost"),
-					false, 0, offset)
-				task.State.StateChannel <- task_state.StateHelper{Status: task_state.ERR, Offset: offset}
-				<-routineLimiter
-				return err
-				//}
-			}
-
-			task.State.StateChannel <- task_state.StateHelper{Status: task_state.COMPLETED, Offset: offset}
-			<-routineLimiter
-			return nil
-		})
-	}
-	_ = group.Wait()
-	close(routineLimiter)
-	close(dataChannel)
-	task.PostTaskExceptionHandling(collectionObject)
-	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-
-}
-
-func (task *ValidateTask) PostTaskExceptionHandling(collectionObject *cb_sdk.CollectionObject) {
-	task.Result.StopStoringResult()
-	task.State.StopStoringState()
-}
-
-func (task *ValidateTask) MatchResultSeed(resultSeed string) (bool, error) {
-	defer task.lock.Unlock()
-	task.lock.Lock()
-	if fmt.Sprintf("%d", task.ResultSeed) == resultSeed {
-		if task.TaskPending {
-			return true, task_errors.ErrTaskInPendingState
-		}
-		if task.Result == nil {
-			task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
-		}
-		return true, nil
-	}
-	return false, nil
-}
-
-func (task *ValidateTask) GetCollectionObject() (*cb_sdk.CollectionObject, error) {
-	return task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope,
-		task.Collection)
-}
-
-func (task *ValidateTask) SetException(exceptions Exceptions) {
-}
-
-func (task *ValidateTask) GetOperationConfig() (*OperationConfig, *task_state.TaskState) {
-	return nil, task.State
-}
diff --git a/internal/tasks/bulk_query_cb/task_run_query.go b/internal/tasks/bulk_query_cb/task_run_query.go
index c051952..4dc7d17 100644
--- a/internal/tasks/bulk_query_cb/task_run_query.go
+++ b/internal/tasks/bulk_query_cb/task_run_query.go
@@ -1,255 +1,256 @@
 package bulk_query_cb
 
-import (
-	"fmt"
-	"github.com/couchbase/gocb/v2"
-	"github.com/couchbaselabs/sirius/internal/cb_sdk"
-	"github.com/couchbaselabs/sirius/internal/docgenerator"
-	"github.com/couchbaselabs/sirius/internal/task_errors"
-	"github.com/couchbaselabs/sirius/internal/task_result"
-	"github.com/couchbaselabs/sirius/internal/tasks"
-	"github.com/couchbaselabs/sirius/internal/template"
-	"golang.org/x/sync/errgroup"
-	"log"
-	"strings"
-	"time"
-)
-
-type QueryTask struct {
-	IdentifierToken      string                       `json:"identifierToken" doc:"true"`
-	ClusterConfig        *cb_sdk.ClusterConfig        `json:"clusterConfig" doc:"true"`
-	Bucket               string                       `json:"bucket" doc:"true"`
-	Scope                string                       `json:"scope,omitempty" doc:"true"`
-	Collection           string                       `json:"collection,omitempty" doc:"true"`
-	QueryOperationConfig *cb_sdk.QueryOperationConfig `json:"operationConfig,omitempty" doc:"true"`
-	Template             template.Template            `json:"-" doc:"false"`
-	Operation            string                       `json:"operation" doc:"false"`
-	ResultSeed           int64                        `json:"resultSeed" doc:"false"`
-	TaskPending          bool                         `json:"taskPending" doc:"false"`
-	BuildIndex           bool                         `json:"buildIndex" doc:"false"`
-	Result               *task_result.TaskResult      `json:"-" doc:"false"`
-	gen                  *docgenerator.QueryGenerator `json:"-" doc:"false"`
-	req                  *tasks.Request               `json:"-" doc:"false"`
-}
-
-func (task *QueryTask) Describe() string {
-	return " Query task runs N1QL query over a period of time over a bucket.\n"
-}
-
-func (task *QueryTask) CollectionIdentifier() string {
-	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
-	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
-		task.Collection}, ":")
-}
-
-func (task *QueryTask) CheckIfPending() bool {
-	return task.TaskPending
-}
-
-func (task *QueryTask) Config(req *tasks.Request, reRun bool) (int64, error) {
-	task.TaskPending = true
-	task.BuildIndex = false
-	task.req = req
-
-	if task.req == nil {
-		task.TaskPending = false
-		return 0, task_errors.ErrRequestIsNil
-	}
-
-	task.req.ReconnectionManager()
-	if _, err := task.req.GetCluster(task.ClusterConfig); err != nil {
-		task.TaskPending = false
-		return 0, err
-	}
-
-	if !reRun {
-		task.ResultSeed = int64(time.Now().UnixNano())
-		task.Operation = tasks.QueryOperation
-		task.BuildIndex = true
-
-		if task.Bucket == "" {
-			task.Bucket = cb_sdk.DefaultBucket
-		}
-		if task.Scope == "" {
-			task.Scope = cb_sdk.DefaultScope
-		}
-		if task.Collection == "" {
-			task.Collection = cb_sdk.DefaultCollection
-		}
-
-		if err := cb_sdk.ConfigQueryOperationConfig(task.QueryOperationConfig); err != nil {
-			task.Result.ErrorOther = err.Error()
-		}
-
-		task.Template = template.InitialiseTemplate(task.QueryOperationConfig.Template)
-
-	} else {
-		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-	}
-	return task.ResultSeed, nil
-}
-
-func (task *QueryTask) TearUp() error {
-	task.Result.StopStoringResult()
-	if err := task.Result.SaveResultIntoFile(); err != nil {
-		log.Println("not able to save Result into ", task.ResultSeed)
-	}
-	task.TaskPending = false
-	return task.req.SaveRequestIntoFile()
-}
-
-func (task *QueryTask) Do() error {
-
-	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
-
-	cluster, err := task.req.GetCluster(task.ClusterConfig)
-	if err != nil {
-		task.Result.ErrorOther = err.Error()
-		return task.TearUp()
-	}
-
-	s, err1 := task.req.GetScope(task.ClusterConfig, task.Bucket, task.Scope)
-	if err1 != nil {
-		task.Result.ErrorOther = err1.Error()
-		return task.TearUp()
-	}
-
-	c, err := task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope, task.Collection)
-	if err != nil {
-		task.Result.ErrorOther = err1.Error()
-		if err := task.Result.SaveResultIntoFile(); err != nil {
-			log.Println("not able to save Result into ", task.ResultSeed)
-			return err
-		}
-		return task.TearUp()
-	}
-
-	task.gen = docgenerator.ConfigQueryGenerator(task.Template)
-	// check if indexes needs to be build
-	if task.BuildIndex {
-		if task.QueryOperationConfig.BuildIndexViaSDK {
-			buildIndexWithSDKs(task, cluster, s, c.Collection)
-		} else {
-			buildIndexViaN1QL(task, cluster, s, c.Collection)
-		}
-		task.BuildIndex = false
-	}
-
-	runN1qlQuery(task, cluster, s, c.Collection)
-
-	return task.TearUp()
-}
-
-// buildIndexSDKManager handles the SDK call
-func buildIndexSDKManager(cluster *gocb.Cluster, bucketName string, scopeName string, collectionName string, indexType string, indexName string, fieldNameList []string) error {
-	manager := cluster.QueryIndexes()
-	if indexType == tasks.CreatePrimaryIndex {
-		if err := manager.CreatePrimaryIndex(bucketName,
-			&gocb.CreatePrimaryQueryIndexOptions{Deferred: true, ScopeName: scopeName, CollectionName: collectionName},
-		); err != nil {
-			return err
-		}
-	} else if indexType == tasks.CreateIndex {
-		if err := manager.CreateIndex(bucketName, indexName, fieldNameList,
-			&gocb.CreateQueryIndexOptions{Deferred: true, ScopeName: scopeName, CollectionName: collectionName},
-		); err != nil {
-			return err
-		}
-	} else if indexType == tasks.BuildIndex {
-		indexesToBuild, err := manager.BuildDeferredIndexes(bucketName,
-			&gocb.BuildDeferredQueryIndexOptions{CollectionName: collectionName, ScopeName: scopeName})
-		if err != nil {
-			return err
-		}
-		err = manager.WatchIndexes(bucketName, indexesToBuild, time.Duration(time.Duration(tasks.WatchIndexDuration)*time.Second), nil)
-		if err != nil {
-			return err
-		}
-	}
-	return nil
-}
-
-// buildIndexWithSDK builds indexes by sending SDK call
-func buildIndexWithSDKs(task *QueryTask, cluster *gocb.Cluster, scope *gocb.Scope, collection *gocb.Collection) {
-
-	if err := buildIndexSDKManager(cluster, task.Bucket, scope.Name(), collection.Name(),
-		tasks.CreatePrimaryIndex, "", []string{""}); err != nil {
-		task.Result.IncrementQueryFailure(fmt.Sprintf("Create primary index on `%s`.%s.%s",
-			task.Bucket, scope.Name(), collection.Name()), err)
-	}
-
-	indexes, err := task.gen.Template.GenerateIndexesForSdk()
-	if err != nil {
-		log.Println("Get indexes for cb_sdk failed.....")
-		return
-	}
-	for indexName, indexFields := range indexes {
-		if err = buildIndexSDKManager(cluster, task.Bucket, scope.Name(), collection.Name(),
-			tasks.CreateIndex, indexName, indexFields); err != nil {
-			task.Result.IncrementQueryFailure(fmt.Sprintf("Create index %s On `%s`.%s.%s (%s)",
-				indexName, task.Bucket, scope.Name(), collection.Name(), indexFields), err)
-		}
-	}
-
-	if err = buildIndexSDKManager(cluster, task.Bucket, scope.Name(), collection.Name(),
-		tasks.BuildIndex, "", []string{""}); err != nil {
-		task.Result.IncrementQueryFailure(fmt.Sprintf("Build index on `%s`.%s.%s",
-			task.Bucket, scope.Name(), collection.Name()), err)
-	}
-}
-
-// buildIndexViaN1QL builds indexes by sending n1ql queries
-func buildIndexViaN1QL(task *QueryTask, cluster *gocb.Cluster, scope *gocb.Scope, collection *gocb.Collection) {
-	query := fmt.Sprintf("CREATE PRIMARY INDEX ON `%s`.`%s`.`%s`;", task.Bucket, scope.Name(), collection.Name())
-	if _, err := cluster.Query(query, &gocb.QueryOptions{}); err != nil {
-		task.Result.IncrementQueryFailure(query, err)
-	}
-
-	indexes, err := task.gen.Template.GenerateIndexes(task.Bucket, scope.Name(), collection.Name())
-	if err != nil {
-		log.Println("Get sample indexes failed.....")
-	}
-
-	for i := 0; i < len(indexes); i++ {
-		if _, err := cluster.Query(indexes[i], &gocb.QueryOptions{}); err != nil {
-			task.Result.IncrementQueryFailure(indexes[i], err)
-		}
-	}
-}
-
-// runN1qlQuery runs query over a duration of time
-func runN1qlQuery(task *QueryTask, cluster *gocb.Cluster, scope *gocb.Scope, collection *gocb.Collection) {
-
-	if task.req.ContextClosed() {
-		return
-	}
-
-	routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
-	group := errgroup.Group{}
-	queries, err := task.gen.Template.GenerateQueries(task.Bucket, scope.Name(), collection.Name())
-	if err != nil {
-		log.Println("Get sample queries failed.....")
-		return
-	}
-
-	expirationTime := time.Now().Add(time.Duration(task.QueryOperationConfig.Duration) * time.Second)
-	for time.Now().Before(expirationTime) {
-		routineLimiter <- struct{}{}
-		group.Go(func() error {
-			for i := 0; i < len(queries); i++ {
-				_, err := cluster.Query(queries[i], &gocb.QueryOptions{})
-				if err != nil {
-					task.Result.IncrementQueryFailure(queries[i], err)
-				}
-			}
-
-			<-routineLimiter
-			return nil
-		})
-
-	}
-
-	_ = group.Wait()
-	close(routineLimiter)
-	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-}
+//
+//import (
+//	"fmt"
+//	"github.com/couchbase/gocb/v2"
+//	"github.com/couchbaselabs/sirius/internal/cb_sdk"
+//	"github.com/couchbaselabs/sirius/internal/docgenerator"
+//	"github.com/couchbaselabs/sirius/internal/err_sirius"
+//	"github.com/couchbaselabs/sirius/internal/task_result"
+//	"github.com/couchbaselabs/sirius/internal/tasks"
+//	"github.com/couchbaselabs/sirius/internal/template"
+//	"golang.org/x/sync/errgroup"
+//	"log"
+//	"strings"
+//	"time"
+//)
+//
+//type QueryTask struct {
+//	IdentifierToken      string                       `json:"identifierToken" doc:"true"`
+//	ClusterConfig        *cb_sdk.ClusterConfig        `json:"clusterConfig" doc:"true"`
+//	Bucket               string                       `json:"bucket" doc:"true"`
+//	Scope                string                       `json:"scope,omitempty" doc:"true"`
+//	Collection           string                       `json:"collection,omitempty" doc:"true"`
+//	QueryOperationConfig *cb_sdk.QueryOperationConfig `json:"operationConfig,omitempty" doc:"true"`
+//	Template             template.Template            `json:"-" doc:"false"`
+//	Operation            string                       `json:"operation" doc:"false"`
+//	ResultSeed           int64                        `json:"resultSeed" doc:"false"`
+//	TaskPending          bool                         `json:"taskPending" doc:"false"`
+//	BuildIndex           bool                         `json:"buildIndex" doc:"false"`
+//	Result               *task_result.TaskResult      `json:"-" doc:"false"`
+//	gen                  *docgenerator.QueryGenerator `json:"-" doc:"false"`
+//	req                  *tasks.Request               `json:"-" doc:"false"`
+//}
+//
+//func (task *QueryTask) Describe() string {
+//	return " Query task runs N1QL query over a period of time over a bucket.\n"
+//}
+//
+//func (task *QueryTask) MetaDataIdentifier() string {
+//	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
+//	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
+//		task.Collection}, ":")
+//}
+//
+//func (task *QueryTask) CheckIfPending() bool {
+//	return task.TaskPending
+//}
+//
+//func (task *QueryTask) Config(req *tasks.Request, reRun bool) (int64, error) {
+//	task.TaskPending = true
+//	task.BuildIndex = false
+//	task.req = req
+//
+//	if task.req == nil {
+//		task.TaskPending = false
+//		return 0, err_sirius.RequestIsNil
+//	}
+//
+//	task.req.ReconnectionManager()
+//	if _, err_sirius := task.req.GetCluster(task.ClusterConfig); err_sirius != nil {
+//		task.TaskPending = false
+//		return 0, err_sirius
+//	}
+//
+//	if !reRun {
+//		task.ResultSeed = int64(time.Now().UnixNano())
+//		task.Operation = tasks.QueryOperation
+//		task.BuildIndex = true
+//
+//		if task.Bucket == "" {
+//			task.Bucket = cb_sdk.DefaultBucket
+//		}
+//		if task.Scope == "" {
+//			task.Scope = cb_sdk.DefaultScope
+//		}
+//		if task.Collection == "" {
+//			task.Collection = cb_sdk.DefaultCollection
+//		}
+//
+//		if err_sirius := cb_sdk.ConfigQueryOperationConfig(task.QueryOperationConfig); err_sirius != nil {
+//			task.Result.ErrorOther = err_sirius.Error()
+//		}
+//
+//		task.Template = template.InitialiseTemplate(task.QueryOperationConfig.Template)
+//
+//	} else {
+//		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//	}
+//	return task.ResultSeed, nil
+//}
+//
+//func (task *QueryTask) TearUp() error {
+//	task.Result.StopStoringResult()
+//	if err_sirius := task.Result.SaveResultIntoFile(); err_sirius != nil {
+//		log.Println("not able to save Result into ", task.ResultSeed)
+//	}
+//	task.TaskPending = false
+//	return task.req.SaveRequestIntoFile()
+//}
+//
+//func (task *QueryTask) Do() error {
+//
+//	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
+//
+//	cluster, err_sirius := task.req.GetCluster(task.ClusterConfig)
+//	if err_sirius != nil {
+//		task.Result.ErrorOther = err_sirius.Error()
+//		return task.TearUp()
+//	}
+//
+//	s, err1 := task.req.GetScope(task.ClusterConfig, task.Bucket, task.Scope)
+//	if err1 != nil {
+//		task.Result.ErrorOther = err1.Error()
+//		return task.TearUp()
+//	}
+//
+//	c, err_sirius := task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope, task.Collection)
+//	if err_sirius != nil {
+//		task.Result.ErrorOther = err1.Error()
+//		if err_sirius := task.Result.SaveResultIntoFile(); err_sirius != nil {
+//			log.Println("not able to save Result into ", task.ResultSeed)
+//			return err_sirius
+//		}
+//		return task.TearUp()
+//	}
+//
+//	task.gen = docgenerator.ConfigQueryGenerator(task.Template)
+//	// check if indexes needs to be build
+//	if task.BuildIndex {
+//		if task.QueryOperationConfig.BuildIndexViaSDK {
+//			buildIndexWithSDKs(task, cluster, s, c.Collection)
+//		} else {
+//			buildIndexViaN1QL(task, cluster, s, c.Collection)
+//		}
+//		task.BuildIndex = false
+//	}
+//
+//	runN1qlQuery(task, cluster, s, c.Collection)
+//
+//	return task.TearUp()
+//}
+//
+//// buildIndexSDKManager handles the SDK call
+//func buildIndexSDKManager(cluster *gocb.Cluster, bucketName string, scopeName string, collectionName string, indexType string, indexName string, fieldNameList []string) error {
+//	manager := cluster.QueryIndexes()
+//	if indexType == tasks.CreatePrimaryIndex {
+//		if err_sirius := manager.CreatePrimaryIndex(bucketName,
+//			&gocb.CreatePrimaryQueryIndexOptions{Deferred: true, ScopeName: scopeName, CollectionName: collectionName},
+//		); err_sirius != nil {
+//			return err_sirius
+//		}
+//	} else if indexType == tasks.CreateIndex {
+//		if err_sirius := manager.CreateIndex(bucketName, indexName, fieldNameList,
+//			&gocb.CreateQueryIndexOptions{Deferred: true, ScopeName: scopeName, CollectionName: collectionName},
+//		); err_sirius != nil {
+//			return err_sirius
+//		}
+//	} else if indexType == tasks.BuildIndex {
+//		indexesToBuild, err_sirius := manager.BuildDeferredIndexes(bucketName,
+//			&gocb.BuildDeferredQueryIndexOptions{CollectionName: collectionName, ScopeName: scopeName})
+//		if err_sirius != nil {
+//			return err_sirius
+//		}
+//		err_sirius = manager.WatchIndexes(bucketName, indexesToBuild, time.Duration(time.Duration(tasks.WatchIndexDuration)*time.Second), nil)
+//		if err_sirius != nil {
+//			return err_sirius
+//		}
+//	}
+//	return nil
+//}
+//
+//// buildIndexWithSDK builds indexes by sending SDK call
+//func buildIndexWithSDKs(task *QueryTask, cluster *gocb.Cluster, scope *gocb.Scope, collection *gocb.Collection) {
+//
+//	if err_sirius := buildIndexSDKManager(cluster, task.Bucket, scope.Name(), collection.Name(),
+//		tasks.CreatePrimaryIndex, "", []string{""}); err_sirius != nil {
+//		task.Result.IncrementQueryFailure(fmt.Sprintf("Create primary index on `%s`.%s.%s",
+//			task.Bucket, scope.Name(), collection.Name()), err_sirius)
+//	}
+//
+//	indexes, err_sirius := task.gen.Template.GenerateIndexesForSdk()
+//	if err_sirius != nil {
+//		log.Println("Get indexes for cb_sdk failed.....")
+//		return
+//	}
+//	for indexName, indexFields := range indexes {
+//		if err_sirius = buildIndexSDKManager(cluster, task.Bucket, scope.Name(), collection.Name(),
+//			tasks.CreateIndex, indexName, indexFields); err_sirius != nil {
+//			task.Result.IncrementQueryFailure(fmt.Sprintf("Create index %s On `%s`.%s.%s (%s)",
+//				indexName, task.Bucket, scope.Name(), collection.Name(), indexFields), err_sirius)
+//		}
+//	}
+//
+//	if err_sirius = buildIndexSDKManager(cluster, task.Bucket, scope.Name(), collection.Name(),
+//		tasks.BuildIndex, "", []string{""}); err_sirius != nil {
+//		task.Result.IncrementQueryFailure(fmt.Sprintf("Build index on `%s`.%s.%s",
+//			task.Bucket, scope.Name(), collection.Name()), err_sirius)
+//	}
+//}
+//
+//// buildIndexViaN1QL builds indexes by sending n1ql queries
+//func buildIndexViaN1QL(task *QueryTask, cluster *gocb.Cluster, scope *gocb.Scope, collection *gocb.Collection) {
+//	query := fmt.Sprintf("CREATE PRIMARY INDEX ON `%s`.`%s`.`%s`;", task.Bucket, scope.Name(), collection.Name())
+//	if _, err_sirius := cluster.Query(query, &gocb.QueryOptions{}); err_sirius != nil {
+//		task.Result.IncrementQueryFailure(query, err_sirius)
+//	}
+//
+//	indexes, err_sirius := task.gen.Template.GenerateIndexes(task.Bucket, scope.Name(), collection.Name())
+//	if err_sirius != nil {
+//		log.Println("Get sample indexes failed.....")
+//	}
+//
+//	for i := 0; i < len(indexes); i++ {
+//		if _, err_sirius := cluster.Query(indexes[i], &gocb.QueryOptions{}); err_sirius != nil {
+//			task.Result.IncrementQueryFailure(indexes[i], err_sirius)
+//		}
+//	}
+//}
+//
+//// runN1qlQuery runs query over a duration of time
+//func runN1qlQuery(task *QueryTask, cluster *gocb.Cluster, scope *gocb.Scope, collection *gocb.Collection) {
+//
+//	if task.req.ContextClosed() {
+//		return
+//	}
+//
+//	routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
+//	group := errgroup.Group{}
+//	queries, err_sirius := task.gen.Template.GenerateQueries(task.Bucket, scope.Name(), collection.Name())
+//	if err_sirius != nil {
+//		log.Println("Get sample queries failed.....")
+//		return
+//	}
+//
+//	expirationTime := time.Now().Add(time.Duration(task.QueryOperationConfig.Duration) * time.Second)
+//	for time.Now().Before(expirationTime) {
+//		routineLimiter <- struct{}{}
+//		group.Go(func() error {
+//			for i := 0; i < len(queries); i++ {
+//				_, err_sirius := cluster.Query(queries[i], &gocb.QueryOptions{})
+//				if err_sirius != nil {
+//					task.Result.IncrementQueryFailure(queries[i], err_sirius)
+//				}
+//			}
+//
+//			<-routineLimiter
+//			return nil
+//		})
+//
+//	}
+//
+//	_ = group.Wait()
+//	close(routineLimiter)
+//	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//}
diff --git a/internal/tasks/db_util/warmup_database.go b/internal/tasks/db_util/warmup_database.go
new file mode 100644
index 0000000..80db404
--- /dev/null
+++ b/internal/tasks/db_util/warmup_database.go
@@ -0,0 +1,73 @@
+package db_util
+
+import (
+	"github.com/couchbaselabs/sirius/internal/db"
+	"github.com/couchbaselabs/sirius/internal/err_sirius"
+	"github.com/couchbaselabs/sirius/internal/task_result"
+	"github.com/couchbaselabs/sirius/internal/tasks"
+	"log"
+	"time"
+)
+
+type BucketWarmUpTask struct {
+	IdentifierToken string `json:"identifierToken" doc:"true"`
+	tasks.DatabaseInformation
+	Result      *task_result.TaskResult `json:"-" doc:"false"`
+	Operation   string                  `json:"operation" doc:"false"`
+	ResultSeed  int64                   `json:"resultSeed" doc:"false"`
+	req         *tasks.Request          `json:"-" doc:"false"`
+	TaskPending bool                    `json:"taskPending" doc:"false"`
+}
+
+func (t *BucketWarmUpTask) Describe() string {
+	return "This API aids in warming up a Couchbase bucket or establishing connections to KV services."
+}
+
+func (t *BucketWarmUpTask) Do() {
+	t.Result = task_result.ConfigTaskResult(t.Operation, t.ResultSeed)
+
+	database, err := db.ConfigDatabase(t.DBType)
+
+	if err != nil {
+		t.Result.ErrorOther = err.Error()
+		_ = t.TearUp()
+	}
+	if err = database.Connect(t.ConnStr, t.Username, t.Password, t.Extra); err != nil {
+		t.Result.ErrorOther = err.Error()
+		_ = t.TearUp()
+	}
+
+	if err = database.Warmup(t.ConnStr, t.Username, t.Password, t.Extra); err != nil {
+		t.Result.ErrorOther = err.Error()
+		_ = t.TearUp()
+	}
+
+	_ = t.TearUp()
+}
+
+func (t *BucketWarmUpTask) Config(req *tasks.Request, reRun bool) (int64, error) {
+	t.TaskPending = false
+	t.req = req
+
+	if t.req == nil {
+		return 0, err_sirius.RequestIsNil
+	}
+
+	t.ResultSeed = int64(time.Now().UnixNano())
+	t.Operation = tasks.BucketWarmUpOperation
+
+	return t.ResultSeed, nil
+}
+
+func (t *BucketWarmUpTask) CheckIfPending() bool {
+	return t.TaskPending
+}
+
+func (t *BucketWarmUpTask) TearUp() error {
+	t.Result.StopStoringResult()
+	if err := t.Result.SaveResultIntoFile(); err != nil {
+		log.Println("not able to save Result into ", t.ResultSeed, t.Operation)
+	}
+	t.TaskPending = false
+	return nil
+}
diff --git a/internal/tasks/helper.go b/internal/tasks/helper.go
index 2a877aa..72f5924 100644
--- a/internal/tasks/helper.go
+++ b/internal/tasks/helper.go
@@ -5,7 +5,6 @@ import (
 )
 
 const (
-	MaxConcurrentRoutines               = 128
 	DefaultIdentifierToken              = "default"
 	WatchIndexDuration           int    = 120
 	InsertOperation              string = "insert"
diff --git a/internal/tasks/key_based_loading_cb/helper.go b/internal/tasks/key_based_loading_cb/helper.go
index df20911..680b3bc 100644
--- a/internal/tasks/key_based_loading_cb/helper.go
+++ b/internal/tasks/key_based_loading_cb/helper.go
@@ -1,6 +1,6 @@
 package key_based_loading_cb
 
-import "github.com/couchbaselabs/sirius/internal/task_errors"
+import "github.com/couchbaselabs/sirius/internal/err_sirius"
 
 type SingleOperationConfig struct {
 	Keys     []string `json:"keys" doc:"true"`
@@ -10,7 +10,7 @@ type SingleOperationConfig struct {
 
 func ConfigSingleOperationConfig(s *SingleOperationConfig) error {
 	if s == nil {
-		return task_errors.ErrParsingSingleOperationConfig
+		return err_sirius.ParsingSingleOperationConfig
 	}
 	return nil
 }
@@ -23,7 +23,7 @@ type SingleSubDocOperationConfig struct {
 
 func ConfigSingleSubDocOperationConfig(s *SingleSubDocOperationConfig) error {
 	if s == nil {
-		return task_errors.ErrParsingSingleSubDocOperationConfig
+		return err_sirius.ParsingSingleSubDocOperationConfig
 	}
 	return nil
 }
diff --git a/internal/tasks/key_based_loading_cb/task_single_create.go b/internal/tasks/key_based_loading_cb/task_single_create.go
index ad5c4d8..2317236 100644
--- a/internal/tasks/key_based_loading_cb/task_single_create.go
+++ b/internal/tasks/key_based_loading_cb/task_single_create.go
@@ -1,186 +1,187 @@
 package key_based_loading_cb
 
-import (
-	"github.com/couchbase/gocb/v2"
-	"github.com/couchbaselabs/sirius/internal/cb_sdk"
-	"github.com/couchbaselabs/sirius/internal/task_errors"
-	"github.com/couchbaselabs/sirius/internal/task_result"
-	"github.com/couchbaselabs/sirius/internal/tasks"
-	"github.com/couchbaselabs/sirius/internal/template"
-	"github.com/jaswdr/faker"
-	"golang.org/x/sync/errgroup"
-	"log"
-	"math/rand"
-	"strings"
-	"time"
-)
-
-type SingleInsertTask struct {
-	IdentifierToken       string                  `json:"identifierToken" doc:"true"`
-	ClusterConfig         *cb_sdk.ClusterConfig   `json:"clusterConfig" doc:"true"`
-	Bucket                string                  `json:"bucket" doc:"true"`
-	Scope                 string                  `json:"scope,omitempty" doc:"true"`
-	Collection            string                  `json:"collection,omitempty" doc:"true"`
-	InsertOptions         *cb_sdk.InsertOptions   `json:"insertOptions,omitempty" doc:"true"`
-	SingleOperationConfig *SingleOperationConfig  `json:"singleOperationConfig" doc:"true"`
-	Operation             string                  `json:"operation" doc:"false"`
-	ResultSeed            int64                   `json:"resultSeed" doc:"false"`
-	TaskPending           bool                    `json:"taskPending" doc:"false"`
-	Result                *task_result.TaskResult `json:"Result" doc:"false"`
-	req                   *tasks.Request          `json:"-" doc:"false"`
-}
-
-func (task *SingleInsertTask) Describe() string {
-	return "Single insert task create key value in Couchbase.\n"
-}
-
-func (task *SingleInsertTask) CollectionIdentifier() string {
-	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
-	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
-		task.Collection}, ":")
-}
-
-func (task *SingleInsertTask) CheckIfPending() bool {
-	return task.TaskPending
-}
-
-// Config configures  the insert task
-func (task *SingleInsertTask) Config(req *tasks.Request, reRun bool) (int64, error) {
-	task.TaskPending = true
-	task.req = req
-
-	if task.req == nil {
-		task.TaskPending = false
-		return 0, task_errors.ErrRequestIsNil
-	}
-
-	task.req.ReconnectionManager()
-	if _, err := task.req.GetCluster(task.ClusterConfig); err != nil {
-		task.TaskPending = false
-		return 0, err
-	}
-
-	task.req.ReconfigureDocumentManager()
-
-	if !reRun {
-		task.ResultSeed = int64(time.Now().UnixNano())
-		task.Operation = tasks.SingleInsertOperation
-
-		if task.Bucket == "" {
-			task.Bucket = cb_sdk.DefaultBucket
-		}
-		if task.Scope == "" {
-			task.Scope = cb_sdk.DefaultScope
-		}
-		if task.Collection == "" {
-			task.Collection = cb_sdk.DefaultCollection
-		}
-
-		if err := cb_sdk.ConfigInsertOptions(task.InsertOptions); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-
-		if err := ConfigSingleOperationConfig(task.SingleOperationConfig); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-
-	} else {
-		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-	}
-	return task.ResultSeed, nil
-}
-
-func (task *SingleInsertTask) TearUp() error {
-	task.Result.StopStoringResult()
-	if err := task.Result.SaveResultIntoFile(); err != nil {
-		log.Println("not able to save Result into ", task.ResultSeed)
-	}
-	task.Result = nil
-	task.TaskPending = false
-	return task.req.SaveRequestIntoFile()
-}
-
-func (task *SingleInsertTask) Do() error {
-
-	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
-
-	collectionObject, err1 := task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope,
-		task.Collection)
-
-	if err1 != nil {
-		task.Result.ErrorOther = err1.Error()
-		task.Result.FailWholeSingleOperation(task.SingleOperationConfig.Keys, err1)
-		return task.TearUp()
-	}
-
-	singleInsertDocuments(task, collectionObject)
-
-	task.Result.Success = int64(len(task.SingleOperationConfig.Keys)) - task.Result.Failure
-	return task.TearUp()
-}
-
-// singleInsertDocuments uploads new documents in a bucket.scope.collection in a defined batch size at multiple iterations.
-func singleInsertDocuments(task *SingleInsertTask, collectionObject *cb_sdk.CollectionObject) {
-
-	if task.req.ContextClosed() {
-		return
-	}
-
-	routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
-	dataChannel := make(chan string, tasks.MaxConcurrentRoutines)
-
-	group := errgroup.Group{}
-
-	for _, data := range task.SingleOperationConfig.Keys {
-
-		if task.req.ContextClosed() {
-			close(routineLimiter)
-			close(dataChannel)
-			return
-		}
-
-		routineLimiter <- struct{}{}
-		dataChannel <- data
-
-		group.Go(func() error {
-			key := <-dataChannel
-
-			documentMetaData := task.req.DocumentsMeta.GetDocumentsMetadata(task.CollectionIdentifier(), key,
-				task.SingleOperationConfig.Template,
-				task.SingleOperationConfig.DocSize, false)
-
-			fake := faker.NewWithSeed(rand.NewSource(int64(documentMetaData.Seed)))
-
-			t := template.InitialiseTemplate(documentMetaData.Template)
-
-			doc, _ := t.GenerateDocument(&fake, documentMetaData.DocSize)
-
-			initTime := time.Now().UTC().Format(time.RFC850)
-			m, err := collectionObject.Collection.Insert(key, doc, &gocb.InsertOptions{
-				DurabilityLevel: cb_sdk.GetDurability(task.InsertOptions.Durability),
-				PersistTo:       task.InsertOptions.PersistTo,
-				ReplicateTo:     task.InsertOptions.ReplicateTo,
-				Timeout:         time.Duration(task.InsertOptions.Timeout) * time.Second,
-				Expiry:          time.Duration(task.InsertOptions.Expiry) * time.Second,
-			})
-
-			if err != nil {
-				task.Result.CreateSingleErrorResult(initTime, key, err.Error(), false, 0)
-				<-routineLimiter
-				return err
-
-			}
-
-			task.Result.CreateSingleErrorResult(initTime, key, "", true, uint64(m.Cas()))
-			<-routineLimiter
-			return nil
-		})
-	}
-
-	_ = group.Wait()
-	close(routineLimiter)
-	close(dataChannel)
-	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-}
+//
+//import (
+//	"github.com/couchbase/gocb/v2"
+//	"github.com/couchbaselabs/sirius/internal/cb_sdk"
+//	"github.com/couchbaselabs/sirius/internal/err_sirius"
+//	"github.com/couchbaselabs/sirius/internal/task_result"
+//	"github.com/couchbaselabs/sirius/internal/tasks"
+//	"github.com/couchbaselabs/sirius/internal/template"
+//	"github.com/jaswdr/faker"
+//	"golang.org/x/sync/errgroup"
+//	"log"
+//	"math/rand"
+//	"strings"
+//	"time"
+//)
+//
+//type SingleInsertTask struct {
+//	IdentifierToken       string                  `json:"identifierToken" doc:"true"`
+//	ClusterConfig         *cb_sdk.ClusterConfig   `json:"clusterConfig" doc:"true"`
+//	Bucket                string                  `json:"bucket" doc:"true"`
+//	Scope                 string                  `json:"scope,omitempty" doc:"true"`
+//	Collection            string                  `json:"collection,omitempty" doc:"true"`
+//	InsertOptions         *cb_sdk.InsertOptions   `json:"insertOptions,omitempty" doc:"true"`
+//	SingleOperationConfig *SingleOperationConfig  `json:"singleOperationConfig" doc:"true"`
+//	Operation             string                  `json:"operation" doc:"false"`
+//	ResultSeed            int64                   `json:"resultSeed" doc:"false"`
+//	TaskPending           bool                    `json:"taskPending" doc:"false"`
+//	Result                *task_result.TaskResult `json:"Result" doc:"false"`
+//	req                   *tasks.Request          `json:"-" doc:"false"`
+//}
+//
+//func (task *SingleInsertTask) Describe() string {
+//	return "Single insert task create key value in Couchbase.\n"
+//}
+//
+//func (task *SingleInsertTask) MetaDataIdentifier() string {
+//	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
+//	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
+//		task.Collection}, ":")
+//}
+//
+//func (task *SingleInsertTask) CheckIfPending() bool {
+//	return task.TaskPending
+//}
+//
+//// Config configures  the insert task
+//func (task *SingleInsertTask) Config(req *tasks.Request, reRun bool) (int64, error) {
+//	task.TaskPending = true
+//	task.req = req
+//
+//	if task.req == nil {
+//		task.TaskPending = false
+//		return 0, err_sirius.RequestIsNil
+//	}
+//
+//	task.req.ReconnectionManager()
+//	if _, err_sirius := task.req.GetCluster(task.ClusterConfig); err_sirius != nil {
+//		task.TaskPending = false
+//		return 0, err_sirius
+//	}
+//
+//	task.req.ReconfigureDocumentManager()
+//
+//	if !reRun {
+//		task.ResultSeed = int64(time.Now().UnixNano())
+//		task.Operation = tasks.SingleInsertOperation
+//
+//		if task.Bucket == "" {
+//			task.Bucket = cb_sdk.DefaultBucket
+//		}
+//		if task.Scope == "" {
+//			task.Scope = cb_sdk.DefaultScope
+//		}
+//		if task.Collection == "" {
+//			task.Collection = cb_sdk.DefaultCollection
+//		}
+//
+//		if err_sirius := cb_sdk.ConfigInsertOptions(task.InsertOptions); err_sirius != nil {
+//			task.TaskPending = false
+//			return 0, err_sirius
+//		}
+//
+//		if err_sirius := ConfigSingleOperationConfig(task.SingleOperationConfig); err_sirius != nil {
+//			task.TaskPending = false
+//			return 0, err_sirius
+//		}
+//
+//	} else {
+//		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//	}
+//	return task.ResultSeed, nil
+//}
+//
+//func (task *SingleInsertTask) TearUp() error {
+//	task.Result.StopStoringResult()
+//	if err_sirius := task.Result.SaveResultIntoFile(); err_sirius != nil {
+//		log.Println("not able to save Result into ", task.ResultSeed)
+//	}
+//	task.Result = nil
+//	task.TaskPending = false
+//	return task.req.SaveRequestIntoFile()
+//}
+//
+//func (task *SingleInsertTask) Do() error {
+//
+//	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
+//
+//	collectionObject, err1 := task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope,
+//		task.Collection)
+//
+//	if err1 != nil {
+//		task.Result.ErrorOther = err1.Error()
+//		task.Result.FailWholeSingleOperation(task.SingleOperationConfig.Keys, err1)
+//		return task.TearUp()
+//	}
+//
+//	singleInsertDocuments(task, collectionObject)
+//
+//	task.Result.Success = int64(len(task.SingleOperationConfig.Keys)) - task.Result.Failure
+//	return task.TearUp()
+//}
+//
+//// singleInsertDocuments uploads new documents in a bucket.scope.collection in a defined batch size at multiple iterations.
+//func singleInsertDocuments(task *SingleInsertTask, collectionObject *cb_sdk.CollectionObject) {
+//
+//	if task.req.ContextClosed() {
+//		return
+//	}
+//
+//	routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
+//	dataChannel := make(chan string, tasks.MaxConcurrentRoutines)
+//
+//	group := errgroup.Group{}
+//
+//	for _, data := range task.SingleOperationConfig.Keys {
+//
+//		if task.req.ContextClosed() {
+//			close(routineLimiter)
+//			close(dataChannel)
+//			return
+//		}
+//
+//		routineLimiter <- struct{}{}
+//		dataChannel <- data
+//
+//		group.Go(func() error {
+//			key := <-dataChannel
+//
+//			documentMetaData := task.req.DocumentsMeta.GetDocumentsMetadata(task.MetaDataIdentifier(), key,
+//				task.SingleOperationConfig.Template,
+//				task.SingleOperationConfig.DocSize, false)
+//
+//			fake := faker.NewWithSeed(rand.NewSource(int64(documentMetaData.Seed)))
+//
+//			t := template.InitialiseTemplate(documentMetaData.Template)
+//
+//			doc, _ := t.GenerateDocument(&fake, documentMetaData.DocSize)
+//
+//			initTime := time.Now().UTC().Format(time.RFC850)
+//			m, err_sirius := collectionObject.Collection.Insert(key, doc, &gocb.InsertOptions{
+//				DurabilityLevel: cb_sdk.GetDurability(task.InsertOptions.Durability),
+//				PersistTo:       task.InsertOptions.PersistTo,
+//				ReplicateTo:     task.InsertOptions.ReplicateTo,
+//				Timeout:         time.Duration(task.InsertOptions.Timeout) * time.Second,
+//				Expiry:          time.Duration(task.InsertOptions.Expiry) * time.Second,
+//			})
+//
+//			if err_sirius != nil {
+//				task.Result.CreateSingleErrorResult(initTime, key, err_sirius.Error(), false, 0)
+//				<-routineLimiter
+//				return err_sirius
+//
+//			}
+//
+//			task.Result.CreateSingleErrorResult(initTime, key, "", true, uint64(m.Cas()))
+//			<-routineLimiter
+//			return nil
+//		})
+//	}
+//
+//	_ = group.Wait()
+//	close(routineLimiter)
+//	close(dataChannel)
+//	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//}
diff --git a/internal/tasks/key_based_loading_cb/task_single_delete.go b/internal/tasks/key_based_loading_cb/task_single_delete.go
index 1877434..3ff665a 100644
--- a/internal/tasks/key_based_loading_cb/task_single_delete.go
+++ b/internal/tasks/key_based_loading_cb/task_single_delete.go
@@ -1,172 +1,173 @@
 package key_based_loading_cb
 
-import (
-	"github.com/couchbase/gocb/v2"
-	"github.com/couchbaselabs/sirius/internal/cb_sdk"
-	"github.com/couchbaselabs/sirius/internal/task_errors"
-	"github.com/couchbaselabs/sirius/internal/task_result"
-	"github.com/couchbaselabs/sirius/internal/tasks"
-	"golang.org/x/sync/errgroup"
-	"log"
-	"strings"
-	"time"
-)
-
-type SingleDeleteTask struct {
-	IdentifierToken       string                  `json:"identifierToken" doc:"true"`
-	ClusterConfig         *cb_sdk.ClusterConfig   `json:"clusterConfig" doc:"true"`
-	Bucket                string                  `json:"bucket" doc:"true"`
-	Scope                 string                  `json:"scope,omitempty" doc:"true"`
-	Collection            string                  `json:"collection,omitempty" doc:"true"`
-	RemoveOptions         *cb_sdk.RemoveOptions   `json:"removeOptions,omitempty" doc:"true"`
-	SingleOperationConfig *SingleOperationConfig  `json:"singleOperationConfig" doc:"true"`
-	Operation             string                  `json:"operation" doc:"false"`
-	ResultSeed            int64                   `json:"resultSeed" doc:"false"`
-	TaskPending           bool                    `json:"taskPending" doc:"false"`
-	Result                *task_result.TaskResult `json:"-" doc:"false"`
-	req                   *tasks.Request          `json:"-" doc:"false"`
-}
-
-func (task *SingleDeleteTask) Describe() string {
-	return "Single delete task deletes key in Couchbase.\n"
-}
-
-func (task *SingleDeleteTask) CollectionIdentifier() string {
-	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
-	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
-		task.Collection}, ":")
-}
-
-func (task *SingleDeleteTask) CheckIfPending() bool {
-	return task.TaskPending
-}
-
-// Config configures  the delete task
-func (task *SingleDeleteTask) Config(req *tasks.Request, reRun bool) (int64, error) {
-	task.TaskPending = true
-	task.req = req
-
-	if task.req == nil {
-		task.TaskPending = false
-		return 0, task_errors.ErrRequestIsNil
-	}
-
-	task.req.ReconnectionManager()
-	if _, err := task.req.GetCluster(task.ClusterConfig); err != nil {
-		task.TaskPending = false
-		return 0, err
-	}
-
-	task.req.ReconfigureDocumentManager()
-
-	if !reRun {
-		task.ResultSeed = int64(time.Now().UnixNano())
-		task.Operation = tasks.SingleDeleteOperation
-
-		if task.Bucket == "" {
-			task.Bucket = cb_sdk.DefaultBucket
-		}
-		if task.Scope == "" {
-			task.Scope = cb_sdk.DefaultScope
-		}
-		if task.Collection == "" {
-			task.Collection = cb_sdk.DefaultCollection
-		}
-
-		if err := cb_sdk.ConfigRemoveOptions(task.RemoveOptions); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-
-		if err := ConfigSingleOperationConfig(task.SingleOperationConfig); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-	} else {
-		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-	}
-	return task.ResultSeed, nil
-}
-
-func (task *SingleDeleteTask) TearUp() error {
-	task.Result.StopStoringResult()
-	if err := task.Result.SaveResultIntoFile(); err != nil {
-		log.Println("not able to save Result into ", task.ResultSeed)
-	}
-	task.Result = nil
-	task.TaskPending = false
-	return task.req.SaveRequestIntoFile()
-}
-
-func (task *SingleDeleteTask) Do() error {
-
-	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
-
-	collectionObject, err1 := task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope,
-		task.Collection)
-
-	if err1 != nil {
-		task.Result.ErrorOther = err1.Error()
-		task.Result.FailWholeSingleOperation(task.SingleOperationConfig.Keys, err1)
-		return task.TearUp()
-	}
-
-	singleDeleteDocuments(task, collectionObject)
-
-	task.Result.Success = int64(len(task.SingleOperationConfig.Keys)) - task.Result.Failure
-	return task.TearUp()
-}
-
-// singleDeleteDocuments uploads new documents in a bucket.scope.collection in a defined batch size at multiple iterations.
-func singleDeleteDocuments(task *SingleDeleteTask, collectionObject *cb_sdk.CollectionObject) {
-
-	if task.req.ContextClosed() {
-		return
-	}
-
-	routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
-	dataChannel := make(chan string, tasks.MaxConcurrentRoutines)
-
-	group := errgroup.Group{}
-
-	for _, data := range task.SingleOperationConfig.Keys {
-
-		if task.req.ContextClosed() {
-			close(routineLimiter)
-			close(dataChannel)
-			return
-		}
-
-		routineLimiter <- struct{}{}
-		dataChannel <- data
-
-		group.Go(func() error {
-			key := <-dataChannel
-
-			task.req.DocumentsMeta.RemoveDocument(task.CollectionIdentifier(), key)
-
-			initTime := time.Now().UTC().Format(time.RFC850)
-			r, err := collectionObject.Collection.Remove(key, &gocb.RemoveOptions{
-				Cas:             gocb.Cas(task.RemoveOptions.Cas),
-				PersistTo:       task.RemoveOptions.PersistTo,
-				ReplicateTo:     task.RemoveOptions.ReplicateTo,
-				DurabilityLevel: cb_sdk.GetDurability(task.RemoveOptions.Durability),
-				Timeout:         time.Duration(task.RemoveOptions.Timeout) * time.Second,
-			})
-			if err != nil {
-				task.Result.CreateSingleErrorResult(initTime, key, err.Error(), false, 0)
-				<-routineLimiter
-				return err
-			}
-
-			task.Result.CreateSingleErrorResult(initTime, key, "", true, uint64(r.Cas()))
-			<-routineLimiter
-			return nil
-		})
-	}
-
-	_ = group.Wait()
-	close(routineLimiter)
-	close(dataChannel)
-	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-}
+//
+//import (
+//	"github.com/couchbase/gocb/v2"
+//	"github.com/couchbaselabs/sirius/internal/cb_sdk"
+//	"github.com/couchbaselabs/sirius/internal/err_sirius"
+//	"github.com/couchbaselabs/sirius/internal/task_result"
+//	"github.com/couchbaselabs/sirius/internal/tasks"
+//	"golang.org/x/sync/errgroup"
+//	"log"
+//	"strings"
+//	"time"
+//)
+//
+//type SingleDeleteTask struct {
+//	IdentifierToken       string                  `json:"identifierToken" doc:"true"`
+//	ClusterConfig         *cb_sdk.ClusterConfig   `json:"clusterConfig" doc:"true"`
+//	Bucket                string                  `json:"bucket" doc:"true"`
+//	Scope                 string                  `json:"scope,omitempty" doc:"true"`
+//	Collection            string                  `json:"collection,omitempty" doc:"true"`
+//	RemoveOptions         *cb_sdk.RemoveOptions   `json:"removeOptions,omitempty" doc:"true"`
+//	SingleOperationConfig *SingleOperationConfig  `json:"singleOperationConfig" doc:"true"`
+//	Operation             string                  `json:"operation" doc:"false"`
+//	ResultSeed            int64                   `json:"resultSeed" doc:"false"`
+//	TaskPending           bool                    `json:"taskPending" doc:"false"`
+//	Result                *task_result.TaskResult `json:"-" doc:"false"`
+//	req                   *tasks.Request          `json:"-" doc:"false"`
+//}
+//
+//func (task *SingleDeleteTask) Describe() string {
+//	return "Single delete task deletes key in Couchbase.\n"
+//}
+//
+//func (task *SingleDeleteTask) MetaDataIdentifier() string {
+//	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
+//	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
+//		task.Collection}, ":")
+//}
+//
+//func (task *SingleDeleteTask) CheckIfPending() bool {
+//	return task.TaskPending
+//}
+//
+//// Config configures  the delete task
+//func (task *SingleDeleteTask) Config(req *tasks.Request, reRun bool) (int64, error) {
+//	task.TaskPending = true
+//	task.req = req
+//
+//	if task.req == nil {
+//		task.TaskPending = false
+//		return 0, err_sirius.RequestIsNil
+//	}
+//
+//	task.req.ReconnectionManager()
+//	if _, err_sirius := task.req.GetCluster(task.ClusterConfig); err_sirius != nil {
+//		task.TaskPending = false
+//		return 0, err_sirius
+//	}
+//
+//	task.req.ReconfigureDocumentManager()
+//
+//	if !reRun {
+//		task.ResultSeed = int64(time.Now().UnixNano())
+//		task.Operation = tasks.SingleDeleteOperation
+//
+//		if task.Bucket == "" {
+//			task.Bucket = cb_sdk.DefaultBucket
+//		}
+//		if task.Scope == "" {
+//			task.Scope = cb_sdk.DefaultScope
+//		}
+//		if task.Collection == "" {
+//			task.Collection = cb_sdk.DefaultCollection
+//		}
+//
+//		if err_sirius := cb_sdk.ConfigRemoveOptions(task.RemoveOptions); err_sirius != nil {
+//			task.TaskPending = false
+//			return 0, err_sirius
+//		}
+//
+//		if err_sirius := ConfigSingleOperationConfig(task.SingleOperationConfig); err_sirius != nil {
+//			task.TaskPending = false
+//			return 0, err_sirius
+//		}
+//	} else {
+//		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//	}
+//	return task.ResultSeed, nil
+//}
+//
+//func (task *SingleDeleteTask) TearUp() error {
+//	task.Result.StopStoringResult()
+//	if err_sirius := task.Result.SaveResultIntoFile(); err_sirius != nil {
+//		log.Println("not able to save Result into ", task.ResultSeed)
+//	}
+//	task.Result = nil
+//	task.TaskPending = false
+//	return task.req.SaveRequestIntoFile()
+//}
+//
+//func (task *SingleDeleteTask) Do() error {
+//
+//	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
+//
+//	collectionObject, err1 := task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope,
+//		task.Collection)
+//
+//	if err1 != nil {
+//		task.Result.ErrorOther = err1.Error()
+//		task.Result.FailWholeSingleOperation(task.SingleOperationConfig.Keys, err1)
+//		return task.TearUp()
+//	}
+//
+//	singleDeleteDocuments(task, collectionObject)
+//
+//	task.Result.Success = int64(len(task.SingleOperationConfig.Keys)) - task.Result.Failure
+//	return task.TearUp()
+//}
+//
+//// singleDeleteDocuments uploads new documents in a bucket.scope.collection in a defined batch size at multiple iterations.
+//func singleDeleteDocuments(task *SingleDeleteTask, collectionObject *cb_sdk.CollectionObject) {
+//
+//	if task.req.ContextClosed() {
+//		return
+//	}
+//
+//	routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
+//	dataChannel := make(chan string, tasks.MaxConcurrentRoutines)
+//
+//	group := errgroup.Group{}
+//
+//	for _, data := range task.SingleOperationConfig.Keys {
+//
+//		if task.req.ContextClosed() {
+//			close(routineLimiter)
+//			close(dataChannel)
+//			return
+//		}
+//
+//		routineLimiter <- struct{}{}
+//		dataChannel <- data
+//
+//		group.Go(func() error {
+//			key := <-dataChannel
+//
+//			task.req.DocumentsMeta.RemoveDocument(task.MetaDataIdentifier(), key)
+//
+//			initTime := time.Now().UTC().Format(time.RFC850)
+//			r, err_sirius := collectionObject.Collection.Remove(key, &gocb.RemoveOptions{
+//				Cas:             gocb.Cas(task.RemoveOptions.Cas),
+//				PersistTo:       task.RemoveOptions.PersistTo,
+//				ReplicateTo:     task.RemoveOptions.ReplicateTo,
+//				DurabilityLevel: cb_sdk.GetDurability(task.RemoveOptions.Durability),
+//				Timeout:         time.Duration(task.RemoveOptions.Timeout) * time.Second,
+//			})
+//			if err_sirius != nil {
+//				task.Result.CreateSingleErrorResult(initTime, key, err_sirius.Error(), false, 0)
+//				<-routineLimiter
+//				return err_sirius
+//			}
+//
+//			task.Result.CreateSingleErrorResult(initTime, key, "", true, uint64(r.Cas()))
+//			<-routineLimiter
+//			return nil
+//		})
+//	}
+//
+//	_ = group.Wait()
+//	close(routineLimiter)
+//	close(dataChannel)
+//	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//}
diff --git a/internal/tasks/key_based_loading_cb/task_single_read.go b/internal/tasks/key_based_loading_cb/task_single_read.go
index b418070..2468a13 100644
--- a/internal/tasks/key_based_loading_cb/task_single_read.go
+++ b/internal/tasks/key_based_loading_cb/task_single_read.go
@@ -1,159 +1,160 @@
 package key_based_loading_cb
 
-import (
-	"github.com/couchbaselabs/sirius/internal/cb_sdk"
-	"github.com/couchbaselabs/sirius/internal/task_errors"
-	"github.com/couchbaselabs/sirius/internal/task_result"
-	"github.com/couchbaselabs/sirius/internal/tasks"
-	"golang.org/x/sync/errgroup"
-	"log"
-	"strings"
-	"time"
-)
-
-type SingleReadTask struct {
-	IdentifierToken       string                  `json:"identifierToken" doc:"true"`
-	ClusterConfig         *cb_sdk.ClusterConfig   `json:"clusterConfig" doc:"true"`
-	Bucket                string                  `json:"bucket" doc:"true"`
-	Scope                 string                  `json:"scope,omitempty" doc:"true"`
-	Collection            string                  `json:"collection,omitempty" doc:"true"`
-	SingleOperationConfig *SingleOperationConfig  `json:"singleOperationConfig" doc:"true"`
-	Operation             string                  `json:"operation" doc:"false"`
-	ResultSeed            int64                   `json:"resultSeed" doc:"false"`
-	TaskPending           bool                    `json:"taskPending" doc:"false"`
-	Result                *task_result.TaskResult `json:"-" doc:"false"`
-	req                   *tasks.Request          `json:"-" doc:"false"`
-}
-
-func (task *SingleReadTask) Describe() string {
-	return "Single read task reads key value in couchbase and validates.\n"
-}
-
-func (task *SingleReadTask) CollectionIdentifier() string {
-	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
-	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
-		task.Collection}, ":")
-}
-
-func (task *SingleReadTask) CheckIfPending() bool {
-	return task.TaskPending
-}
-
-// Config configures  the insert task
-func (task *SingleReadTask) Config(req *tasks.Request, reRun bool) (int64, error) {
-	task.TaskPending = true
-	task.req = req
-
-	if task.req == nil {
-		task.TaskPending = false
-		return 0, task_errors.ErrRequestIsNil
-	}
-
-	task.req.ReconnectionManager()
-	if _, err := task.req.GetCluster(task.ClusterConfig); err != nil {
-		task.TaskPending = false
-		return 0, err
-	}
-
-	task.req.ReconfigureDocumentManager()
-
-	if !reRun {
-		task.ResultSeed = int64(time.Now().UnixNano())
-		task.Operation = tasks.SingleReadOperation
-
-		if task.Bucket == "" {
-			task.Bucket = cb_sdk.DefaultBucket
-		}
-		if task.Scope == "" {
-			task.Scope = cb_sdk.DefaultScope
-		}
-		if task.Collection == "" {
-			task.Collection = cb_sdk.DefaultCollection
-		}
-
-		if err := ConfigSingleOperationConfig(task.SingleOperationConfig); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-	} else {
-		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-	}
-	return task.ResultSeed, nil
-}
-
-func (task *SingleReadTask) TearUp() error {
-	task.Result.StopStoringResult()
-	if err := task.Result.SaveResultIntoFile(); err != nil {
-		log.Println("not able to save Result into ", task.ResultSeed)
-	}
-	task.Result = nil
-	task.TaskPending = false
-	return task.req.SaveRequestIntoFile()
-}
-
-func (task *SingleReadTask) Do() error {
-
-	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
-
-	collectionObject, err1 := task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope,
-		task.Collection)
-
-	if err1 != nil {
-		task.Result.ErrorOther = err1.Error()
-		task.Result.FailWholeSingleOperation(task.SingleOperationConfig.Keys, err1)
-		return task.TearUp()
-	}
-
-	singleReadDocuments(task, collectionObject)
-
-	task.Result.Success = int64(len(task.SingleOperationConfig.Keys)) - task.Result.Failure
-	return task.TearUp()
-}
-
-// singleDeleteDocuments uploads new documents in a bucket.scope.collection in a defined batch size at multiple iterations.
-func singleReadDocuments(task *SingleReadTask, collectionObject *cb_sdk.CollectionObject) {
-
-	if task.req.ContextClosed() {
-		return
-	}
-
-	routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
-	dataChannel := make(chan string, tasks.MaxConcurrentRoutines)
-
-	group := errgroup.Group{}
-
-	for _, data := range task.SingleOperationConfig.Keys {
-
-		if task.req.ContextClosed() {
-			close(routineLimiter)
-			close(dataChannel)
-			return
-		}
-
-		routineLimiter <- struct{}{}
-		dataChannel <- data
-
-		group.Go(func() error {
-			key := <-dataChannel
-			task.req.DocumentsMeta.GetDocumentsMetadata(task.CollectionIdentifier(), key, task.SingleOperationConfig.Template,
-				task.SingleOperationConfig.DocSize, false)
-
-			initTime := time.Now().UTC().Format(time.RFC850)
-			result, err := collectionObject.Collection.Get(key, nil)
-			if err != nil {
-				task.Result.CreateSingleErrorResult(initTime, key, err.Error(), false, 0)
-				<-routineLimiter
-				return err
-			}
-
-			task.Result.CreateSingleErrorResult(initTime, key, "", true, uint64(result.Cas()))
-			<-routineLimiter
-			return nil
-		})
-	}
-
-	_ = group.Wait()
-	close(routineLimiter)
-	close(dataChannel)
-	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-}
+//
+//import (
+//	"github.com/couchbaselabs/sirius/internal/cb_sdk"
+//	"github.com/couchbaselabs/sirius/internal/err_sirius"
+//	"github.com/couchbaselabs/sirius/internal/task_result"
+//	"github.com/couchbaselabs/sirius/internal/tasks"
+//	"golang.org/x/sync/errgroup"
+//	"log"
+//	"strings"
+//	"time"
+//)
+//
+//type SingleReadTask struct {
+//	IdentifierToken       string                  `json:"identifierToken" doc:"true"`
+//	ClusterConfig         *cb_sdk.ClusterConfig   `json:"clusterConfig" doc:"true"`
+//	Bucket                string                  `json:"bucket" doc:"true"`
+//	Scope                 string                  `json:"scope,omitempty" doc:"true"`
+//	Collection            string                  `json:"collection,omitempty" doc:"true"`
+//	SingleOperationConfig *SingleOperationConfig  `json:"singleOperationConfig" doc:"true"`
+//	Operation             string                  `json:"operation" doc:"false"`
+//	ResultSeed            int64                   `json:"resultSeed" doc:"false"`
+//	TaskPending           bool                    `json:"taskPending" doc:"false"`
+//	Result                *task_result.TaskResult `json:"-" doc:"false"`
+//	req                   *tasks.Request          `json:"-" doc:"false"`
+//}
+//
+//func (task *SingleReadTask) Describe() string {
+//	return "Single read task reads key value in couchbase and validates.\n"
+//}
+//
+//func (task *SingleReadTask) MetaDataIdentifier() string {
+//	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
+//	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
+//		task.Collection}, ":")
+//}
+//
+//func (task *SingleReadTask) CheckIfPending() bool {
+//	return task.TaskPending
+//}
+//
+//// Config configures  the insert task
+//func (task *SingleReadTask) Config(req *tasks.Request, reRun bool) (int64, error) {
+//	task.TaskPending = true
+//	task.req = req
+//
+//	if task.req == nil {
+//		task.TaskPending = false
+//		return 0, err_sirius.RequestIsNil
+//	}
+//
+//	task.req.ReconnectionManager()
+//	if _, err_sirius := task.req.GetCluster(task.ClusterConfig); err_sirius != nil {
+//		task.TaskPending = false
+//		return 0, err_sirius
+//	}
+//
+//	task.req.ReconfigureDocumentManager()
+//
+//	if !reRun {
+//		task.ResultSeed = int64(time.Now().UnixNano())
+//		task.Operation = tasks.SingleReadOperation
+//
+//		if task.Bucket == "" {
+//			task.Bucket = cb_sdk.DefaultBucket
+//		}
+//		if task.Scope == "" {
+//			task.Scope = cb_sdk.DefaultScope
+//		}
+//		if task.Collection == "" {
+//			task.Collection = cb_sdk.DefaultCollection
+//		}
+//
+//		if err_sirius := ConfigSingleOperationConfig(task.SingleOperationConfig); err_sirius != nil {
+//			task.TaskPending = false
+//			return 0, err_sirius
+//		}
+//	} else {
+//		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//	}
+//	return task.ResultSeed, nil
+//}
+//
+//func (task *SingleReadTask) TearUp() error {
+//	task.Result.StopStoringResult()
+//	if err_sirius := task.Result.SaveResultIntoFile(); err_sirius != nil {
+//		log.Println("not able to save Result into ", task.ResultSeed)
+//	}
+//	task.Result = nil
+//	task.TaskPending = false
+//	return task.req.SaveRequestIntoFile()
+//}
+//
+//func (task *SingleReadTask) Do() error {
+//
+//	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
+//
+//	collectionObject, err1 := task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope,
+//		task.Collection)
+//
+//	if err1 != nil {
+//		task.Result.ErrorOther = err1.Error()
+//		task.Result.FailWholeSingleOperation(task.SingleOperationConfig.Keys, err1)
+//		return task.TearUp()
+//	}
+//
+//	singleReadDocuments(task, collectionObject)
+//
+//	task.Result.Success = int64(len(task.SingleOperationConfig.Keys)) - task.Result.Failure
+//	return task.TearUp()
+//}
+//
+//// singleDeleteDocuments uploads new documents in a bucket.scope.collection in a defined batch size at multiple iterations.
+//func singleReadDocuments(task *SingleReadTask, collectionObject *cb_sdk.CollectionObject) {
+//
+//	if task.req.ContextClosed() {
+//		return
+//	}
+//
+//	routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
+//	dataChannel := make(chan string, tasks.MaxConcurrentRoutines)
+//
+//	group := errgroup.Group{}
+//
+//	for _, data := range task.SingleOperationConfig.Keys {
+//
+//		if task.req.ContextClosed() {
+//			close(routineLimiter)
+//			close(dataChannel)
+//			return
+//		}
+//
+//		routineLimiter <- struct{}{}
+//		dataChannel <- data
+//
+//		group.Go(func() error {
+//			key := <-dataChannel
+//			task.req.DocumentsMeta.GetDocumentsMetadata(task.MetaDataIdentifier(), key, task.SingleOperationConfig.Template,
+//				task.SingleOperationConfig.DocSize, false)
+//
+//			initTime := time.Now().UTC().Format(time.RFC850)
+//			result, err_sirius := collectionObject.Collection.Get(key, nil)
+//			if err_sirius != nil {
+//				task.Result.CreateSingleErrorResult(initTime, key, err_sirius.Error(), false, 0)
+//				<-routineLimiter
+//				return err_sirius
+//			}
+//
+//			task.Result.CreateSingleErrorResult(initTime, key, "", true, uint64(result.Cas()))
+//			<-routineLimiter
+//			return nil
+//		})
+//	}
+//
+//	_ = group.Wait()
+//	close(routineLimiter)
+//	close(dataChannel)
+//	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//}
diff --git a/internal/tasks/key_based_loading_cb/task_single_replace.go b/internal/tasks/key_based_loading_cb/task_single_replace.go
index 36612ea..398aed3 100644
--- a/internal/tasks/key_based_loading_cb/task_single_replace.go
+++ b/internal/tasks/key_based_loading_cb/task_single_replace.go
@@ -1,185 +1,186 @@
 package key_based_loading_cb
 
-import (
-	"github.com/couchbase/gocb/v2"
-	"github.com/couchbaselabs/sirius/internal/cb_sdk"
-	"github.com/couchbaselabs/sirius/internal/task_errors"
-	"github.com/couchbaselabs/sirius/internal/task_result"
-	"github.com/couchbaselabs/sirius/internal/tasks"
-	"github.com/couchbaselabs/sirius/internal/template"
-	"github.com/jaswdr/faker"
-	"golang.org/x/sync/errgroup"
-	"log"
-	"math/rand"
-	"strings"
-	"time"
-)
-
-type SingleReplaceTask struct {
-	IdentifierToken       string                  `json:"identifierToken" doc:"true"`
-	ClusterConfig         *cb_sdk.ClusterConfig   `json:"clusterConfig" doc:"true"`
-	Bucket                string                  `json:"bucket" doc:"true"`
-	Scope                 string                  `json:"scope,omitempty" doc:"true"`
-	Collection            string                  `json:"collection,omitempty" doc:"true"`
-	ReplaceOptions        *cb_sdk.ReplaceOptions  `json:"replaceOptions,omitempty" doc:"true"`
-	SingleOperationConfig *SingleOperationConfig  `json:"singleOperationConfig" doc:"true"`
-	Operation             string                  `json:"operation" doc:"false"`
-	ResultSeed            int64                   `json:"resultSeed" doc:"false"`
-	TaskPending           bool                    `json:"taskPending" doc:"false"`
-	Result                *task_result.TaskResult `json:"-" doc:"false"`
-	req                   *tasks.Request          `json:"-" doc:"false"`
-}
-
-func (task *SingleReplaceTask) Describe() string {
-	return "Single replace task a document in the collection in Couchbase.\n"
-}
-
-func (task *SingleReplaceTask) CollectionIdentifier() string {
-	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
-	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
-		task.Collection}, ":")
-}
-
-func (task *SingleReplaceTask) CheckIfPending() bool {
-	return task.TaskPending
-}
-
-// Config configures  the insert task
-func (task *SingleReplaceTask) Config(req *tasks.Request, reRun bool) (int64, error) {
-	task.TaskPending = true
-	task.req = req
-
-	if task.req == nil {
-		task.TaskPending = false
-		return 0, task_errors.ErrRequestIsNil
-	}
-
-	task.req.ReconnectionManager()
-	if _, err := task.req.GetCluster(task.ClusterConfig); err != nil {
-		task.TaskPending = false
-		return 0, err
-	}
-
-	task.req.ReconfigureDocumentManager()
-
-	if !reRun {
-		task.ResultSeed = int64(time.Now().UnixNano())
-		task.Operation = tasks.SingleReplaceOperation
-
-		if task.Bucket == "" {
-			task.Bucket = cb_sdk.DefaultBucket
-		}
-		if task.Scope == "" {
-			task.Scope = cb_sdk.DefaultScope
-		}
-		if task.Collection == "" {
-			task.Collection = cb_sdk.DefaultCollection
-		}
-
-		if err := cb_sdk.ConfigReplaceOptions(task.ReplaceOptions); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-
-		if err := ConfigSingleOperationConfig(task.SingleOperationConfig); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-	} else {
-		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-	}
-	return task.ResultSeed, nil
-}
-
-func (task *SingleReplaceTask) TearUp() error {
-	task.Result.StopStoringResult()
-	if err := task.Result.SaveResultIntoFile(); err != nil {
-		log.Println("not able to save Result into ", task.ResultSeed)
-	}
-	task.Result = nil
-	task.TaskPending = false
-	return task.req.SaveRequestIntoFile()
-}
-
-func (task *SingleReplaceTask) Do() error {
-
-	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
-
-	collectionObject, err1 := task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope,
-		task.Collection)
-
-	if err1 != nil {
-		task.Result.ErrorOther = err1.Error()
-		task.Result.FailWholeSingleOperation(task.SingleOperationConfig.Keys, err1)
-		return task.TearUp()
-	}
-
-	singleReplaceDocuments(task, collectionObject)
-
-	task.Result.Success = int64(len(task.SingleOperationConfig.Keys)) - task.Result.Failure
-	return task.TearUp()
-}
-
-// singleReplaceDocuments uploads new documents in a bucket.scope.
-// collection in a defined batch size at multiple iterations.
-func singleReplaceDocuments(task *SingleReplaceTask, collectionObject *cb_sdk.CollectionObject) {
-
-	if task.req.ContextClosed() {
-		return
-	}
-
-	routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
-	dataChannel := make(chan string, tasks.MaxConcurrentRoutines)
-
-	group := errgroup.Group{}
-
-	for _, data := range task.SingleOperationConfig.Keys {
-
-		if task.req.ContextClosed() {
-			close(routineLimiter)
-			close(dataChannel)
-			return
-		}
-
-		routineLimiter <- struct{}{}
-		dataChannel <- data
-
-		group.Go(func() error {
-			key := <-dataChannel
-
-			documentMetaData := task.req.DocumentsMeta.GetDocumentsMetadata(task.CollectionIdentifier(), key, task.SingleOperationConfig.Template,
-				task.SingleOperationConfig.DocSize, true)
-
-			fake := faker.NewWithSeed(rand.NewSource(int64(documentMetaData.Seed)))
-
-			t := template.InitialiseTemplate(documentMetaData.Template)
-
-			doc, _ := t.GenerateDocument(&fake, documentMetaData.DocSize)
-
-			initTime := time.Now().UTC().Format(time.RFC850)
-			result, err := collectionObject.Collection.Replace(key, doc, &gocb.ReplaceOptions{
-				Expiry:          time.Duration(task.ReplaceOptions.Expiry) * time.Second,
-				Cas:             gocb.Cas(task.ReplaceOptions.Cas),
-				PersistTo:       task.ReplaceOptions.PersistTo,
-				ReplicateTo:     task.ReplaceOptions.ReplicateTo,
-				DurabilityLevel: cb_sdk.GetDurability(task.ReplaceOptions.Durability),
-				Timeout:         time.Duration(task.ReplaceOptions.Timeout) * time.Second,
-			})
-
-			if err != nil {
-				task.Result.CreateSingleErrorResult(initTime, key, err.Error(), false, 0)
-				<-routineLimiter
-				return err
-			}
-
-			task.Result.CreateSingleErrorResult(initTime, key, "", true, uint64(result.Cas()))
-			<-routineLimiter
-			return nil
-		})
-	}
-
-	_ = group.Wait()
-	close(routineLimiter)
-	close(dataChannel)
-	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-}
+//
+//import (
+//	"github.com/couchbase/gocb/v2"
+//	"github.com/couchbaselabs/sirius/internal/cb_sdk"
+//	"github.com/couchbaselabs/sirius/internal/err_sirius"
+//	"github.com/couchbaselabs/sirius/internal/task_result"
+//	"github.com/couchbaselabs/sirius/internal/tasks"
+//	"github.com/couchbaselabs/sirius/internal/template"
+//	"github.com/jaswdr/faker"
+//	"golang.org/x/sync/errgroup"
+//	"log"
+//	"math/rand"
+//	"strings"
+//	"time"
+//)
+//
+//type SingleReplaceTask struct {
+//	IdentifierToken       string                  `json:"identifierToken" doc:"true"`
+//	ClusterConfig         *cb_sdk.ClusterConfig   `json:"clusterConfig" doc:"true"`
+//	Bucket                string                  `json:"bucket" doc:"true"`
+//	Scope                 string                  `json:"scope,omitempty" doc:"true"`
+//	Collection            string                  `json:"collection,omitempty" doc:"true"`
+//	ReplaceOptions        *cb_sdk.ReplaceOptions  `json:"replaceOptions,omitempty" doc:"true"`
+//	SingleOperationConfig *SingleOperationConfig  `json:"singleOperationConfig" doc:"true"`
+//	Operation             string                  `json:"operation" doc:"false"`
+//	ResultSeed            int64                   `json:"resultSeed" doc:"false"`
+//	TaskPending           bool                    `json:"taskPending" doc:"false"`
+//	Result                *task_result.TaskResult `json:"-" doc:"false"`
+//	req                   *tasks.Request          `json:"-" doc:"false"`
+//}
+//
+//func (task *SingleReplaceTask) Describe() string {
+//	return "Single replace task a document in the collection in Couchbase.\n"
+//}
+//
+//func (task *SingleReplaceTask) MetaDataIdentifier() string {
+//	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
+//	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
+//		task.Collection}, ":")
+//}
+//
+//func (task *SingleReplaceTask) CheckIfPending() bool {
+//	return task.TaskPending
+//}
+//
+//// Config configures  the insert task
+//func (task *SingleReplaceTask) Config(req *tasks.Request, reRun bool) (int64, error) {
+//	task.TaskPending = true
+//	task.req = req
+//
+//	if task.req == nil {
+//		task.TaskPending = false
+//		return 0, err_sirius.RequestIsNil
+//	}
+//
+//	task.req.ReconnectionManager()
+//	if _, err_sirius := task.req.GetCluster(task.ClusterConfig); err_sirius != nil {
+//		task.TaskPending = false
+//		return 0, err_sirius
+//	}
+//
+//	task.req.ReconfigureDocumentManager()
+//
+//	if !reRun {
+//		task.ResultSeed = int64(time.Now().UnixNano())
+//		task.Operation = tasks.SingleReplaceOperation
+//
+//		if task.Bucket == "" {
+//			task.Bucket = cb_sdk.DefaultBucket
+//		}
+//		if task.Scope == "" {
+//			task.Scope = cb_sdk.DefaultScope
+//		}
+//		if task.Collection == "" {
+//			task.Collection = cb_sdk.DefaultCollection
+//		}
+//
+//		if err_sirius := cb_sdk.ConfigReplaceOptions(task.ReplaceOptions); err_sirius != nil {
+//			task.TaskPending = false
+//			return 0, err_sirius
+//		}
+//
+//		if err_sirius := ConfigSingleOperationConfig(task.SingleOperationConfig); err_sirius != nil {
+//			task.TaskPending = false
+//			return 0, err_sirius
+//		}
+//	} else {
+//		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//	}
+//	return task.ResultSeed, nil
+//}
+//
+//func (task *SingleReplaceTask) TearUp() error {
+//	task.Result.StopStoringResult()
+//	if err_sirius := task.Result.SaveResultIntoFile(); err_sirius != nil {
+//		log.Println("not able to save Result into ", task.ResultSeed)
+//	}
+//	task.Result = nil
+//	task.TaskPending = false
+//	return task.req.SaveRequestIntoFile()
+//}
+//
+//func (task *SingleReplaceTask) Do() error {
+//
+//	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
+//
+//	collectionObject, err1 := task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope,
+//		task.Collection)
+//
+//	if err1 != nil {
+//		task.Result.ErrorOther = err1.Error()
+//		task.Result.FailWholeSingleOperation(task.SingleOperationConfig.Keys, err1)
+//		return task.TearUp()
+//	}
+//
+//	singleReplaceDocuments(task, collectionObject)
+//
+//	task.Result.Success = int64(len(task.SingleOperationConfig.Keys)) - task.Result.Failure
+//	return task.TearUp()
+//}
+//
+//// singleReplaceDocuments uploads new documents in a bucket.scope.
+//// collection in a defined batch size at multiple iterations.
+//func singleReplaceDocuments(task *SingleReplaceTask, collectionObject *cb_sdk.CollectionObject) {
+//
+//	if task.req.ContextClosed() {
+//		return
+//	}
+//
+//	routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
+//	dataChannel := make(chan string, tasks.MaxConcurrentRoutines)
+//
+//	group := errgroup.Group{}
+//
+//	for _, data := range task.SingleOperationConfig.Keys {
+//
+//		if task.req.ContextClosed() {
+//			close(routineLimiter)
+//			close(dataChannel)
+//			return
+//		}
+//
+//		routineLimiter <- struct{}{}
+//		dataChannel <- data
+//
+//		group.Go(func() error {
+//			key := <-dataChannel
+//
+//			documentMetaData := task.req.DocumentsMeta.GetDocumentsMetadata(task.MetaDataIdentifier(), key, task.SingleOperationConfig.Template,
+//				task.SingleOperationConfig.DocSize, true)
+//
+//			fake := faker.NewWithSeed(rand.NewSource(int64(documentMetaData.Seed)))
+//
+//			t := template.InitialiseTemplate(documentMetaData.Template)
+//
+//			doc, _ := t.GenerateDocument(&fake, documentMetaData.DocSize)
+//
+//			initTime := time.Now().UTC().Format(time.RFC850)
+//			result, err_sirius := collectionObject.Collection.Replace(key, doc, &gocb.ReplaceOptions{
+//				Expiry:          time.Duration(task.ReplaceOptions.Expiry) * time.Second,
+//				Cas:             gocb.Cas(task.ReplaceOptions.Cas),
+//				PersistTo:       task.ReplaceOptions.PersistTo,
+//				ReplicateTo:     task.ReplaceOptions.ReplicateTo,
+//				DurabilityLevel: cb_sdk.GetDurability(task.ReplaceOptions.Durability),
+//				Timeout:         time.Duration(task.ReplaceOptions.Timeout) * time.Second,
+//			})
+//
+//			if err_sirius != nil {
+//				task.Result.CreateSingleErrorResult(initTime, key, err_sirius.Error(), false, 0)
+//				<-routineLimiter
+//				return err_sirius
+//			}
+//
+//			task.Result.CreateSingleErrorResult(initTime, key, "", true, uint64(result.Cas()))
+//			<-routineLimiter
+//			return nil
+//		})
+//	}
+//
+//	_ = group.Wait()
+//	close(routineLimiter)
+//	close(dataChannel)
+//	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//}
diff --git a/internal/tasks/key_based_loading_cb/task_single_sub_doc_delete.go b/internal/tasks/key_based_loading_cb/task_single_sub_doc_delete.go
index a50062f..593ce0b 100644
--- a/internal/tasks/key_based_loading_cb/task_single_sub_doc_delete.go
+++ b/internal/tasks/key_based_loading_cb/task_single_sub_doc_delete.go
@@ -1,178 +1,179 @@
 package key_based_loading_cb
 
-import (
-	"github.com/couchbase/gocb/v2"
-	"github.com/couchbaselabs/sirius/internal/cb_sdk"
-	"github.com/couchbaselabs/sirius/internal/task_errors"
-	"github.com/couchbaselabs/sirius/internal/task_result"
-	"github.com/couchbaselabs/sirius/internal/tasks"
-	"github.com/couchbaselabs/sirius/internal/template"
-	"log"
-	"strings"
-	"time"
-)
-
-type SingleSubDocDelete struct {
-	IdentifierToken             string                       `json:"identifierToken" doc:"true"`
-	ClusterConfig               *cb_sdk.ClusterConfig        `json:"clusterConfig" doc:"true"`
-	Bucket                      string                       `json:"bucket" doc:"true"`
-	Scope                       string                       `json:"scope,omitempty" doc:"true"`
-	Collection                  string                       `json:"collection,omitempty" doc:"true"`
-	SingleSubDocOperationConfig *SingleSubDocOperationConfig `json:"singleSubDocOperationConfig" doc:"true"`
-	RemoveSpecOptions           *cb_sdk.RemoveSpecOptions    `json:"removeSpecOptions" doc:"true"`
-	MutateInOptions             *cb_sdk.MutateInOptions      `json:"mutateInOptions" doc:"true"`
-	Operation                   string                       `json:"operation" doc:"false"`
-	ResultSeed                  int64                        `json:"resultSeed" doc:"false"`
-	TaskPending                 bool                         `json:"taskPending" doc:"false"`
-	Result                      *task_result.TaskResult      `json:"-" doc:"false"`
-	req                         *tasks.Request               `json:"-" doc:"false"`
-}
-
-func (task *SingleSubDocDelete) Describe() string {
-	return "SingleSingleSubDocDelete inserts a Sub-Document as per user's input [No Random data]"
-}
-
-func (task *SingleSubDocDelete) CollectionIdentifier() string {
-	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
-	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
-		task.Collection}, ":")
-}
-
-func (task *SingleSubDocDelete) CheckIfPending() bool {
-	return task.TaskPending
-}
-
-// Config configures  the insert task
-func (task *SingleSubDocDelete) Config(req *tasks.Request, reRun bool) (int64, error) {
-	task.TaskPending = true
-	task.req = req
-
-	if task.req == nil {
-		task.TaskPending = false
-		return 0, task_errors.ErrRequestIsNil
-	}
-
-	task.req.ReconnectionManager()
-	if _, err := task.req.GetCluster(task.ClusterConfig); err != nil {
-		task.TaskPending = false
-		return 0, err
-	}
-
-	task.req.ReconfigureDocumentManager()
-
-	if !reRun {
-		task.ResultSeed = int64(time.Now().UnixNano())
-		task.Operation = tasks.SingleSubDocDeleteOperation
-
-		if task.Bucket == "" {
-			task.Bucket = cb_sdk.DefaultBucket
-		}
-		if task.Scope == "" {
-			task.Scope = cb_sdk.DefaultScope
-		}
-		if task.Collection == "" {
-			task.Collection = cb_sdk.DefaultCollection
-		}
-
-		if err := ConfigSingleSubDocOperationConfig(task.SingleSubDocOperationConfig); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-
-		if err := cb_sdk.ConfigRemoveSpecOptions(task.RemoveSpecOptions); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-
-		if err := cb_sdk.ConfigMutateInOptions(task.MutateInOptions); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-	} else {
-		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-	}
-	return task.ResultSeed, nil
-}
-
-func (task *SingleSubDocDelete) TearUp() error {
-	task.Result.StopStoringResult()
-	if err := task.Result.SaveResultIntoFile(); err != nil {
-		log.Println("not able to save Result into ", task.ResultSeed, task.Operation)
-	}
-	task.Result = nil
-	task.TaskPending = false
-	return task.req.SaveRequestIntoFile()
-}
-
-func (task *SingleSubDocDelete) Do() error {
-
-	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
-
-	collectionObject, err1 := task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope,
-		task.Collection)
-
-	if err1 != nil {
-		task.Result.ErrorOther = err1.Error()
-		task.Result.FailWholeSingleOperation([]string{task.SingleSubDocOperationConfig.Key}, err1)
-		return task.TearUp()
-	}
-
-	singleDeleteSubDocuments(task, collectionObject)
-
-	task.Result.Success = 1 - task.Result.Failure
-	return task.TearUp()
-}
-
-// singleInsertSubDocuments uploads new documents in a bucket.scope.collection in a defined batch size at multiple iterations.
-func singleDeleteSubDocuments(task *SingleSubDocDelete, collectionObject *cb_sdk.CollectionObject) {
-
-	if task.req.ContextClosed() {
-		return
-	}
-
-	var iOps []gocb.MutateInSpec
-	key := task.SingleSubDocOperationConfig.Key
-	documentMetaData := task.req.DocumentsMeta.GetDocumentsMetadata(task.CollectionIdentifier(), key, "", 0, false)
-
-	for _, path := range task.SingleSubDocOperationConfig.Paths {
-		_ = documentMetaData.SubDocument(path, task.RemoveSpecOptions.IsXattr, task.SingleSubDocOperationConfig.DocSize,
-			false)
-
-		documentMetaData.RemovePath(path)
-
-		iOps = append(iOps, gocb.RemoveSpec(path, &gocb.RemoveSpecOptions{
-			IsXattr: task.RemoveSpecOptions.IsXattr,
-		}))
-	}
-
-	if !task.RemoveSpecOptions.IsXattr {
-		iOps = append(iOps, gocb.IncrementSpec(template.MutatedPath,
-			int64(template.MutateFieldIncrement), &gocb.CounterSpecOptions{
-				CreatePath: true,
-				IsXattr:    false,
-			}))
-	}
-
-	initTime := time.Now().UTC().Format(time.RFC850)
-	result, err := collectionObject.Collection.MutateIn(key, iOps, &gocb.MutateInOptions{
-		Expiry:          time.Duration(task.MutateInOptions.Expiry) * time.Second,
-		Cas:             gocb.Cas(task.MutateInOptions.Cas),
-		PersistTo:       task.MutateInOptions.PersistTo,
-		ReplicateTo:     task.MutateInOptions.ReplicateTo,
-		DurabilityLevel: cb_sdk.GetDurability(task.MutateInOptions.Durability),
-		StoreSemantic:   cb_sdk.GetStoreSemantic(task.MutateInOptions.StoreSemantic),
-		Timeout:         time.Duration(task.MutateInOptions.Timeout) * time.Second,
-		PreserveExpiry:  task.MutateInOptions.PreserveExpiry,
-	})
-
-	if err != nil {
-		task.Result.CreateSingleErrorResult(initTime, key, err.Error(), false, 0)
-	} else {
-		if !task.RemoveSpecOptions.IsXattr {
-			documentMetaData.IncrementMutationCount()
-		}
-		task.Result.CreateSingleErrorResult(initTime, key, "", true, uint64(result.Cas()))
-	}
-
-	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-}
+//
+//import (
+//	"github.com/couchbase/gocb/v2"
+//	"github.com/couchbaselabs/sirius/internal/cb_sdk"
+//	"github.com/couchbaselabs/sirius/internal/err_sirius"
+//	"github.com/couchbaselabs/sirius/internal/task_result"
+//	"github.com/couchbaselabs/sirius/internal/tasks"
+//	"github.com/couchbaselabs/sirius/internal/template"
+//	"log"
+//	"strings"
+//	"time"
+//)
+//
+//type SingleSubDocDelete struct {
+//	IdentifierToken             string                       `json:"identifierToken" doc:"true"`
+//	ClusterConfig               *cb_sdk.ClusterConfig        `json:"clusterConfig" doc:"true"`
+//	Bucket                      string                       `json:"bucket" doc:"true"`
+//	Scope                       string                       `json:"scope,omitempty" doc:"true"`
+//	Collection                  string                       `json:"collection,omitempty" doc:"true"`
+//	SingleSubDocOperationConfig *SingleSubDocOperationConfig `json:"singleSubDocOperationConfig" doc:"true"`
+//	RemoveSpecOptions           *cb_sdk.RemoveSpecOptions    `json:"removeSpecOptions" doc:"true"`
+//	MutateInOptions             *cb_sdk.MutateInOptions      `json:"mutateInOptions" doc:"true"`
+//	Operation                   string                       `json:"operation" doc:"false"`
+//	ResultSeed                  int64                        `json:"resultSeed" doc:"false"`
+//	TaskPending                 bool                         `json:"taskPending" doc:"false"`
+//	Result                      *task_result.TaskResult      `json:"-" doc:"false"`
+//	req                         *tasks.Request               `json:"-" doc:"false"`
+//}
+//
+//func (task *SingleSubDocDelete) Describe() string {
+//	return "SingleSingleSubDocDelete inserts a Sub-Document as per user's input [No Random data]"
+//}
+//
+//func (task *SingleSubDocDelete) MetaDataIdentifier() string {
+//	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
+//	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
+//		task.Collection}, ":")
+//}
+//
+//func (task *SingleSubDocDelete) CheckIfPending() bool {
+//	return task.TaskPending
+//}
+//
+//// Config configures  the insert task
+//func (task *SingleSubDocDelete) Config(req *tasks.Request, reRun bool) (int64, error) {
+//	task.TaskPending = true
+//	task.req = req
+//
+//	if task.req == nil {
+//		task.TaskPending = false
+//		return 0, err_sirius.RequestIsNil
+//	}
+//
+//	task.req.ReconnectionManager()
+//	if _, err_sirius := task.req.GetCluster(task.ClusterConfig); err_sirius != nil {
+//		task.TaskPending = false
+//		return 0, err_sirius
+//	}
+//
+//	task.req.ReconfigureDocumentManager()
+//
+//	if !reRun {
+//		task.ResultSeed = int64(time.Now().UnixNano())
+//		task.Operation = tasks.SingleSubDocDeleteOperation
+//
+//		if task.Bucket == "" {
+//			task.Bucket = cb_sdk.DefaultBucket
+//		}
+//		if task.Scope == "" {
+//			task.Scope = cb_sdk.DefaultScope
+//		}
+//		if task.Collection == "" {
+//			task.Collection = cb_sdk.DefaultCollection
+//		}
+//
+//		if err_sirius := ConfigSingleSubDocOperationConfig(task.SingleSubDocOperationConfig); err_sirius != nil {
+//			task.TaskPending = false
+//			return 0, err_sirius
+//		}
+//
+//		if err_sirius := cb_sdk.ConfigRemoveSpecOptions(task.RemoveSpecOptions); err_sirius != nil {
+//			task.TaskPending = false
+//			return 0, err_sirius
+//		}
+//
+//		if err_sirius := cb_sdk.ConfigMutateInOptions(task.MutateInOptions); err_sirius != nil {
+//			task.TaskPending = false
+//			return 0, err_sirius
+//		}
+//	} else {
+//		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//	}
+//	return task.ResultSeed, nil
+//}
+//
+//func (task *SingleSubDocDelete) TearUp() error {
+//	task.Result.StopStoringResult()
+//	if err_sirius := task.Result.SaveResultIntoFile(); err_sirius != nil {
+//		log.Println("not able to save Result into ", task.ResultSeed, task.Operation)
+//	}
+//	task.Result = nil
+//	task.TaskPending = false
+//	return task.req.SaveRequestIntoFile()
+//}
+//
+//func (task *SingleSubDocDelete) Do() error {
+//
+//	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
+//
+//	collectionObject, err1 := task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope,
+//		task.Collection)
+//
+//	if err1 != nil {
+//		task.Result.ErrorOther = err1.Error()
+//		task.Result.FailWholeSingleOperation([]string{task.SingleSubDocOperationConfig.Key}, err1)
+//		return task.TearUp()
+//	}
+//
+//	singleDeleteSubDocuments(task, collectionObject)
+//
+//	task.Result.Success = 1 - task.Result.Failure
+//	return task.TearUp()
+//}
+//
+//// singleInsertSubDocuments uploads new documents in a bucket.scope.collection in a defined batch size at multiple iterations.
+//func singleDeleteSubDocuments(task *SingleSubDocDelete, collectionObject *cb_sdk.CollectionObject) {
+//
+//	if task.req.ContextClosed() {
+//		return
+//	}
+//
+//	var iOps []gocb.MutateInSpec
+//	key := task.SingleSubDocOperationConfig.Key
+//	documentMetaData := task.req.DocumentsMeta.GetDocumentsMetadata(task.MetaDataIdentifier(), key, "", 0, false)
+//
+//	for _, path := range task.SingleSubDocOperationConfig.Paths {
+//		_ = documentMetaData.SubDocument(path, task.RemoveSpecOptions.IsXattr, task.SingleSubDocOperationConfig.DocSize,
+//			false)
+//
+//		documentMetaData.RemovePath(path)
+//
+//		iOps = append(iOps, gocb.RemoveSpec(path, &gocb.RemoveSpecOptions{
+//			IsXattr: task.RemoveSpecOptions.IsXattr,
+//		}))
+//	}
+//
+//	if !task.RemoveSpecOptions.IsXattr {
+//		iOps = append(iOps, gocb.IncrementSpec(template.MutatedPath,
+//			int64(template.MutateFieldIncrement), &gocb.CounterSpecOptions{
+//				CreatePath: true,
+//				IsXattr:    false,
+//			}))
+//	}
+//
+//	initTime := time.Now().UTC().Format(time.RFC850)
+//	result, err_sirius := collectionObject.Collection.MutateIn(key, iOps, &gocb.MutateInOptions{
+//		Expiry:          time.Duration(task.MutateInOptions.Expiry) * time.Second,
+//		Cas:             gocb.Cas(task.MutateInOptions.Cas),
+//		PersistTo:       task.MutateInOptions.PersistTo,
+//		ReplicateTo:     task.MutateInOptions.ReplicateTo,
+//		DurabilityLevel: cb_sdk.GetDurability(task.MutateInOptions.Durability),
+//		StoreSemantic:   cb_sdk.GetStoreSemantic(task.MutateInOptions.StoreSemantic),
+//		Timeout:         time.Duration(task.MutateInOptions.Timeout) * time.Second,
+//		PreserveExpiry:  task.MutateInOptions.PreserveExpiry,
+//	})
+//
+//	if err_sirius != nil {
+//		task.Result.CreateSingleErrorResult(initTime, key, err_sirius.Error(), false, 0)
+//	} else {
+//		if !task.RemoveSpecOptions.IsXattr {
+//			documentMetaData.IncrementMutationCount()
+//		}
+//		task.Result.CreateSingleErrorResult(initTime, key, "", true, uint64(result.Cas()))
+//	}
+//
+//	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//}
diff --git a/internal/tasks/key_based_loading_cb/task_single_sub_doc_insert.go b/internal/tasks/key_based_loading_cb/task_single_sub_doc_insert.go
index 7e60dda..2feabfe 100644
--- a/internal/tasks/key_based_loading_cb/task_single_sub_doc_insert.go
+++ b/internal/tasks/key_based_loading_cb/task_single_sub_doc_insert.go
@@ -1,183 +1,184 @@
 package key_based_loading_cb
 
-import (
-	"github.com/couchbase/gocb/v2"
-	"github.com/couchbaselabs/sirius/internal/cb_sdk"
-	"github.com/couchbaselabs/sirius/internal/task_errors"
-	"github.com/couchbaselabs/sirius/internal/task_result"
-	"github.com/couchbaselabs/sirius/internal/tasks"
-	"github.com/couchbaselabs/sirius/internal/template"
-	"github.com/jaswdr/faker"
-	"log"
-	"math/rand"
-	"strings"
-	"time"
-)
-
-type SingleSubDocInsert struct {
-	IdentifierToken             string                       `json:"identifierToken" doc:"true"`
-	ClusterConfig               *cb_sdk.ClusterConfig        `json:"clusterConfig" doc:"true"`
-	Bucket                      string                       `json:"bucket" doc:"true"`
-	Scope                       string                       `json:"scope,omitempty" doc:"true"`
-	Collection                  string                       `json:"collection,omitempty" doc:"true"`
-	SingleSubDocOperationConfig *SingleSubDocOperationConfig `json:"singleSubDocOperationConfig" doc:"true"`
-	InsertSpecOptions           *cb_sdk.InsertSpecOptions    `json:"insertSpecOptions" doc:"true"`
-	MutateInOptions             *cb_sdk.MutateInOptions      `json:"mutateInOptions" doc:"true"`
-	Operation                   string                       `json:"operation" doc:"false"`
-	ResultSeed                  int64                        `json:"resultSeed" doc:"false"`
-	TaskPending                 bool                         `json:"taskPending" doc:"false"`
-	Result                      *task_result.TaskResult      `json:"-" doc:"false"`
-	req                         *tasks.Request               `json:"-" doc:"false"`
-}
-
-func (task *SingleSubDocInsert) Describe() string {
-	return "SingleSingleSubDocInsert inserts a Sub-Document as per user's input [No Random data]"
-}
-
-func (task *SingleSubDocInsert) CollectionIdentifier() string {
-	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
-	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
-		task.Collection}, ":")
-}
-
-func (task *SingleSubDocInsert) CheckIfPending() bool {
-	return task.TaskPending
-}
-
-// Config configures  the insert task
-func (task *SingleSubDocInsert) Config(req *tasks.Request, reRun bool) (int64, error) {
-	task.TaskPending = true
-	task.req = req
-
-	if task.req == nil {
-		task.TaskPending = false
-		return 0, task_errors.ErrRequestIsNil
-	}
-
-	task.req.ReconnectionManager()
-	if _, err := task.req.GetCluster(task.ClusterConfig); err != nil {
-		task.TaskPending = false
-		return 0, err
-	}
-
-	task.req.ReconfigureDocumentManager()
-
-	if !reRun {
-		task.ResultSeed = int64(time.Now().UnixNano())
-		task.Operation = tasks.SingleSubDocInsertOperation
-
-		if task.Bucket == "" {
-			task.Bucket = cb_sdk.DefaultBucket
-		}
-		if task.Scope == "" {
-			task.Scope = cb_sdk.DefaultScope
-		}
-		if task.Collection == "" {
-			task.Collection = cb_sdk.DefaultCollection
-		}
-
-		if err := ConfigSingleSubDocOperationConfig(task.SingleSubDocOperationConfig); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-
-		if err := cb_sdk.ConfigInsertSpecOptions(task.InsertSpecOptions); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-
-		if err := cb_sdk.ConfigMutateInOptions(task.MutateInOptions); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-	} else {
-		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-	}
-	return task.ResultSeed, nil
-}
-
-func (task *SingleSubDocInsert) TearUp() error {
-	task.Result.StopStoringResult()
-	if err := task.Result.SaveResultIntoFile(); err != nil {
-		log.Println("not able to save Result into ", task.ResultSeed, task.Operation)
-	}
-	task.Result = nil
-	task.TaskPending = false
-	return task.req.SaveRequestIntoFile()
-}
-
-func (task *SingleSubDocInsert) Do() error {
-
-	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
-
-	collectionObject, err1 := task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope,
-		task.Collection)
-
-	if err1 != nil {
-		task.Result.ErrorOther = err1.Error()
-		task.Result.FailWholeSingleOperation([]string{task.SingleSubDocOperationConfig.Key}, err1)
-		return task.TearUp()
-	}
-
-	singleInsertSubDocuments(task, collectionObject)
-
-	task.Result.Success = 1 - task.Result.Failure
-	return task.TearUp()
-}
-
-// singleInsertSubDocuments uploads new documents in a bucket.scope.collection in a defined batch size at multiple iterations.
-func singleInsertSubDocuments(task *SingleSubDocInsert, collectionObject *cb_sdk.CollectionObject) {
-
-	if task.req.ContextClosed() {
-		return
-	}
-
-	var iOps []gocb.MutateInSpec
-	key := task.SingleSubDocOperationConfig.Key
-	documentMetaData := task.req.DocumentsMeta.GetDocumentsMetadata(task.CollectionIdentifier(), key, "", 0, false)
-
-	for _, path := range task.SingleSubDocOperationConfig.Paths {
-		subDocument := documentMetaData.SubDocument(path, task.InsertSpecOptions.IsXattr,
-			task.SingleSubDocOperationConfig.DocSize, false)
-
-		fake := faker.NewWithSeed(rand.NewSource(int64(subDocument.Seed)))
-
-		value := subDocument.GenerateValue(&fake)
-
-		iOps = append(iOps, gocb.InsertSpec(path, value, &gocb.InsertSpecOptions{
-			CreatePath: task.InsertSpecOptions.CreatePath,
-			IsXattr:    task.InsertSpecOptions.IsXattr,
-		}))
-	}
-
-	if !task.InsertSpecOptions.IsXattr {
-		iOps = append(iOps, gocb.IncrementSpec(template.MutatedPath,
-			int64(template.MutateFieldIncrement), &gocb.CounterSpecOptions{
-				CreatePath: true,
-				IsXattr:    false,
-			}))
-	}
-
-	initTime := time.Now().UTC().Format(time.RFC850)
-	result, err := collectionObject.Collection.MutateIn(key, iOps, &gocb.MutateInOptions{
-		Expiry:          time.Duration(task.MutateInOptions.Expiry) * time.Second,
-		Cas:             gocb.Cas(task.MutateInOptions.Cas),
-		PersistTo:       task.MutateInOptions.PersistTo,
-		ReplicateTo:     task.MutateInOptions.ReplicateTo,
-		DurabilityLevel: cb_sdk.GetDurability(task.MutateInOptions.Durability),
-		StoreSemantic:   cb_sdk.GetStoreSemantic(task.MutateInOptions.StoreSemantic),
-		Timeout:         time.Duration(task.MutateInOptions.Timeout) * time.Second,
-		PreserveExpiry:  task.MutateInOptions.PreserveExpiry,
-	})
-
-	if err != nil {
-		task.Result.CreateSingleErrorResult(initTime, key, err.Error(), false, 0)
-	} else {
-		if !task.InsertSpecOptions.IsXattr {
-			documentMetaData.IncrementMutationCount()
-		}
-		task.Result.CreateSingleErrorResult(initTime, key, "", true, uint64(result.Cas()))
-	}
-
-	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-}
+//
+//import (
+//	"github.com/couchbase/gocb/v2"
+//	"github.com/couchbaselabs/sirius/internal/cb_sdk"
+//	"github.com/couchbaselabs/sirius/internal/err_sirius"
+//	"github.com/couchbaselabs/sirius/internal/task_result"
+//	"github.com/couchbaselabs/sirius/internal/tasks"
+//	"github.com/couchbaselabs/sirius/internal/template"
+//	"github.com/jaswdr/faker"
+//	"log"
+//	"math/rand"
+//	"strings"
+//	"time"
+//)
+//
+//type SingleSubDocInsert struct {
+//	IdentifierToken             string                       `json:"identifierToken" doc:"true"`
+//	ClusterConfig               *cb_sdk.ClusterConfig        `json:"clusterConfig" doc:"true"`
+//	Bucket                      string                       `json:"bucket" doc:"true"`
+//	Scope                       string                       `json:"scope,omitempty" doc:"true"`
+//	Collection                  string                       `json:"collection,omitempty" doc:"true"`
+//	SingleSubDocOperationConfig *SingleSubDocOperationConfig `json:"singleSubDocOperationConfig" doc:"true"`
+//	InsertSpecOptions           *cb_sdk.InsertSpecOptions    `json:"insertSpecOptions" doc:"true"`
+//	MutateInOptions             *cb_sdk.MutateInOptions      `json:"mutateInOptions" doc:"true"`
+//	Operation                   string                       `json:"operation" doc:"false"`
+//	ResultSeed                  int64                        `json:"resultSeed" doc:"false"`
+//	TaskPending                 bool                         `json:"taskPending" doc:"false"`
+//	Result                      *task_result.TaskResult      `json:"-" doc:"false"`
+//	req                         *tasks.Request               `json:"-" doc:"false"`
+//}
+//
+//func (task *SingleSubDocInsert) Describe() string {
+//	return "SingleSingleSubDocInsert inserts a Sub-Document as per user's input [No Random data]"
+//}
+//
+//func (task *SingleSubDocInsert) MetaDataIdentifier() string {
+//	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
+//	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
+//		task.Collection}, ":")
+//}
+//
+//func (task *SingleSubDocInsert) CheckIfPending() bool {
+//	return task.TaskPending
+//}
+//
+//// Config configures  the insert task
+//func (task *SingleSubDocInsert) Config(req *tasks.Request, reRun bool) (int64, error) {
+//	task.TaskPending = true
+//	task.req = req
+//
+//	if task.req == nil {
+//		task.TaskPending = false
+//		return 0, err_sirius.RequestIsNil
+//	}
+//
+//	task.req.ReconnectionManager()
+//	if _, err_sirius := task.req.GetCluster(task.ClusterConfig); err_sirius != nil {
+//		task.TaskPending = false
+//		return 0, err_sirius
+//	}
+//
+//	task.req.ReconfigureDocumentManager()
+//
+//	if !reRun {
+//		task.ResultSeed = int64(time.Now().UnixNano())
+//		task.Operation = tasks.SingleSubDocInsertOperation
+//
+//		if task.Bucket == "" {
+//			task.Bucket = cb_sdk.DefaultBucket
+//		}
+//		if task.Scope == "" {
+//			task.Scope = cb_sdk.DefaultScope
+//		}
+//		if task.Collection == "" {
+//			task.Collection = cb_sdk.DefaultCollection
+//		}
+//
+//		if err_sirius := ConfigSingleSubDocOperationConfig(task.SingleSubDocOperationConfig); err_sirius != nil {
+//			task.TaskPending = false
+//			return 0, err_sirius
+//		}
+//
+//		if err_sirius := cb_sdk.ConfigInsertSpecOptions(task.InsertSpecOptions); err_sirius != nil {
+//			task.TaskPending = false
+//			return 0, err_sirius
+//		}
+//
+//		if err_sirius := cb_sdk.ConfigMutateInOptions(task.MutateInOptions); err_sirius != nil {
+//			task.TaskPending = false
+//			return 0, err_sirius
+//		}
+//	} else {
+//		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//	}
+//	return task.ResultSeed, nil
+//}
+//
+//func (task *SingleSubDocInsert) TearUp() error {
+//	task.Result.StopStoringResult()
+//	if err_sirius := task.Result.SaveResultIntoFile(); err_sirius != nil {
+//		log.Println("not able to save Result into ", task.ResultSeed, task.Operation)
+//	}
+//	task.Result = nil
+//	task.TaskPending = false
+//	return task.req.SaveRequestIntoFile()
+//}
+//
+//func (task *SingleSubDocInsert) Do() error {
+//
+//	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
+//
+//	collectionObject, err1 := task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope,
+//		task.Collection)
+//
+//	if err1 != nil {
+//		task.Result.ErrorOther = err1.Error()
+//		task.Result.FailWholeSingleOperation([]string{task.SingleSubDocOperationConfig.Key}, err1)
+//		return task.TearUp()
+//	}
+//
+//	singleInsertSubDocuments(task, collectionObject)
+//
+//	task.Result.Success = 1 - task.Result.Failure
+//	return task.TearUp()
+//}
+//
+//// singleInsertSubDocuments uploads new documents in a bucket.scope.collection in a defined batch size at multiple iterations.
+//func singleInsertSubDocuments(task *SingleSubDocInsert, collectionObject *cb_sdk.CollectionObject) {
+//
+//	if task.req.ContextClosed() {
+//		return
+//	}
+//
+//	var iOps []gocb.MutateInSpec
+//	key := task.SingleSubDocOperationConfig.Key
+//	documentMetaData := task.req.DocumentsMeta.GetDocumentsMetadata(task.MetaDataIdentifier(), key, "", 0, false)
+//
+//	for _, path := range task.SingleSubDocOperationConfig.Paths {
+//		subDocument := documentMetaData.SubDocument(path, task.InsertSpecOptions.IsXattr,
+//			task.SingleSubDocOperationConfig.DocSize, false)
+//
+//		fake := faker.NewWithSeed(rand.NewSource(int64(subDocument.Seed)))
+//
+//		value := subDocument.GenerateValue(&fake)
+//
+//		iOps = append(iOps, gocb.InsertSpec(path, value, &gocb.InsertSpecOptions{
+//			CreatePath: task.InsertSpecOptions.CreatePath,
+//			IsXattr:    task.InsertSpecOptions.IsXattr,
+//		}))
+//	}
+//
+//	if !task.InsertSpecOptions.IsXattr {
+//		iOps = append(iOps, gocb.IncrementSpec(template.MutatedPath,
+//			int64(template.MutateFieldIncrement), &gocb.CounterSpecOptions{
+//				CreatePath: true,
+//				IsXattr:    false,
+//			}))
+//	}
+//
+//	initTime := time.Now().UTC().Format(time.RFC850)
+//	result, err_sirius := collectionObject.Collection.MutateIn(key, iOps, &gocb.MutateInOptions{
+//		Expiry:          time.Duration(task.MutateInOptions.Expiry) * time.Second,
+//		Cas:             gocb.Cas(task.MutateInOptions.Cas),
+//		PersistTo:       task.MutateInOptions.PersistTo,
+//		ReplicateTo:     task.MutateInOptions.ReplicateTo,
+//		DurabilityLevel: cb_sdk.GetDurability(task.MutateInOptions.Durability),
+//		StoreSemantic:   cb_sdk.GetStoreSemantic(task.MutateInOptions.StoreSemantic),
+//		Timeout:         time.Duration(task.MutateInOptions.Timeout) * time.Second,
+//		PreserveExpiry:  task.MutateInOptions.PreserveExpiry,
+//	})
+//
+//	if err_sirius != nil {
+//		task.Result.CreateSingleErrorResult(initTime, key, err_sirius.Error(), false, 0)
+//	} else {
+//		if !task.InsertSpecOptions.IsXattr {
+//			documentMetaData.IncrementMutationCount()
+//		}
+//		task.Result.CreateSingleErrorResult(initTime, key, "", true, uint64(result.Cas()))
+//	}
+//
+//	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//}
diff --git a/internal/tasks/key_based_loading_cb/task_single_sub_doc_read.go b/internal/tasks/key_based_loading_cb/task_single_sub_doc_read.go
index 8b30d2f..406a4df 100644
--- a/internal/tasks/key_based_loading_cb/task_single_sub_doc_read.go
+++ b/internal/tasks/key_based_loading_cb/task_single_sub_doc_read.go
@@ -1,167 +1,168 @@
 package key_based_loading_cb
 
-import (
-	"github.com/couchbase/gocb/v2"
-	"github.com/couchbaselabs/sirius/internal/cb_sdk"
-	"github.com/couchbaselabs/sirius/internal/task_errors"
-	"github.com/couchbaselabs/sirius/internal/task_result"
-	"github.com/couchbaselabs/sirius/internal/tasks"
-	"log"
-	"strings"
-	"time"
-)
-
-type SingleSubDocRead struct {
-	IdentifierToken             string                       `json:"identifierToken" doc:"true"`
-	ClusterConfig               *cb_sdk.ClusterConfig        `json:"clusterConfig" doc:"true"`
-	Bucket                      string                       `json:"bucket" doc:"true"`
-	Scope                       string                       `json:"scope,omitempty" doc:"true"`
-	Collection                  string                       `json:"collection,omitempty" doc:"true"`
-	SingleSubDocOperationConfig *SingleSubDocOperationConfig `json:"singleSubDocOperationConfig" doc:"true"`
-	LookupInOptions             *cb_sdk.LookupInOptions      `json:"lookupInOptions" doc:"true"`
-	GetSpecOptions              *cb_sdk.GetSpecOptions       `json:"getSpecOptions" doc:"true"`
-	Operation                   string                       `json:"operation" doc:"false"`
-	ResultSeed                  int64                        `json:"resultSeed" doc:"false"`
-	TaskPending                 bool                         `json:"taskPending" doc:"false"`
-	Result                      *task_result.TaskResult      `json:"-" doc:"false"`
-	req                         *tasks.Request               `json:"-" doc:"false"`
-}
-
-func (task *SingleSubDocRead) Describe() string {
-	return "SingleSingleSubDocRead inserts a Sub-Document as per user's input [No Random data]"
-}
-
-func (task *SingleSubDocRead) CollectionIdentifier() string {
-	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
-	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
-		task.Collection}, ":")
-}
-
-func (task *SingleSubDocRead) CheckIfPending() bool {
-	return task.TaskPending
-}
-
-// Config configures  the insert task
-func (task *SingleSubDocRead) Config(req *tasks.Request, reRun bool) (int64, error) {
-	task.TaskPending = true
-	task.req = req
-
-	if task.req == nil {
-		task.TaskPending = false
-		return 0, task_errors.ErrRequestIsNil
-	}
-
-	task.req.ReconnectionManager()
-	if _, err := task.req.GetCluster(task.ClusterConfig); err != nil {
-		task.TaskPending = false
-		return 0, err
-	}
-
-	if !reRun {
-		task.ResultSeed = int64(time.Now().UnixNano())
-		task.Operation = tasks.SingleSubDocReadOperation
-
-		if task.Bucket == "" {
-			task.Bucket = cb_sdk.DefaultBucket
-		}
-		if task.Scope == "" {
-			task.Scope = cb_sdk.DefaultScope
-		}
-		if task.Collection == "" {
-			task.Collection = cb_sdk.DefaultCollection
-		}
-
-		if err := ConfigSingleSubDocOperationConfig(task.SingleSubDocOperationConfig); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-
-		if err := cb_sdk.ConfigGetSpecOptions(task.GetSpecOptions); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-
-		if err := cb_sdk.ConfigLookupInOptions(task.LookupInOptions); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-
-	} else {
-		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-	}
-	return task.ResultSeed, nil
-}
-
-func (task *SingleSubDocRead) TearUp() error {
-	if err := task.Result.SaveResultIntoFile(); err != nil {
-		log.Println("not able to save Result into ", task.ResultSeed, task.Operation)
-	}
-	task.Result.StopStoringResult()
-	task.Result = nil
-	task.TaskPending = false
-	return task.req.SaveRequestIntoFile()
-}
-
-func (task *SingleSubDocRead) Do() error {
-
-	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
-
-	collectionObject, err1 := task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope,
-		task.Collection)
-
-	if err1 != nil {
-		task.Result.ErrorOther = err1.Error()
-		task.Result.FailWholeSingleOperation([]string{task.SingleSubDocOperationConfig.Key}, err1)
-		return task.TearUp()
-	}
-
-	singleReadSubDocuments(task, collectionObject)
-	task.Result.Success = int64(1) - task.Result.Failure
-	return task.TearUp()
-}
-
-// singleInsertSubDocuments uploads new documents in a bucket.scope.collection in a defined batch size at multiple iterations.
-func singleReadSubDocuments(task *SingleSubDocRead, collectionObject *cb_sdk.CollectionObject) {
-
-	if task.req.ContextClosed() {
-		return
-	}
-
-	var iOps []gocb.LookupInSpec
-	key := task.SingleSubDocOperationConfig.Key
-	documentMetaData := task.req.DocumentsMeta.GetDocumentsMetadata(task.CollectionIdentifier(), key, "", 0, false)
-
-	for _, path := range task.SingleSubDocOperationConfig.Paths {
-
-		documentMetaData.SubDocument(path, task.GetSpecOptions.IsXattr, task.SingleSubDocOperationConfig.DocSize,
-			false)
-
-		iOps = append(iOps, gocb.GetSpec(path, &gocb.GetSpecOptions{
-			IsXattr: task.GetSpecOptions.IsXattr,
-		}))
-	}
-
-	initTime := time.Now().UTC().Format(time.RFC850)
-	result, err := collectionObject.Collection.LookupIn(key, iOps, &gocb.LookupInOptions{
-		Timeout: time.Duration(task.LookupInOptions.Timeout) * time.Second,
-	})
-
-	if err != nil {
-		task.Result.CreateSingleErrorResult(initTime, key, err.Error(), false, 0)
-	} else {
-		flag := true
-		for index := range task.SingleSubDocOperationConfig.Paths {
-			var val interface{}
-			if err := result.ContentAt(uint(index), &val); err != nil {
-				task.Result.Failure++
-				task.Result.CreateSingleErrorResult(initTime, key, err.Error(), false, 0)
-				flag = false
-				break
-			}
-		}
-		if flag {
-			task.Result.CreateSingleErrorResult(initTime, key, "", true, uint64(result.Cas()))
-		}
-	}
-	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-}
+//
+//import (
+//	"github.com/couchbase/gocb/v2"
+//	"github.com/couchbaselabs/sirius/internal/cb_sdk"
+//	"github.com/couchbaselabs/sirius/internal/err_sirius"
+//	"github.com/couchbaselabs/sirius/internal/task_result"
+//	"github.com/couchbaselabs/sirius/internal/tasks"
+//	"log"
+//	"strings"
+//	"time"
+//)
+//
+//type SingleSubDocRead struct {
+//	IdentifierToken             string                       `json:"identifierToken" doc:"true"`
+//	ClusterConfig               *cb_sdk.ClusterConfig        `json:"clusterConfig" doc:"true"`
+//	Bucket                      string                       `json:"bucket" doc:"true"`
+//	Scope                       string                       `json:"scope,omitempty" doc:"true"`
+//	Collection                  string                       `json:"collection,omitempty" doc:"true"`
+//	SingleSubDocOperationConfig *SingleSubDocOperationConfig `json:"singleSubDocOperationConfig" doc:"true"`
+//	LookupInOptions             *cb_sdk.LookupInOptions      `json:"lookupInOptions" doc:"true"`
+//	GetSpecOptions              *cb_sdk.GetSpecOptions       `json:"getSpecOptions" doc:"true"`
+//	Operation                   string                       `json:"operation" doc:"false"`
+//	ResultSeed                  int64                        `json:"resultSeed" doc:"false"`
+//	TaskPending                 bool                         `json:"taskPending" doc:"false"`
+//	Result                      *task_result.TaskResult      `json:"-" doc:"false"`
+//	req                         *tasks.Request               `json:"-" doc:"false"`
+//}
+//
+//func (task *SingleSubDocRead) Describe() string {
+//	return "SingleSingleSubDocRead inserts a Sub-Document as per user's input [No Random data]"
+//}
+//
+//func (task *SingleSubDocRead) MetaDataIdentifier() string {
+//	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
+//	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
+//		task.Collection}, ":")
+//}
+//
+//func (task *SingleSubDocRead) CheckIfPending() bool {
+//	return task.TaskPending
+//}
+//
+//// Config configures  the insert task
+//func (task *SingleSubDocRead) Config(req *tasks.Request, reRun bool) (int64, error) {
+//	task.TaskPending = true
+//	task.req = req
+//
+//	if task.req == nil {
+//		task.TaskPending = false
+//		return 0, err_sirius.RequestIsNil
+//	}
+//
+//	task.req.ReconnectionManager()
+//	if _, err_sirius := task.req.GetCluster(task.ClusterConfig); err_sirius != nil {
+//		task.TaskPending = false
+//		return 0, err_sirius
+//	}
+//
+//	if !reRun {
+//		task.ResultSeed = int64(time.Now().UnixNano())
+//		task.Operation = tasks.SingleSubDocReadOperation
+//
+//		if task.Bucket == "" {
+//			task.Bucket = cb_sdk.DefaultBucket
+//		}
+//		if task.Scope == "" {
+//			task.Scope = cb_sdk.DefaultScope
+//		}
+//		if task.Collection == "" {
+//			task.Collection = cb_sdk.DefaultCollection
+//		}
+//
+//		if err_sirius := ConfigSingleSubDocOperationConfig(task.SingleSubDocOperationConfig); err_sirius != nil {
+//			task.TaskPending = false
+//			return 0, err_sirius
+//		}
+//
+//		if err_sirius := cb_sdk.ConfigGetSpecOptions(task.GetSpecOptions); err_sirius != nil {
+//			task.TaskPending = false
+//			return 0, err_sirius
+//		}
+//
+//		if err_sirius := cb_sdk.ConfigLookupInOptions(task.LookupInOptions); err_sirius != nil {
+//			task.TaskPending = false
+//			return 0, err_sirius
+//		}
+//
+//	} else {
+//		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//	}
+//	return task.ResultSeed, nil
+//}
+//
+//func (task *SingleSubDocRead) TearUp() error {
+//	if err_sirius := task.Result.SaveResultIntoFile(); err_sirius != nil {
+//		log.Println("not able to save Result into ", task.ResultSeed, task.Operation)
+//	}
+//	task.Result.StopStoringResult()
+//	task.Result = nil
+//	task.TaskPending = false
+//	return task.req.SaveRequestIntoFile()
+//}
+//
+//func (task *SingleSubDocRead) Do() error {
+//
+//	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
+//
+//	collectionObject, err1 := task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope,
+//		task.Collection)
+//
+//	if err1 != nil {
+//		task.Result.ErrorOther = err1.Error()
+//		task.Result.FailWholeSingleOperation([]string{task.SingleSubDocOperationConfig.Key}, err1)
+//		return task.TearUp()
+//	}
+//
+//	singleReadSubDocuments(task, collectionObject)
+//	task.Result.Success = int64(1) - task.Result.Failure
+//	return task.TearUp()
+//}
+//
+//// singleInsertSubDocuments uploads new documents in a bucket.scope.collection in a defined batch size at multiple iterations.
+//func singleReadSubDocuments(task *SingleSubDocRead, collectionObject *cb_sdk.CollectionObject) {
+//
+//	if task.req.ContextClosed() {
+//		return
+//	}
+//
+//	var iOps []gocb.LookupInSpec
+//	key := task.SingleSubDocOperationConfig.Key
+//	documentMetaData := task.req.DocumentsMeta.GetDocumentsMetadata(task.MetaDataIdentifier(), key, "", 0, false)
+//
+//	for _, path := range task.SingleSubDocOperationConfig.Paths {
+//
+//		documentMetaData.SubDocument(path, task.GetSpecOptions.IsXattr, task.SingleSubDocOperationConfig.DocSize,
+//			false)
+//
+//		iOps = append(iOps, gocb.GetSpec(path, &gocb.GetSpecOptions{
+//			IsXattr: task.GetSpecOptions.IsXattr,
+//		}))
+//	}
+//
+//	initTime := time.Now().UTC().Format(time.RFC850)
+//	result, err_sirius := collectionObject.Collection.LookupIn(key, iOps, &gocb.LookupInOptions{
+//		Timeout: time.Duration(task.LookupInOptions.Timeout) * time.Second,
+//	})
+//
+//	if err_sirius != nil {
+//		task.Result.CreateSingleErrorResult(initTime, key, err_sirius.Error(), false, 0)
+//	} else {
+//		flag := true
+//		for index := range task.SingleSubDocOperationConfig.Paths {
+//			var val interface{}
+//			if err_sirius := result.ContentAt(uint(index), &val); err_sirius != nil {
+//				task.Result.Failure++
+//				task.Result.CreateSingleErrorResult(initTime, key, err_sirius.Error(), false, 0)
+//				flag = false
+//				break
+//			}
+//		}
+//		if flag {
+//			task.Result.CreateSingleErrorResult(initTime, key, "", true, uint64(result.Cas()))
+//		}
+//	}
+//	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//}
diff --git a/internal/tasks/key_based_loading_cb/task_single_sub_doc_replace.go b/internal/tasks/key_based_loading_cb/task_single_sub_doc_replace.go
index ec232c7..f7b5f39 100644
--- a/internal/tasks/key_based_loading_cb/task_single_sub_doc_replace.go
+++ b/internal/tasks/key_based_loading_cb/task_single_sub_doc_replace.go
@@ -1,180 +1,181 @@
 package key_based_loading_cb
 
-import (
-	"github.com/couchbase/gocb/v2"
-	"github.com/couchbaselabs/sirius/internal/cb_sdk"
-	"github.com/couchbaselabs/sirius/internal/task_errors"
-	"github.com/couchbaselabs/sirius/internal/task_result"
-	"github.com/couchbaselabs/sirius/internal/tasks"
-	"github.com/couchbaselabs/sirius/internal/template"
-	"github.com/jaswdr/faker"
-	"log"
-	"math/rand"
-	"strings"
-	"time"
-)
-
-type SingleSubDocReplace struct {
-	IdentifierToken             string                       `json:"identifierToken" doc:"true"`
-	ClusterConfig               *cb_sdk.ClusterConfig        `json:"clusterConfig" doc:"true"`
-	Bucket                      string                       `json:"bucket" doc:"true"`
-	Scope                       string                       `json:"scope,omitempty" doc:"true"`
-	Collection                  string                       `json:"collection,omitempty" doc:"true"`
-	SingleSubDocOperationConfig *SingleSubDocOperationConfig `json:"singleSubDocOperationConfig" doc:"true"`
-	ReplaceSpecOptions          *cb_sdk.ReplaceSpecOptions   `json:"replaceSpecOptions" doc:"true"`
-	MutateInOptions             *cb_sdk.MutateInOptions      `json:"mutateInOptions" doc:"true"`
-	Operation                   string                       `json:"operation" doc:"false"`
-	ResultSeed                  int64                        `json:"resultSeed" doc:"false"`
-	TaskPending                 bool                         `json:"taskPending" doc:"false"`
-	Result                      *task_result.TaskResult      `json:"-" doc:"false"`
-	req                         *tasks.Request               `json:"-" doc:"false"`
-}
-
-func (task *SingleSubDocReplace) Describe() string {
-	return "SingleSingleSubDocReplace inserts a Sub-Document as per user's input [No Random data]"
-}
-
-func (task *SingleSubDocReplace) CollectionIdentifier() string {
-	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
-	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
-		task.Collection}, ":")
-}
-
-func (task *SingleSubDocReplace) CheckIfPending() bool {
-	return task.TaskPending
-}
-
-// Config configures  the insert task
-func (task *SingleSubDocReplace) Config(req *tasks.Request, reRun bool) (int64, error) {
-	task.TaskPending = true
-	task.req = req
-
-	if task.req == nil {
-		task.TaskPending = false
-		return 0, task_errors.ErrRequestIsNil
-	}
-
-	task.req.ReconnectionManager()
-	if _, err := task.req.GetCluster(task.ClusterConfig); err != nil {
-		task.TaskPending = false
-		return 0, err
-	}
-
-	if !reRun {
-		task.ResultSeed = int64(time.Now().UnixNano())
-		task.Operation = tasks.SingleSubDocReplaceOperation
-
-		if task.Bucket == "" {
-			task.Bucket = cb_sdk.DefaultBucket
-		}
-		if task.Scope == "" {
-			task.Scope = cb_sdk.DefaultScope
-		}
-		if task.Collection == "" {
-			task.Collection = cb_sdk.DefaultCollection
-		}
-
-		if err := ConfigSingleSubDocOperationConfig(task.SingleSubDocOperationConfig); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-
-		if err := cb_sdk.ConfigReplaceSpecOptions(task.ReplaceSpecOptions); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-
-		if err := cb_sdk.ConfigMutateInOptions(task.MutateInOptions); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-	} else {
-		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-	}
-	return task.ResultSeed, nil
-}
-
-func (task *SingleSubDocReplace) TearUp() error {
-	task.Result.StopStoringResult()
-	if err := task.Result.SaveResultIntoFile(); err != nil {
-		log.Println("not able to save Result into ", task.ResultSeed, task.Operation)
-	}
-	task.Result = nil
-	task.TaskPending = false
-	return task.req.SaveRequestIntoFile()
-}
-
-func (task *SingleSubDocReplace) Do() error {
-
-	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
-
-	collectionObject, err1 := task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope,
-		task.Collection)
-
-	if err1 != nil {
-		task.Result.ErrorOther = err1.Error()
-		task.Result.FailWholeSingleOperation([]string{task.SingleSubDocOperationConfig.Key}, err1)
-		return task.TearUp()
-	}
-
-	singleReplaceSubDocuments(task, collectionObject)
-
-	task.Result.Success = int64(1) - task.Result.Failure
-	return task.TearUp()
-}
-
-// singleReplaceSubDocuments uploads new documents in a bucket.scope.collection in a defined batch size at multiple iterations.
-func singleReplaceSubDocuments(task *SingleSubDocReplace, collectionObject *cb_sdk.CollectionObject) {
-
-	if task.req.ContextClosed() {
-		return
-	}
-
-	var iOps []gocb.MutateInSpec
-	key := task.SingleSubDocOperationConfig.Key
-	documentMetaData := task.req.DocumentsMeta.GetDocumentsMetadata(task.CollectionIdentifier(), key, "", 0, false)
-
-	for _, path := range task.SingleSubDocOperationConfig.Paths {
-		subDocument := documentMetaData.SubDocument(path, task.ReplaceSpecOptions.IsXattr, task.SingleSubDocOperationConfig.
-			DocSize, true)
-
-		fake := faker.NewWithSeed(rand.NewSource(int64(subDocument.Seed)))
-
-		value := subDocument.GenerateValue(&fake)
-
-		iOps = append(iOps, gocb.ReplaceSpec(path, value, &gocb.ReplaceSpecOptions{
-			IsXattr: task.ReplaceSpecOptions.IsXattr,
-		}))
-	}
-
-	if !task.ReplaceSpecOptions.IsXattr {
-		iOps = append(iOps, gocb.IncrementSpec(template.MutatedPath,
-			int64(template.MutateFieldIncrement), &gocb.CounterSpecOptions{
-				CreatePath: true,
-				IsXattr:    false,
-			}))
-	}
-
-	initTime := time.Now().UTC().Format(time.RFC850)
-	result, err := collectionObject.Collection.MutateIn(key, iOps, &gocb.MutateInOptions{
-		Expiry:          time.Duration(task.MutateInOptions.Expiry) * time.Second,
-		Cas:             gocb.Cas(task.MutateInOptions.Cas),
-		PersistTo:       task.MutateInOptions.PersistTo,
-		ReplicateTo:     task.MutateInOptions.ReplicateTo,
-		DurabilityLevel: cb_sdk.GetDurability(task.MutateInOptions.Durability),
-		StoreSemantic:   cb_sdk.GetStoreSemantic(task.MutateInOptions.StoreSemantic),
-		Timeout:         time.Duration(task.MutateInOptions.Timeout) * time.Second,
-		PreserveExpiry:  task.MutateInOptions.PreserveExpiry,
-	})
-
-	if err != nil {
-		task.Result.CreateSingleErrorResult(initTime, key, err.Error(), false, 0)
-	} else {
-		if !task.ReplaceSpecOptions.IsXattr {
-			documentMetaData.IncrementMutationCount()
-		}
-		task.Result.CreateSingleErrorResult(initTime, key, "", true, uint64(result.Cas()))
-	}
-
-	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-}
+//
+//import (
+//	"github.com/couchbase/gocb/v2"
+//	"github.com/couchbaselabs/sirius/internal/cb_sdk"
+//	"github.com/couchbaselabs/sirius/internal/err_sirius"
+//	"github.com/couchbaselabs/sirius/internal/task_result"
+//	"github.com/couchbaselabs/sirius/internal/tasks"
+//	"github.com/couchbaselabs/sirius/internal/template"
+//	"github.com/jaswdr/faker"
+//	"log"
+//	"math/rand"
+//	"strings"
+//	"time"
+//)
+//
+//type SingleSubDocReplace struct {
+//	IdentifierToken             string                       `json:"identifierToken" doc:"true"`
+//	ClusterConfig               *cb_sdk.ClusterConfig        `json:"clusterConfig" doc:"true"`
+//	Bucket                      string                       `json:"bucket" doc:"true"`
+//	Scope                       string                       `json:"scope,omitempty" doc:"true"`
+//	Collection                  string                       `json:"collection,omitempty" doc:"true"`
+//	SingleSubDocOperationConfig *SingleSubDocOperationConfig `json:"singleSubDocOperationConfig" doc:"true"`
+//	ReplaceSpecOptions          *cb_sdk.ReplaceSpecOptions   `json:"replaceSpecOptions" doc:"true"`
+//	MutateInOptions             *cb_sdk.MutateInOptions      `json:"mutateInOptions" doc:"true"`
+//	Operation                   string                       `json:"operation" doc:"false"`
+//	ResultSeed                  int64                        `json:"resultSeed" doc:"false"`
+//	TaskPending                 bool                         `json:"taskPending" doc:"false"`
+//	Result                      *task_result.TaskResult      `json:"-" doc:"false"`
+//	req                         *tasks.Request               `json:"-" doc:"false"`
+//}
+//
+//func (task *SingleSubDocReplace) Describe() string {
+//	return "SingleSingleSubDocReplace inserts a Sub-Document as per user's input [No Random data]"
+//}
+//
+//func (task *SingleSubDocReplace) MetaDataIdentifier() string {
+//	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
+//	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
+//		task.Collection}, ":")
+//}
+//
+//func (task *SingleSubDocReplace) CheckIfPending() bool {
+//	return task.TaskPending
+//}
+//
+//// Config configures  the insert task
+//func (task *SingleSubDocReplace) Config(req *tasks.Request, reRun bool) (int64, error) {
+//	task.TaskPending = true
+//	task.req = req
+//
+//	if task.req == nil {
+//		task.TaskPending = false
+//		return 0, err_sirius.RequestIsNil
+//	}
+//
+//	task.req.ReconnectionManager()
+//	if _, err_sirius := task.req.GetCluster(task.ClusterConfig); err_sirius != nil {
+//		task.TaskPending = false
+//		return 0, err_sirius
+//	}
+//
+//	if !reRun {
+//		task.ResultSeed = int64(time.Now().UnixNano())
+//		task.Operation = tasks.SingleSubDocReplaceOperation
+//
+//		if task.Bucket == "" {
+//			task.Bucket = cb_sdk.DefaultBucket
+//		}
+//		if task.Scope == "" {
+//			task.Scope = cb_sdk.DefaultScope
+//		}
+//		if task.Collection == "" {
+//			task.Collection = cb_sdk.DefaultCollection
+//		}
+//
+//		if err_sirius := ConfigSingleSubDocOperationConfig(task.SingleSubDocOperationConfig); err_sirius != nil {
+//			task.TaskPending = false
+//			return 0, err_sirius
+//		}
+//
+//		if err_sirius := cb_sdk.ConfigReplaceSpecOptions(task.ReplaceSpecOptions); err_sirius != nil {
+//			task.TaskPending = false
+//			return 0, err_sirius
+//		}
+//
+//		if err_sirius := cb_sdk.ConfigMutateInOptions(task.MutateInOptions); err_sirius != nil {
+//			task.TaskPending = false
+//			return 0, err_sirius
+//		}
+//	} else {
+//		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//	}
+//	return task.ResultSeed, nil
+//}
+//
+//func (task *SingleSubDocReplace) TearUp() error {
+//	task.Result.StopStoringResult()
+//	if err_sirius := task.Result.SaveResultIntoFile(); err_sirius != nil {
+//		log.Println("not able to save Result into ", task.ResultSeed, task.Operation)
+//	}
+//	task.Result = nil
+//	task.TaskPending = false
+//	return task.req.SaveRequestIntoFile()
+//}
+//
+//func (task *SingleSubDocReplace) Do() error {
+//
+//	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
+//
+//	collectionObject, err1 := task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope,
+//		task.Collection)
+//
+//	if err1 != nil {
+//		task.Result.ErrorOther = err1.Error()
+//		task.Result.FailWholeSingleOperation([]string{task.SingleSubDocOperationConfig.Key}, err1)
+//		return task.TearUp()
+//	}
+//
+//	singleReplaceSubDocuments(task, collectionObject)
+//
+//	task.Result.Success = int64(1) - task.Result.Failure
+//	return task.TearUp()
+//}
+//
+//// singleReplaceSubDocuments uploads new documents in a bucket.scope.collection in a defined batch size at multiple iterations.
+//func singleReplaceSubDocuments(task *SingleSubDocReplace, collectionObject *cb_sdk.CollectionObject) {
+//
+//	if task.req.ContextClosed() {
+//		return
+//	}
+//
+//	var iOps []gocb.MutateInSpec
+//	key := task.SingleSubDocOperationConfig.Key
+//	documentMetaData := task.req.DocumentsMeta.GetDocumentsMetadata(task.MetaDataIdentifier(), key, "", 0, false)
+//
+//	for _, path := range task.SingleSubDocOperationConfig.Paths {
+//		subDocument := documentMetaData.SubDocument(path, task.ReplaceSpecOptions.IsXattr, task.SingleSubDocOperationConfig.
+//			DocSize, true)
+//
+//		fake := faker.NewWithSeed(rand.NewSource(int64(subDocument.Seed)))
+//
+//		value := subDocument.GenerateValue(&fake)
+//
+//		iOps = append(iOps, gocb.ReplaceSpec(path, value, &gocb.ReplaceSpecOptions{
+//			IsXattr: task.ReplaceSpecOptions.IsXattr,
+//		}))
+//	}
+//
+//	if !task.ReplaceSpecOptions.IsXattr {
+//		iOps = append(iOps, gocb.IncrementSpec(template.MutatedPath,
+//			int64(template.MutateFieldIncrement), &gocb.CounterSpecOptions{
+//				CreatePath: true,
+//				IsXattr:    false,
+//			}))
+//	}
+//
+//	initTime := time.Now().UTC().Format(time.RFC850)
+//	result, err_sirius := collectionObject.Collection.MutateIn(key, iOps, &gocb.MutateInOptions{
+//		Expiry:          time.Duration(task.MutateInOptions.Expiry) * time.Second,
+//		Cas:             gocb.Cas(task.MutateInOptions.Cas),
+//		PersistTo:       task.MutateInOptions.PersistTo,
+//		ReplicateTo:     task.MutateInOptions.ReplicateTo,
+//		DurabilityLevel: cb_sdk.GetDurability(task.MutateInOptions.Durability),
+//		StoreSemantic:   cb_sdk.GetStoreSemantic(task.MutateInOptions.StoreSemantic),
+//		Timeout:         time.Duration(task.MutateInOptions.Timeout) * time.Second,
+//		PreserveExpiry:  task.MutateInOptions.PreserveExpiry,
+//	})
+//
+//	if err_sirius != nil {
+//		task.Result.CreateSingleErrorResult(initTime, key, err_sirius.Error(), false, 0)
+//	} else {
+//		if !task.ReplaceSpecOptions.IsXattr {
+//			documentMetaData.IncrementMutationCount()
+//		}
+//		task.Result.CreateSingleErrorResult(initTime, key, "", true, uint64(result.Cas()))
+//	}
+//
+//	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//}
diff --git a/internal/tasks/key_based_loading_cb/task_single_sub_doc_upsert.go b/internal/tasks/key_based_loading_cb/task_single_sub_doc_upsert.go
index 2b58846..7066aa8 100644
--- a/internal/tasks/key_based_loading_cb/task_single_sub_doc_upsert.go
+++ b/internal/tasks/key_based_loading_cb/task_single_sub_doc_upsert.go
@@ -1,193 +1,194 @@
 package key_based_loading_cb
 
-import (
-	"github.com/couchbase/gocb/v2"
-	"github.com/couchbaselabs/sirius/internal/cb_sdk"
-	"github.com/couchbaselabs/sirius/internal/task_errors"
-	"github.com/couchbaselabs/sirius/internal/task_result"
-	"github.com/couchbaselabs/sirius/internal/tasks"
-	"github.com/couchbaselabs/sirius/internal/template"
-	"github.com/jaswdr/faker"
-	"log"
-	"math/rand"
-	"strings"
-	"time"
-)
-
-type SingleSubDocUpsert struct {
-	IdentifierToken             string                       `json:"identifierToken" doc:"true"`
-	ClusterConfig               *cb_sdk.ClusterConfig        `json:"clusterConfig" doc:"true"`
-	Bucket                      string                       `json:"bucket" doc:"true"`
-	Scope                       string                       `json:"scope,omitempty" doc:"true"`
-	Collection                  string                       `json:"collection,omitempty" doc:"true"`
-	SingleSubDocOperationConfig *SingleSubDocOperationConfig `json:"singleSubDocOperationConfig" doc:"true"`
-	InsertSpecOptions           *cb_sdk.InsertSpecOptions    `json:"insertSpecOptions" doc:"true"`
-	MutateInOptions             *cb_sdk.MutateInOptions      `json:"mutateInOptions" doc:"true"`
-	Operation                   string                       `json:"operation" doc:"false"`
-	ResultSeed                  int64                        `json:"resultSeed" doc:"false"`
-	TaskPending                 bool                         `json:"taskPending" doc:"false"`
-	Result                      *task_result.TaskResult      `json:"-" doc:"false"`
-	req                         *tasks.Request               `json:"-" doc:"false"`
-}
-
-func (task *SingleSubDocUpsert) Describe() string {
-	return "SingleSingleSubDocUpsert inserts a Sub-Document as per user's input [No Random data]"
-}
-
-func (task *SingleSubDocUpsert) CollectionIdentifier() string {
-	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
-	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
-		task.Collection}, ":")
-}
-
-func (task *SingleSubDocUpsert) CheckIfPending() bool {
-	return task.TaskPending
-}
-
-// Config configures  the insert task
-func (task *SingleSubDocUpsert) Config(req *tasks.Request, reRun bool) (int64, error) {
-	task.TaskPending = true
-	task.req = req
-
-	if task.req == nil {
-		task.TaskPending = false
-		return 0, task_errors.ErrRequestIsNil
-	}
-
-	task.req.ReconnectionManager()
-	if _, err := task.req.GetCluster(task.ClusterConfig); err != nil {
-		task.TaskPending = false
-		return 0, err
-	}
-
-	task.req.ReconfigureDocumentManager()
-
-	if !reRun {
-		task.ResultSeed = int64(time.Now().UnixNano())
-		task.Operation = tasks.SingleSubDocUpsertOperation
-
-		if task.Bucket == "" {
-			task.Bucket = cb_sdk.DefaultBucket
-		}
-		if task.Scope == "" {
-			task.Scope = cb_sdk.DefaultScope
-		}
-		if task.Collection == "" {
-			task.Collection = cb_sdk.DefaultCollection
-		}
-
-		if err := ConfigSingleSubDocOperationConfig(task.SingleSubDocOperationConfig); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-
-		if err := cb_sdk.ConfigInsertSpecOptions(task.InsertSpecOptions); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-
-		if err := cb_sdk.ConfigMutateInOptions(task.MutateInOptions); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-	} else {
-		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-	}
-	return task.ResultSeed, nil
-}
-
-func (task *SingleSubDocUpsert) TearUp() error {
-	task.Result.StopStoringResult()
-	if err := task.Result.SaveResultIntoFile(); err != nil {
-		log.Println("not able to save Result into ", task.ResultSeed, task.Operation)
-	}
-	task.Result = nil
-	task.TaskPending = false
-	return task.req.SaveRequestIntoFile()
-}
-
-func (task *SingleSubDocUpsert) Do() error {
-
-	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
-
-	collectionObject, err1 := task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope,
-		task.Collection)
-
-	if err1 != nil {
-		task.Result.ErrorOther = err1.Error()
-		task.Result.FailWholeSingleOperation([]string{task.SingleSubDocOperationConfig.Key}, err1)
-		return task.TearUp()
-	}
-
-	singleUpsertSubDocuments(task, collectionObject)
-
-	task.Result.Success = int64(1) - task.Result.Failure
-	return task.TearUp()
-}
-
-// singleInsertSubDocuments uploads new documents in a bucket.scope.collection in a defined batch size at multiple iterations.
-func singleUpsertSubDocuments(task *SingleSubDocUpsert, collectionObject *cb_sdk.CollectionObject) {
-
-	if task.req.ContextClosed() {
-		return
-	}
-
-	var iOps []gocb.MutateInSpec
-	key := task.SingleSubDocOperationConfig.Key
-	documentMetaData := task.req.DocumentsMeta.GetDocumentsMetadata(task.CollectionIdentifier(), key, "", 0, false)
-
-	for _, path := range task.SingleSubDocOperationConfig.Paths {
-		subDocument := documentMetaData.SubDocument(path, task.InsertSpecOptions.IsXattr,
-			task.SingleSubDocOperationConfig.DocSize,
-			false)
-
-		fake := faker.NewWithSeed(rand.NewSource(int64(subDocument.Seed)))
-
-		value := subDocument.GenerateValue(&fake)
-
-		value = subDocument.RetracePreviousMutations(value, &fake)
-
-		value = subDocument.UpdateValue(value, &fake)
-
-		iOps = append(iOps, gocb.UpsertSpec(path, value, &gocb.UpsertSpecOptions{
-			CreatePath: task.InsertSpecOptions.CreatePath,
-			IsXattr:    task.InsertSpecOptions.IsXattr,
-		}))
-	}
-
-	if !task.InsertSpecOptions.IsXattr {
-		iOps = append(iOps, gocb.IncrementSpec(template.MutatedPath,
-			int64(template.MutateFieldIncrement), &gocb.CounterSpecOptions{
-				CreatePath: true,
-				IsXattr:    false,
-			}))
-	}
-
-	initTime := time.Now().UTC().Format(time.RFC850)
-	result, err := collectionObject.Collection.MutateIn(key, iOps, &gocb.MutateInOptions{
-		Expiry:          time.Duration(task.MutateInOptions.Expiry) * time.Second,
-		Cas:             gocb.Cas(task.MutateInOptions.Cas),
-		PersistTo:       task.MutateInOptions.PersistTo,
-		ReplicateTo:     task.MutateInOptions.ReplicateTo,
-		DurabilityLevel: cb_sdk.GetDurability(task.MutateInOptions.Durability),
-		StoreSemantic:   cb_sdk.GetStoreSemantic(task.MutateInOptions.StoreSemantic),
-		Timeout:         time.Duration(task.MutateInOptions.Timeout) * time.Second,
-		PreserveExpiry:  task.MutateInOptions.PreserveExpiry,
-	})
-
-	if err != nil {
-		for _, path := range task.SingleSubDocOperationConfig.Paths {
-			subDocument := documentMetaData.SubDocument(path, task.InsertSpecOptions.IsXattr,
-				task.SingleSubDocOperationConfig.DocSize, false)
-			subDocument.DecrementCount()
-		}
-		task.Result.CreateSingleErrorResult(initTime, key, err.Error(), false, 0)
-	} else {
-		if !task.InsertSpecOptions.IsXattr {
-			documentMetaData.IncrementMutationCount()
-		}
-		task.Result.CreateSingleErrorResult(initTime, key, "", true, uint64(result.Cas()))
-	}
-
-	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-}
+//
+//import (
+//	"github.com/couchbase/gocb/v2"
+//	"github.com/couchbaselabs/sirius/internal/cb_sdk"
+//	"github.com/couchbaselabs/sirius/internal/err_sirius"
+//	"github.com/couchbaselabs/sirius/internal/task_result"
+//	"github.com/couchbaselabs/sirius/internal/tasks"
+//	"github.com/couchbaselabs/sirius/internal/template"
+//	"github.com/jaswdr/faker"
+//	"log"
+//	"math/rand"
+//	"strings"
+//	"time"
+//)
+//
+//type SingleSubDocUpsert struct {
+//	IdentifierToken             string                       `json:"identifierToken" doc:"true"`
+//	ClusterConfig               *cb_sdk.ClusterConfig        `json:"clusterConfig" doc:"true"`
+//	Bucket                      string                       `json:"bucket" doc:"true"`
+//	Scope                       string                       `json:"scope,omitempty" doc:"true"`
+//	Collection                  string                       `json:"collection,omitempty" doc:"true"`
+//	SingleSubDocOperationConfig *SingleSubDocOperationConfig `json:"singleSubDocOperationConfig" doc:"true"`
+//	InsertSpecOptions           *cb_sdk.InsertSpecOptions    `json:"insertSpecOptions" doc:"true"`
+//	MutateInOptions             *cb_sdk.MutateInOptions      `json:"mutateInOptions" doc:"true"`
+//	Operation                   string                       `json:"operation" doc:"false"`
+//	ResultSeed                  int64                        `json:"resultSeed" doc:"false"`
+//	TaskPending                 bool                         `json:"taskPending" doc:"false"`
+//	Result                      *task_result.TaskResult      `json:"-" doc:"false"`
+//	req                         *tasks.Request               `json:"-" doc:"false"`
+//}
+//
+//func (task *SingleSubDocUpsert) Describe() string {
+//	return "SingleSingleSubDocUpsert inserts a Sub-Document as per user's input [No Random data]"
+//}
+//
+//func (task *SingleSubDocUpsert) MetaDataIdentifier() string {
+//	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
+//	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
+//		task.Collection}, ":")
+//}
+//
+//func (task *SingleSubDocUpsert) CheckIfPending() bool {
+//	return task.TaskPending
+//}
+//
+//// Config configures  the insert task
+//func (task *SingleSubDocUpsert) Config(req *tasks.Request, reRun bool) (int64, error) {
+//	task.TaskPending = true
+//	task.req = req
+//
+//	if task.req == nil {
+//		task.TaskPending = false
+//		return 0, err_sirius.RequestIsNil
+//	}
+//
+//	task.req.ReconnectionManager()
+//	if _, err_sirius := task.req.GetCluster(task.ClusterConfig); err_sirius != nil {
+//		task.TaskPending = false
+//		return 0, err_sirius
+//	}
+//
+//	task.req.ReconfigureDocumentManager()
+//
+//	if !reRun {
+//		task.ResultSeed = int64(time.Now().UnixNano())
+//		task.Operation = tasks.SingleSubDocUpsertOperation
+//
+//		if task.Bucket == "" {
+//			task.Bucket = cb_sdk.DefaultBucket
+//		}
+//		if task.Scope == "" {
+//			task.Scope = cb_sdk.DefaultScope
+//		}
+//		if task.Collection == "" {
+//			task.Collection = cb_sdk.DefaultCollection
+//		}
+//
+//		if err_sirius := ConfigSingleSubDocOperationConfig(task.SingleSubDocOperationConfig); err_sirius != nil {
+//			task.TaskPending = false
+//			return 0, err_sirius
+//		}
+//
+//		if err_sirius := cb_sdk.ConfigInsertSpecOptions(task.InsertSpecOptions); err_sirius != nil {
+//			task.TaskPending = false
+//			return 0, err_sirius
+//		}
+//
+//		if err_sirius := cb_sdk.ConfigMutateInOptions(task.MutateInOptions); err_sirius != nil {
+//			task.TaskPending = false
+//			return 0, err_sirius
+//		}
+//	} else {
+//		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//	}
+//	return task.ResultSeed, nil
+//}
+//
+//func (task *SingleSubDocUpsert) TearUp() error {
+//	task.Result.StopStoringResult()
+//	if err_sirius := task.Result.SaveResultIntoFile(); err_sirius != nil {
+//		log.Println("not able to save Result into ", task.ResultSeed, task.Operation)
+//	}
+//	task.Result = nil
+//	task.TaskPending = false
+//	return task.req.SaveRequestIntoFile()
+//}
+//
+//func (task *SingleSubDocUpsert) Do() error {
+//
+//	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
+//
+//	collectionObject, err1 := task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope,
+//		task.Collection)
+//
+//	if err1 != nil {
+//		task.Result.ErrorOther = err1.Error()
+//		task.Result.FailWholeSingleOperation([]string{task.SingleSubDocOperationConfig.Key}, err1)
+//		return task.TearUp()
+//	}
+//
+//	singleUpsertSubDocuments(task, collectionObject)
+//
+//	task.Result.Success = int64(1) - task.Result.Failure
+//	return task.TearUp()
+//}
+//
+//// singleInsertSubDocuments uploads new documents in a bucket.scope.collection in a defined batch size at multiple iterations.
+//func singleUpsertSubDocuments(task *SingleSubDocUpsert, collectionObject *cb_sdk.CollectionObject) {
+//
+//	if task.req.ContextClosed() {
+//		return
+//	}
+//
+//	var iOps []gocb.MutateInSpec
+//	key := task.SingleSubDocOperationConfig.Key
+//	documentMetaData := task.req.DocumentsMeta.GetDocumentsMetadata(task.MetaDataIdentifier(), key, "", 0, false)
+//
+//	for _, path := range task.SingleSubDocOperationConfig.Paths {
+//		subDocument := documentMetaData.SubDocument(path, task.InsertSpecOptions.IsXattr,
+//			task.SingleSubDocOperationConfig.DocSize,
+//			false)
+//
+//		fake := faker.NewWithSeed(rand.NewSource(int64(subDocument.Seed)))
+//
+//		value := subDocument.GenerateValue(&fake)
+//
+//		value = subDocument.RetracePreviousMutations(value, &fake)
+//
+//		value = subDocument.UpdateValue(value, &fake)
+//
+//		iOps = append(iOps, gocb.UpsertSpec(path, value, &gocb.UpsertSpecOptions{
+//			CreatePath: task.InsertSpecOptions.CreatePath,
+//			IsXattr:    task.InsertSpecOptions.IsXattr,
+//		}))
+//	}
+//
+//	if !task.InsertSpecOptions.IsXattr {
+//		iOps = append(iOps, gocb.IncrementSpec(template.MutatedPath,
+//			int64(template.MutateFieldIncrement), &gocb.CounterSpecOptions{
+//				CreatePath: true,
+//				IsXattr:    false,
+//			}))
+//	}
+//
+//	initTime := time.Now().UTC().Format(time.RFC850)
+//	result, err_sirius := collectionObject.Collection.MutateIn(key, iOps, &gocb.MutateInOptions{
+//		Expiry:          time.Duration(task.MutateInOptions.Expiry) * time.Second,
+//		Cas:             gocb.Cas(task.MutateInOptions.Cas),
+//		PersistTo:       task.MutateInOptions.PersistTo,
+//		ReplicateTo:     task.MutateInOptions.ReplicateTo,
+//		DurabilityLevel: cb_sdk.GetDurability(task.MutateInOptions.Durability),
+//		StoreSemantic:   cb_sdk.GetStoreSemantic(task.MutateInOptions.StoreSemantic),
+//		Timeout:         time.Duration(task.MutateInOptions.Timeout) * time.Second,
+//		PreserveExpiry:  task.MutateInOptions.PreserveExpiry,
+//	})
+//
+//	if err_sirius != nil {
+//		for _, path := range task.SingleSubDocOperationConfig.Paths {
+//			subDocument := documentMetaData.SubDocument(path, task.InsertSpecOptions.IsXattr,
+//				task.SingleSubDocOperationConfig.DocSize, false)
+//			subDocument.DecrementCount()
+//		}
+//		task.Result.CreateSingleErrorResult(initTime, key, err_sirius.Error(), false, 0)
+//	} else {
+//		if !task.InsertSpecOptions.IsXattr {
+//			documentMetaData.IncrementMutationCount()
+//		}
+//		task.Result.CreateSingleErrorResult(initTime, key, "", true, uint64(result.Cas()))
+//	}
+//
+//	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//}
diff --git a/internal/tasks/key_based_loading_cb/task_single_touch.go b/internal/tasks/key_based_loading_cb/task_single_touch.go
index 039a6c4..e7f0a8a 100644
--- a/internal/tasks/key_based_loading_cb/task_single_touch.go
+++ b/internal/tasks/key_based_loading_cb/task_single_touch.go
@@ -1,172 +1,173 @@
 package key_based_loading_cb
 
-import (
-	"github.com/couchbase/gocb/v2"
-	"github.com/couchbaselabs/sirius/internal/cb_sdk"
-	"github.com/couchbaselabs/sirius/internal/task_errors"
-	"github.com/couchbaselabs/sirius/internal/task_result"
-	"github.com/couchbaselabs/sirius/internal/tasks"
-	"golang.org/x/sync/errgroup"
-	"log"
-	"strings"
-	"time"
-)
-
-type SingleTouchTask struct {
-	IdentifierToken       string                  `json:"identifierToken" doc:"true"`
-	ClusterConfig         *cb_sdk.ClusterConfig   `json:"clusterConfig" doc:"true"`
-	Bucket                string                  `json:"bucket" doc:"true"`
-	Scope                 string                  `json:"scope,omitempty" doc:"true"`
-	Collection            string                  `json:"collection,omitempty" doc:"true"`
-	InsertOptions         *cb_sdk.InsertOptions   `json:"insertOptions,omitempty" doc:"true"`
-	SingleOperationConfig *SingleOperationConfig  `json:"singleOperationConfig" doc:"true"`
-	Operation             string                  `json:"operation" doc:"false"`
-	ResultSeed            int64                   `json:"resultSeed" doc:"false"`
-	TaskPending           bool                    `json:"taskPending" doc:"false"`
-	Result                *task_result.TaskResult `json:"-" doc:"false"`
-	req                   *tasks.Request          `json:"-" doc:"false"`
-}
-
-func (task *SingleTouchTask) Describe() string {
-	return "Single touch task specifies a new expiry time for a document in Couchbase.\n"
-}
-
-func (task *SingleTouchTask) CollectionIdentifier() string {
-	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
-	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
-		task.Collection}, ":")
-}
-
-func (task *SingleTouchTask) CheckIfPending() bool {
-	return task.TaskPending
-}
-
-// Config configures  the insert task
-func (task *SingleTouchTask) Config(req *tasks.Request, reRun bool) (int64, error) {
-	task.TaskPending = true
-	task.req = req
-
-	if task.req == nil {
-		task.TaskPending = false
-		return 0, task_errors.ErrRequestIsNil
-	}
-
-	task.req.ReconnectionManager()
-	if _, err := task.req.GetCluster(task.ClusterConfig); err != nil {
-		task.TaskPending = false
-		return 0, err
-	}
-
-	task.req.ReconfigureDocumentManager()
-
-	if !reRun {
-		task.ResultSeed = int64(time.Now().UnixNano())
-		task.Operation = tasks.SingleTouchOperation
-
-		if task.Bucket == "" {
-			task.Bucket = cb_sdk.DefaultBucket
-		}
-		if task.Scope == "" {
-			task.Scope = cb_sdk.DefaultScope
-		}
-		if task.Collection == "" {
-			task.Collection = cb_sdk.DefaultCollection
-		}
-
-		if err := cb_sdk.ConfigInsertOptions(task.InsertOptions); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-
-		if err := ConfigSingleOperationConfig(task.SingleOperationConfig); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-	} else {
-		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-	}
-	return task.ResultSeed, nil
-}
-
-func (task *SingleTouchTask) TearUp() error {
-	task.Result.StopStoringResult()
-	if err := task.Result.SaveResultIntoFile(); err != nil {
-		log.Println("not able to save Result into ", task.ResultSeed)
-	}
-	task.Result = nil
-	task.TaskPending = false
-	return task.req.SaveRequestIntoFile()
-}
-
-func (task *SingleTouchTask) Do() error {
-
-	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
-
-	collectionObject, err1 := task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope,
-		task.Collection)
-
-	if err1 != nil {
-		task.Result.ErrorOther = err1.Error()
-		task.Result.FailWholeSingleOperation(task.SingleOperationConfig.Keys, err1)
-		return task.TearUp()
-	}
-
-	singleTouchDocuments(task, collectionObject)
-
-	task.Result.Success = int64(len(task.SingleOperationConfig.Keys)) - task.Result.Failure
-	return task.TearUp()
-}
-
-// singleTouchDocuments uploads new documents in a bucket.scope.
-// collection in a defined batch size at multiple iterations.
-func singleTouchDocuments(task *SingleTouchTask, collectionObject *cb_sdk.CollectionObject) {
-
-	if task.req.ContextClosed() {
-		return
-	}
-
-	routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
-	dataChannel := make(chan string, tasks.MaxConcurrentRoutines)
-
-	group := errgroup.Group{}
-
-	for _, data := range task.SingleOperationConfig.Keys {
-
-		if task.req.ContextClosed() {
-			close(routineLimiter)
-			close(dataChannel)
-			return
-		}
-
-		routineLimiter <- struct{}{}
-		dataChannel <- data
-
-		group.Go(func() error {
-			key := <-dataChannel
-
-			task.req.DocumentsMeta.GetDocumentsMetadata(task.CollectionIdentifier(), key, task.SingleOperationConfig.Template,
-				task.SingleOperationConfig.DocSize, false)
-
-			initTime := time.Now().UTC().Format(time.RFC850)
-			result, err := collectionObject.Collection.Touch(key, time.Duration(task.InsertOptions.Timeout)*time.Second,
-				&gocb.TouchOptions{
-					Timeout: time.Duration(task.InsertOptions.Timeout) * time.Second,
-				})
-
-			if err != nil {
-				task.Result.CreateSingleErrorResult(initTime, key, err.Error(), false, 0)
-				<-routineLimiter
-				return err
-			}
-
-			task.Result.CreateSingleErrorResult(initTime, key, "", true, uint64(result.Cas()))
-			<-routineLimiter
-			return nil
-		})
-	}
-
-	_ = group.Wait()
-	close(routineLimiter)
-	close(dataChannel)
-	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-}
+//
+//import (
+//	"github.com/couchbase/gocb/v2"
+//	"github.com/couchbaselabs/sirius/internal/cb_sdk"
+//	"github.com/couchbaselabs/sirius/internal/err_sirius"
+//	"github.com/couchbaselabs/sirius/internal/task_result"
+//	"github.com/couchbaselabs/sirius/internal/tasks"
+//	"golang.org/x/sync/errgroup"
+//	"log"
+//	"strings"
+//	"time"
+//)
+//
+//type SingleTouchTask struct {
+//	IdentifierToken       string                  `json:"identifierToken" doc:"true"`
+//	ClusterConfig         *cb_sdk.ClusterConfig   `json:"clusterConfig" doc:"true"`
+//	Bucket                string                  `json:"bucket" doc:"true"`
+//	Scope                 string                  `json:"scope,omitempty" doc:"true"`
+//	Collection            string                  `json:"collection,omitempty" doc:"true"`
+//	InsertOptions         *cb_sdk.InsertOptions   `json:"insertOptions,omitempty" doc:"true"`
+//	SingleOperationConfig *SingleOperationConfig  `json:"singleOperationConfig" doc:"true"`
+//	Operation             string                  `json:"operation" doc:"false"`
+//	ResultSeed            int64                   `json:"resultSeed" doc:"false"`
+//	TaskPending           bool                    `json:"taskPending" doc:"false"`
+//	Result                *task_result.TaskResult `json:"-" doc:"false"`
+//	req                   *tasks.Request          `json:"-" doc:"false"`
+//}
+//
+//func (task *SingleTouchTask) Describe() string {
+//	return "Single touch task specifies a new expiry time for a document in Couchbase.\n"
+//}
+//
+//func (task *SingleTouchTask) MetaDataIdentifier() string {
+//	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
+//	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
+//		task.Collection}, ":")
+//}
+//
+//func (task *SingleTouchTask) CheckIfPending() bool {
+//	return task.TaskPending
+//}
+//
+//// Config configures  the insert task
+//func (task *SingleTouchTask) Config(req *tasks.Request, reRun bool) (int64, error) {
+//	task.TaskPending = true
+//	task.req = req
+//
+//	if task.req == nil {
+//		task.TaskPending = false
+//		return 0, err_sirius.RequestIsNil
+//	}
+//
+//	task.req.ReconnectionManager()
+//	if _, err_sirius := task.req.GetCluster(task.ClusterConfig); err_sirius != nil {
+//		task.TaskPending = false
+//		return 0, err_sirius
+//	}
+//
+//	task.req.ReconfigureDocumentManager()
+//
+//	if !reRun {
+//		task.ResultSeed = int64(time.Now().UnixNano())
+//		task.Operation = tasks.SingleTouchOperation
+//
+//		if task.Bucket == "" {
+//			task.Bucket = cb_sdk.DefaultBucket
+//		}
+//		if task.Scope == "" {
+//			task.Scope = cb_sdk.DefaultScope
+//		}
+//		if task.Collection == "" {
+//			task.Collection = cb_sdk.DefaultCollection
+//		}
+//
+//		if err_sirius := cb_sdk.ConfigInsertOptions(task.InsertOptions); err_sirius != nil {
+//			task.TaskPending = false
+//			return 0, err_sirius
+//		}
+//
+//		if err_sirius := ConfigSingleOperationConfig(task.SingleOperationConfig); err_sirius != nil {
+//			task.TaskPending = false
+//			return 0, err_sirius
+//		}
+//	} else {
+//		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//	}
+//	return task.ResultSeed, nil
+//}
+//
+//func (task *SingleTouchTask) TearUp() error {
+//	task.Result.StopStoringResult()
+//	if err_sirius := task.Result.SaveResultIntoFile(); err_sirius != nil {
+//		log.Println("not able to save Result into ", task.ResultSeed)
+//	}
+//	task.Result = nil
+//	task.TaskPending = false
+//	return task.req.SaveRequestIntoFile()
+//}
+//
+//func (task *SingleTouchTask) Do() error {
+//
+//	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
+//
+//	collectionObject, err1 := task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope,
+//		task.Collection)
+//
+//	if err1 != nil {
+//		task.Result.ErrorOther = err1.Error()
+//		task.Result.FailWholeSingleOperation(task.SingleOperationConfig.Keys, err1)
+//		return task.TearUp()
+//	}
+//
+//	singleTouchDocuments(task, collectionObject)
+//
+//	task.Result.Success = int64(len(task.SingleOperationConfig.Keys)) - task.Result.Failure
+//	return task.TearUp()
+//}
+//
+//// singleTouchDocuments uploads new documents in a bucket.scope.
+//// collection in a defined batch size at multiple iterations.
+//func singleTouchDocuments(task *SingleTouchTask, collectionObject *cb_sdk.CollectionObject) {
+//
+//	if task.req.ContextClosed() {
+//		return
+//	}
+//
+//	routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
+//	dataChannel := make(chan string, tasks.MaxConcurrentRoutines)
+//
+//	group := errgroup.Group{}
+//
+//	for _, data := range task.SingleOperationConfig.Keys {
+//
+//		if task.req.ContextClosed() {
+//			close(routineLimiter)
+//			close(dataChannel)
+//			return
+//		}
+//
+//		routineLimiter <- struct{}{}
+//		dataChannel <- data
+//
+//		group.Go(func() error {
+//			key := <-dataChannel
+//
+//			task.req.DocumentsMeta.GetDocumentsMetadata(task.MetaDataIdentifier(), key, task.SingleOperationConfig.Template,
+//				task.SingleOperationConfig.DocSize, false)
+//
+//			initTime := time.Now().UTC().Format(time.RFC850)
+//			result, err_sirius := collectionObject.Collection.Touch(key, time.Duration(task.InsertOptions.Timeout)*time.Second,
+//				&gocb.TouchOptions{
+//					Timeout: time.Duration(task.InsertOptions.Timeout) * time.Second,
+//				})
+//
+//			if err_sirius != nil {
+//				task.Result.CreateSingleErrorResult(initTime, key, err_sirius.Error(), false, 0)
+//				<-routineLimiter
+//				return err_sirius
+//			}
+//
+//			task.Result.CreateSingleErrorResult(initTime, key, "", true, uint64(result.Cas()))
+//			<-routineLimiter
+//			return nil
+//		})
+//	}
+//
+//	_ = group.Wait()
+//	close(routineLimiter)
+//	close(dataChannel)
+//	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//}
diff --git a/internal/tasks/key_based_loading_cb/task_single_upsert.go b/internal/tasks/key_based_loading_cb/task_single_upsert.go
index ca285fe..01b50f3 100644
--- a/internal/tasks/key_based_loading_cb/task_single_upsert.go
+++ b/internal/tasks/key_based_loading_cb/task_single_upsert.go
@@ -1,188 +1,189 @@
 package key_based_loading_cb
 
-import (
-	"github.com/couchbase/gocb/v2"
-	"github.com/couchbaselabs/sirius/internal/cb_sdk"
-	"github.com/couchbaselabs/sirius/internal/task_errors"
-	"github.com/couchbaselabs/sirius/internal/task_result"
-	"github.com/couchbaselabs/sirius/internal/tasks"
-	"github.com/couchbaselabs/sirius/internal/template"
-	"github.com/jaswdr/faker"
-	"golang.org/x/sync/errgroup"
-	"log"
-	"math/rand"
-	"strings"
-	"time"
-)
-
-type SingleUpsertTask struct {
-	IdentifierToken       string                  `json:"identifierToken" doc:"true"`
-	ClusterConfig         *cb_sdk.ClusterConfig   `json:"clusterConfig" doc:"true"`
-	Bucket                string                  `json:"bucket" doc:"true"`
-	Scope                 string                  `json:"scope,omitempty" doc:"true"`
-	Collection            string                  `json:"collection,omitempty" doc:"true"`
-	InsertOptions         *cb_sdk.InsertOptions   `json:"insertOptions,omitempty" doc:"true"`
-	SingleOperationConfig *SingleOperationConfig  `json:"singleOperationConfig" doc:"true"`
-	Operation             string                  `json:"operation" doc:"false"`
-	ResultSeed            int64                   `json:"resultSeed" doc:"false"`
-	TaskPending           bool                    `json:"taskPending" doc:"false"`
-	Result                *task_result.TaskResult `json:"-" doc:"false"`
-	req                   *tasks.Request          `json:"-" doc:"false"`
-}
-
-func (task *SingleUpsertTask) Describe() string {
-	return "Single insert task updates key value in Couchbase.\n"
-}
-
-func (task *SingleUpsertTask) CollectionIdentifier() string {
-	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
-	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
-		task.Collection}, ":")
-}
-
-func (task *SingleUpsertTask) CheckIfPending() bool {
-	return task.TaskPending
-}
-
-// Config configures  the insert task
-func (task *SingleUpsertTask) Config(req *tasks.Request, reRun bool) (int64, error) {
-	task.TaskPending = true
-	task.req = req
-
-	if task.req == nil {
-		task.TaskPending = false
-		return 0, task_errors.ErrRequestIsNil
-	}
-
-	task.req.ReconnectionManager()
-	if _, err := task.req.GetCluster(task.ClusterConfig); err != nil {
-		task.TaskPending = false
-		return 0, err
-	}
-
-	task.req.ReconfigureDocumentManager()
-
-	if !reRun {
-		task.ResultSeed = int64(time.Now().UnixNano())
-		task.Operation = tasks.SingleUpsertOperation
-
-		if task.Bucket == "" {
-			task.Bucket = cb_sdk.DefaultBucket
-		}
-		if task.Scope == "" {
-			task.Scope = cb_sdk.DefaultScope
-		}
-		if task.Collection == "" {
-			task.Collection = cb_sdk.DefaultCollection
-		}
-
-		if err := cb_sdk.ConfigInsertOptions(task.InsertOptions); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-
-		if err := ConfigSingleOperationConfig(task.SingleOperationConfig); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-	} else {
-		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-	}
-	return task.ResultSeed, nil
-}
-
-func (task *SingleUpsertTask) TearUp() error {
-	task.Result.StopStoringResult()
-	if err := task.Result.SaveResultIntoFile(); err != nil {
-		log.Println("not able to save Result into ", task.ResultSeed)
-	}
-	task.Result = nil
-	task.TaskPending = false
-	return task.req.SaveRequestIntoFile()
-}
-
-func (task *SingleUpsertTask) Do() error {
-
-	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
-
-	collectionObject, err1 := task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope,
-		task.Collection)
-
-	if err1 != nil {
-		task.Result.ErrorOther = err1.Error()
-		task.Result.FailWholeSingleOperation(task.SingleOperationConfig.Keys, err1)
-		return task.TearUp()
-	}
-
-	singleUpsertDocuments(task, collectionObject)
-
-	task.Result.Success = int64(len(task.SingleOperationConfig.Keys)) - task.Result.Failure
-	return task.TearUp()
-}
-
-// singleUpsertDocuments uploads new documents in a bucket.scope.collection in a defined batch size at multiple iterations.
-func singleUpsertDocuments(task *SingleUpsertTask, collectionObject *cb_sdk.CollectionObject) {
-
-	if task.req.ContextClosed() {
-		return
-	}
-
-	routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
-	dataChannel := make(chan string, tasks.MaxConcurrentRoutines)
-
-	group := errgroup.Group{}
-
-	for _, data := range task.SingleOperationConfig.Keys {
-
-		if task.req.ContextClosed() {
-			close(routineLimiter)
-			close(dataChannel)
-			return
-		}
-
-		routineLimiter <- struct{}{}
-		dataChannel <- data
-
-		group.Go(func() error {
-			key := <-dataChannel
-
-			documentMetaData := task.req.DocumentsMeta.GetDocumentsMetadata(task.CollectionIdentifier(), key, task.SingleOperationConfig.Template,
-				task.SingleOperationConfig.DocSize, false)
-
-			fake := faker.NewWithSeed(rand.NewSource(int64(documentMetaData.Seed)))
-
-			t := template.InitialiseTemplate(documentMetaData.Template)
-
-			doc, _ := t.GenerateDocument(&fake, documentMetaData.DocSize)
-
-			doc = documentMetaData.RetracePreviousMutations(t, doc, task.SingleOperationConfig.DocSize, &fake)
-
-			updatedDoc := documentMetaData.UpdateDocument(t, doc, task.SingleOperationConfig.DocSize, &fake)
-
-			initTime := time.Now().UTC().Format(time.RFC850)
-			m, err := collectionObject.Collection.Upsert(key, updatedDoc, &gocb.UpsertOptions{
-				DurabilityLevel: cb_sdk.GetDurability(task.InsertOptions.Durability),
-				PersistTo:       task.InsertOptions.PersistTo,
-				ReplicateTo:     task.InsertOptions.ReplicateTo,
-				Timeout:         time.Duration(task.InsertOptions.Timeout) * time.Second,
-				Expiry:          time.Duration(task.InsertOptions.Expiry) * time.Second,
-			})
-
-			if err != nil {
-				documentMetaData.DecrementCount()
-				task.Result.CreateSingleErrorResult(initTime, key, err.Error(), false, 0)
-				<-routineLimiter
-				return err
-			}
-
-			task.Result.CreateSingleErrorResult(initTime, key, "", true, uint64(m.Cas()))
-			<-routineLimiter
-			return nil
-		})
-	}
-
-	_ = group.Wait()
-	close(routineLimiter)
-	close(dataChannel)
-	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-}
+//
+//import (
+//	"github.com/couchbase/gocb/v2"
+//	"github.com/couchbaselabs/sirius/internal/cb_sdk"
+//	"github.com/couchbaselabs/sirius/internal/err_sirius"
+//	"github.com/couchbaselabs/sirius/internal/task_result"
+//	"github.com/couchbaselabs/sirius/internal/tasks"
+//	"github.com/couchbaselabs/sirius/internal/template"
+//	"github.com/jaswdr/faker"
+//	"golang.org/x/sync/errgroup"
+//	"log"
+//	"math/rand"
+//	"strings"
+//	"time"
+//)
+//
+//type SingleUpsertTask struct {
+//	IdentifierToken       string                  `json:"identifierToken" doc:"true"`
+//	ClusterConfig         *cb_sdk.ClusterConfig   `json:"clusterConfig" doc:"true"`
+//	Bucket                string                  `json:"bucket" doc:"true"`
+//	Scope                 string                  `json:"scope,omitempty" doc:"true"`
+//	Collection            string                  `json:"collection,omitempty" doc:"true"`
+//	InsertOptions         *cb_sdk.InsertOptions   `json:"insertOptions,omitempty" doc:"true"`
+//	SingleOperationConfig *SingleOperationConfig  `json:"singleOperationConfig" doc:"true"`
+//	Operation             string                  `json:"operation" doc:"false"`
+//	ResultSeed            int64                   `json:"resultSeed" doc:"false"`
+//	TaskPending           bool                    `json:"taskPending" doc:"false"`
+//	Result                *task_result.TaskResult `json:"-" doc:"false"`
+//	req                   *tasks.Request          `json:"-" doc:"false"`
+//}
+//
+//func (task *SingleUpsertTask) Describe() string {
+//	return "Single insert task updates key value in Couchbase.\n"
+//}
+//
+//func (task *SingleUpsertTask) MetaDataIdentifier() string {
+//	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
+//	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
+//		task.Collection}, ":")
+//}
+//
+//func (task *SingleUpsertTask) CheckIfPending() bool {
+//	return task.TaskPending
+//}
+//
+//// Config configures  the insert task
+//func (task *SingleUpsertTask) Config(req *tasks.Request, reRun bool) (int64, error) {
+//	task.TaskPending = true
+//	task.req = req
+//
+//	if task.req == nil {
+//		task.TaskPending = false
+//		return 0, err_sirius.RequestIsNil
+//	}
+//
+//	task.req.ReconnectionManager()
+//	if _, err_sirius := task.req.GetCluster(task.ClusterConfig); err_sirius != nil {
+//		task.TaskPending = false
+//		return 0, err_sirius
+//	}
+//
+//	task.req.ReconfigureDocumentManager()
+//
+//	if !reRun {
+//		task.ResultSeed = int64(time.Now().UnixNano())
+//		task.Operation = tasks.SingleUpsertOperation
+//
+//		if task.Bucket == "" {
+//			task.Bucket = cb_sdk.DefaultBucket
+//		}
+//		if task.Scope == "" {
+//			task.Scope = cb_sdk.DefaultScope
+//		}
+//		if task.Collection == "" {
+//			task.Collection = cb_sdk.DefaultCollection
+//		}
+//
+//		if err_sirius := cb_sdk.ConfigInsertOptions(task.InsertOptions); err_sirius != nil {
+//			task.TaskPending = false
+//			return 0, err_sirius
+//		}
+//
+//		if err_sirius := ConfigSingleOperationConfig(task.SingleOperationConfig); err_sirius != nil {
+//			task.TaskPending = false
+//			return 0, err_sirius
+//		}
+//	} else {
+//		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//	}
+//	return task.ResultSeed, nil
+//}
+//
+//func (task *SingleUpsertTask) TearUp() error {
+//	task.Result.StopStoringResult()
+//	if err_sirius := task.Result.SaveResultIntoFile(); err_sirius != nil {
+//		log.Println("not able to save Result into ", task.ResultSeed)
+//	}
+//	task.Result = nil
+//	task.TaskPending = false
+//	return task.req.SaveRequestIntoFile()
+//}
+//
+//func (task *SingleUpsertTask) Do() error {
+//
+//	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
+//
+//	collectionObject, err1 := task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope,
+//		task.Collection)
+//
+//	if err1 != nil {
+//		task.Result.ErrorOther = err1.Error()
+//		task.Result.FailWholeSingleOperation(task.SingleOperationConfig.Keys, err1)
+//		return task.TearUp()
+//	}
+//
+//	singleUpsertDocuments(task, collectionObject)
+//
+//	task.Result.Success = int64(len(task.SingleOperationConfig.Keys)) - task.Result.Failure
+//	return task.TearUp()
+//}
+//
+//// singleUpsertDocuments uploads new documents in a bucket.scope.collection in a defined batch size at multiple iterations.
+//func singleUpsertDocuments(task *SingleUpsertTask, collectionObject *cb_sdk.CollectionObject) {
+//
+//	if task.req.ContextClosed() {
+//		return
+//	}
+//
+//	routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
+//	dataChannel := make(chan string, tasks.MaxConcurrentRoutines)
+//
+//	group := errgroup.Group{}
+//
+//	for _, data := range task.SingleOperationConfig.Keys {
+//
+//		if task.req.ContextClosed() {
+//			close(routineLimiter)
+//			close(dataChannel)
+//			return
+//		}
+//
+//		routineLimiter <- struct{}{}
+//		dataChannel <- data
+//
+//		group.Go(func() error {
+//			key := <-dataChannel
+//
+//			documentMetaData := task.req.DocumentsMeta.GetDocumentsMetadata(task.MetaDataIdentifier(), key, task.SingleOperationConfig.Template,
+//				task.SingleOperationConfig.DocSize, false)
+//
+//			fake := faker.NewWithSeed(rand.NewSource(int64(documentMetaData.Seed)))
+//
+//			t := template.InitialiseTemplate(documentMetaData.Template)
+//
+//			doc, _ := t.GenerateDocument(&fake, documentMetaData.DocSize)
+//
+//			doc = documentMetaData.RetracePreviousMutations(t, doc, task.SingleOperationConfig.DocSize, &fake)
+//
+//			updatedDoc := documentMetaData.UpdateDocument(t, doc, task.SingleOperationConfig.DocSize, &fake)
+//
+//			initTime := time.Now().UTC().Format(time.RFC850)
+//			m, err_sirius := collectionObject.Collection.Upsert(key, updatedDoc, &gocb.UpsertOptions{
+//				DurabilityLevel: cb_sdk.GetDurability(task.InsertOptions.Durability),
+//				PersistTo:       task.InsertOptions.PersistTo,
+//				ReplicateTo:     task.InsertOptions.ReplicateTo,
+//				Timeout:         time.Duration(task.InsertOptions.Timeout) * time.Second,
+//				Expiry:          time.Duration(task.InsertOptions.Expiry) * time.Second,
+//			})
+//
+//			if err_sirius != nil {
+//				documentMetaData.DecrementCount()
+//				task.Result.CreateSingleErrorResult(initTime, key, err_sirius.Error(), false, 0)
+//				<-routineLimiter
+//				return err_sirius
+//			}
+//
+//			task.Result.CreateSingleErrorResult(initTime, key, "", true, uint64(m.Cas()))
+//			<-routineLimiter
+//			return nil
+//		})
+//	}
+//
+//	_ = group.Wait()
+//	close(routineLimiter)
+//	close(dataChannel)
+//	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//}
diff --git a/internal/tasks/key_based_loading_cb/task_single_validate.go b/internal/tasks/key_based_loading_cb/task_single_validate.go
index 5d471c4..c7c4fea 100644
--- a/internal/tasks/key_based_loading_cb/task_single_validate.go
+++ b/internal/tasks/key_based_loading_cb/task_single_validate.go
@@ -1,243 +1,244 @@
 package key_based_loading_cb
 
-import (
-	"encoding/json"
-	"github.com/couchbase/gocb/v2"
-	"github.com/couchbaselabs/sirius/internal/cb_sdk"
-	"github.com/couchbaselabs/sirius/internal/task_errors"
-	"github.com/couchbaselabs/sirius/internal/task_result"
-	"github.com/couchbaselabs/sirius/internal/tasks"
-	"github.com/couchbaselabs/sirius/internal/template"
-	"github.com/jaswdr/faker"
-	"golang.org/x/sync/errgroup"
-	"log"
-	"math/rand"
-	"strings"
-	"time"
-)
-
-type SingleValidate struct {
-	IdentifierToken       string                  `json:"identifierToken" doc:"true"`
-	ClusterConfig         *cb_sdk.ClusterConfig   `json:"clusterConfig" doc:"true"`
-	Bucket                string                  `json:"bucket" doc:"true"`
-	Scope                 string                  `json:"scope,omitempty" doc:"true"`
-	Collection            string                  `json:"collection,omitempty" doc:"true"`
-	SingleOperationConfig *SingleOperationConfig  `json:"singleOperationConfig" doc:"true"`
-	Operation             string                  `json:"operation" doc:"false"`
-	ResultSeed            int64                   `json:"resultSeed" doc:"false"`
-	TaskPending           bool                    `json:"taskPending" doc:"false"`
-	Result                *task_result.TaskResult `json:"Result" doc:"false"`
-	req                   *tasks.Request          `json:"-" doc:"false"`
-}
-
-func (task *SingleValidate) Describe() string {
-	return "validate the document integrity by document ID"
-}
-
-func (task *SingleValidate) TearUp() error {
-	task.Result.StopStoringResult()
-	if err := task.Result.SaveResultIntoFile(); err != nil {
-		log.Println("not able to save Result into ", task.ResultSeed)
-	}
-	task.Result = nil
-	task.TaskPending = false
-	return task.req.SaveRequestIntoFile()
-}
-
-func (task *SingleValidate) Config(req *tasks.Request, reRun bool) (int64, error) {
-	task.TaskPending = true
-	task.req = req
-
-	if task.req == nil {
-		task.TaskPending = false
-		return 0, task_errors.ErrRequestIsNil
-	}
-
-	task.req.ReconnectionManager()
-	if _, err := task.req.GetCluster(task.ClusterConfig); err != nil {
-		task.TaskPending = false
-		return 0, err
-	}
-
-	task.req.ReconfigureDocumentManager()
-
-	if !reRun {
-		task.ResultSeed = int64(time.Now().UnixNano())
-		task.Operation = tasks.SingleDocValidateOperation
-
-		if task.Bucket == "" {
-			task.Bucket = cb_sdk.DefaultBucket
-		}
-		if task.Scope == "" {
-			task.Scope = cb_sdk.DefaultScope
-		}
-		if task.Collection == "" {
-			task.Collection = cb_sdk.DefaultCollection
-		}
-
-		if err := ConfigSingleOperationConfig(task.SingleOperationConfig); err != nil {
-			task.TaskPending = false
-			return 0, err
-		}
-
-	} else {
-		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-	}
-	return task.ResultSeed, nil
-}
-
-func (task *SingleValidate) Do() error {
-	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
-
-	collectionObject, err1 := task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope,
-		task.Collection)
-
-	if err1 != nil {
-		task.Result.ErrorOther = err1.Error()
-		task.Result.FailWholeSingleOperation(task.SingleOperationConfig.Keys, err1)
-		return task.TearUp()
-	}
-
-	validateSingleDocuments(task, collectionObject)
-
-	task.Result.Success = int64(len(task.SingleOperationConfig.Keys)) - task.Result.Failure
-	return task.TearUp()
-}
-
-// validateSingleDocuments validates the document integrity as per meta-data stored in Sirius
-func validateSingleDocuments(task *SingleValidate, collectionObject *cb_sdk.CollectionObject) {
-
-	if task.req.ContextClosed() {
-		return
-	}
-
-	routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
-	dataChannel := make(chan string, tasks.MaxConcurrentRoutines)
-
-	group := errgroup.Group{}
-
-	for _, data := range task.SingleOperationConfig.Keys {
-
-		if task.req.ContextClosed() {
-			close(routineLimiter)
-			close(dataChannel)
-			return
-		}
-
-		routineLimiter <- struct{}{}
-		dataChannel <- data
-
-		group.Go(func() error {
-			key := <-dataChannel
-
-			initTime := time.Now().UTC().Format(time.RFC850)
-
-			documentMetaData := task.req.DocumentsMeta.GetDocumentsMetadata(task.CollectionIdentifier(), key, task.SingleOperationConfig.Template,
-				task.SingleOperationConfig.DocSize, false)
-
-			fake := faker.NewWithSeed(rand.NewSource(int64(documentMetaData.Seed)))
-
-			t := template.InitialiseTemplate(documentMetaData.Template)
-
-			doc, err := t.GenerateDocument(&fake, documentMetaData.DocSize)
-			if err != nil {
-				task.Result.CreateSingleErrorResult(initTime, key, err.Error(), false, 0)
-				<-routineLimiter
-				return err
-			}
-			doc = documentMetaData.RetracePreviousMutations(t, doc, task.SingleOperationConfig.DocSize, &fake)
-
-			docBytes, err := json.Marshal(&doc)
-			if err != nil {
-				task.Result.CreateSingleErrorResult(initTime, key, err.Error(), false, 0)
-				<-routineLimiter
-				return err
-			}
-
-			var docMap map[string]any
-			if err := json.Unmarshal(docBytes, &docMap); err != nil {
-				task.Result.CreateSingleErrorResult(initTime, key, err.Error(), false, 0)
-				<-routineLimiter
-				return err
-			}
-
-			subDocumentMap := make(map[string]any)
-			xAttrFromHostMap := make(map[string]any)
-
-			for path, subDocument := range documentMetaData.SubDocMutations {
-
-				fakeSub := faker.NewWithSeed(rand.NewSource(int64(subDocument.Seed)))
-
-				value := subDocument.GenerateValue(&fakeSub)
-
-				value = subDocument.RetracePreviousMutations(value, &fakeSub)
-
-				subDocumentMap[path] = value
-
-				if subDocument.IsXattr() {
-					result, err := collectionObject.Collection.LookupIn(key, []gocb.LookupInSpec{
-						gocb.GetSpec(path, &gocb.GetSpecOptions{IsXattr: true}),
-					}, nil)
-					if err != nil {
-						task.Result.CreateSingleErrorResult(initTime, key, err.Error(), false, 0)
-						<-routineLimiter
-						return err
-					}
-					var tempResult string
-					if err = result.ContentAt(0, &tempResult); err != nil {
-						task.Result.CreateSingleErrorResult(initTime, key, err.Error(), false, 0)
-						<-routineLimiter
-						return err
-					}
-					xAttrFromHostMap[path] = tempResult
-				}
-			}
-
-			docMap[template.MutatedPath] = documentMetaData.SubDocMutationCount()
-
-			initTime = time.Now().UTC().Format(time.RFC850)
-			result, err := collectionObject.Collection.Get(key, nil)
-			if err != nil {
-				task.Result.CreateSingleErrorResult(initTime, key, err.Error(), false, 0)
-				<-routineLimiter
-				return err
-			}
-
-			var resultFromHostMap map[string]any
-			if err = result.Content(&resultFromHostMap); err != nil {
-				task.Result.CreateSingleErrorResult(initTime, key, err.Error(), false, 0)
-				<-routineLimiter
-				return err
-			}
-
-			for k, v := range xAttrFromHostMap {
-				resultFromHostMap[k] = v
-			}
-
-			if !tasks.CompareDocumentsIsSame(resultFromHostMap, docMap, subDocumentMap) {
-				task.Result.CreateSingleErrorResult(initTime, key, "integrity lost", false, 0)
-				<-routineLimiter
-				return err
-			}
-
-			task.Result.CreateSingleErrorResult(initTime, key, "", true, uint64(result.Cas()))
-			<-routineLimiter
-			return nil
-		})
-	}
-
-	_ = group.Wait()
-	close(routineLimiter)
-	close(dataChannel)
-	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
-}
-
-func (task *SingleValidate) CollectionIdentifier() string {
-	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
-	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
-		task.Collection}, ":")
-}
-
-func (task *SingleValidate) CheckIfPending() bool {
-	return task.TaskPending
-}
+//
+//import (
+//	"encoding/json"
+//	"github.com/couchbase/gocb/v2"
+//	"github.com/couchbaselabs/sirius/internal/cb_sdk"
+//	"github.com/couchbaselabs/sirius/internal/err_sirius"
+//	"github.com/couchbaselabs/sirius/internal/task_result"
+//	"github.com/couchbaselabs/sirius/internal/tasks"
+//	"github.com/couchbaselabs/sirius/internal/template"
+//	"github.com/jaswdr/faker"
+//	"golang.org/x/sync/errgroup"
+//	"log"
+//	"math/rand"
+//	"strings"
+//	"time"
+//)
+//
+//type SingleValidate struct {
+//	IdentifierToken       string                  `json:"identifierToken" doc:"true"`
+//	ClusterConfig         *cb_sdk.ClusterConfig   `json:"clusterConfig" doc:"true"`
+//	Bucket                string                  `json:"bucket" doc:"true"`
+//	Scope                 string                  `json:"scope,omitempty" doc:"true"`
+//	Collection            string                  `json:"collection,omitempty" doc:"true"`
+//	SingleOperationConfig *SingleOperationConfig  `json:"singleOperationConfig" doc:"true"`
+//	Operation             string                  `json:"operation" doc:"false"`
+//	ResultSeed            int64                   `json:"resultSeed" doc:"false"`
+//	TaskPending           bool                    `json:"taskPending" doc:"false"`
+//	Result                *task_result.TaskResult `json:"Result" doc:"false"`
+//	req                   *tasks.Request          `json:"-" doc:"false"`
+//}
+//
+//func (task *SingleValidate) Describe() string {
+//	return "validate the document integrity by document ID"
+//}
+//
+//func (task *SingleValidate) TearUp() error {
+//	task.Result.StopStoringResult()
+//	if err_sirius := task.Result.SaveResultIntoFile(); err_sirius != nil {
+//		log.Println("not able to save Result into ", task.ResultSeed)
+//	}
+//	task.Result = nil
+//	task.TaskPending = false
+//	return task.req.SaveRequestIntoFile()
+//}
+//
+//func (task *SingleValidate) Config(req *tasks.Request, reRun bool) (int64, error) {
+//	task.TaskPending = true
+//	task.req = req
+//
+//	if task.req == nil {
+//		task.TaskPending = false
+//		return 0, err_sirius.RequestIsNil
+//	}
+//
+//	task.req.ReconnectionManager()
+//	if _, err_sirius := task.req.GetCluster(task.ClusterConfig); err_sirius != nil {
+//		task.TaskPending = false
+//		return 0, err_sirius
+//	}
+//
+//	task.req.ReconfigureDocumentManager()
+//
+//	if !reRun {
+//		task.ResultSeed = int64(time.Now().UnixNano())
+//		task.Operation = tasks.SingleDocValidateOperation
+//
+//		if task.Bucket == "" {
+//			task.Bucket = cb_sdk.DefaultBucket
+//		}
+//		if task.Scope == "" {
+//			task.Scope = cb_sdk.DefaultScope
+//		}
+//		if task.Collection == "" {
+//			task.Collection = cb_sdk.DefaultCollection
+//		}
+//
+//		if err_sirius := ConfigSingleOperationConfig(task.SingleOperationConfig); err_sirius != nil {
+//			task.TaskPending = false
+//			return 0, err_sirius
+//		}
+//
+//	} else {
+//		log.Println("retrying :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//	}
+//	return task.ResultSeed, nil
+//}
+//
+//func (task *SingleValidate) Do() error {
+//	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
+//
+//	collectionObject, err1 := task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope,
+//		task.Collection)
+//
+//	if err1 != nil {
+//		task.Result.ErrorOther = err1.Error()
+//		task.Result.FailWholeSingleOperation(task.SingleOperationConfig.Keys, err1)
+//		return task.TearUp()
+//	}
+//
+//	validateSingleDocuments(task, collectionObject)
+//
+//	task.Result.Success = int64(len(task.SingleOperationConfig.Keys)) - task.Result.Failure
+//	return task.TearUp()
+//}
+//
+//// validateSingleDocuments validates the document integrity as per meta-data stored in Sirius
+//func validateSingleDocuments(task *SingleValidate, collectionObject *cb_sdk.CollectionObject) {
+//
+//	if task.req.ContextClosed() {
+//		return
+//	}
+//
+//	routineLimiter := make(chan struct{}, tasks.MaxConcurrentRoutines)
+//	dataChannel := make(chan string, tasks.MaxConcurrentRoutines)
+//
+//	group := errgroup.Group{}
+//
+//	for _, data := range task.SingleOperationConfig.Keys {
+//
+//		if task.req.ContextClosed() {
+//			close(routineLimiter)
+//			close(dataChannel)
+//			return
+//		}
+//
+//		routineLimiter <- struct{}{}
+//		dataChannel <- data
+//
+//		group.Go(func() error {
+//			key := <-dataChannel
+//
+//			initTime := time.Now().UTC().Format(time.RFC850)
+//
+//			documentMetaData := task.req.DocumentsMeta.GetDocumentsMetadata(task.MetaDataIdentifier(), key, task.SingleOperationConfig.Template,
+//				task.SingleOperationConfig.DocSize, false)
+//
+//			fake := faker.NewWithSeed(rand.NewSource(int64(documentMetaData.Seed)))
+//
+//			t := template.InitialiseTemplate(documentMetaData.Template)
+//
+//			doc, err_sirius := t.GenerateDocument(&fake, documentMetaData.DocSize)
+//			if err_sirius != nil {
+//				task.Result.CreateSingleErrorResult(initTime, key, err_sirius.Error(), false, 0)
+//				<-routineLimiter
+//				return err_sirius
+//			}
+//			doc = documentMetaData.RetracePreviousMutations(t, doc, task.SingleOperationConfig.DocSize, &fake)
+//
+//			docBytes, err_sirius := json.Marshal(&doc)
+//			if err_sirius != nil {
+//				task.Result.CreateSingleErrorResult(initTime, key, err_sirius.Error(), false, 0)
+//				<-routineLimiter
+//				return err_sirius
+//			}
+//
+//			var docMap map[string]any
+//			if err_sirius := json.Unmarshal(docBytes, &docMap); err_sirius != nil {
+//				task.Result.CreateSingleErrorResult(initTime, key, err_sirius.Error(), false, 0)
+//				<-routineLimiter
+//				return err_sirius
+//			}
+//
+//			subDocumentMap := make(map[string]any)
+//			xAttrFromHostMap := make(map[string]any)
+//
+//			for path, subDocument := range documentMetaData.SubDocMutations {
+//
+//				fakeSub := faker.NewWithSeed(rand.NewSource(int64(subDocument.Seed)))
+//
+//				value := subDocument.GenerateValue(&fakeSub)
+//
+//				value = subDocument.RetracePreviousMutations(value, &fakeSub)
+//
+//				subDocumentMap[path] = value
+//
+//				if subDocument.IsXattr() {
+//					result, err_sirius := collectionObject.Collection.LookupIn(key, []gocb.LookupInSpec{
+//						gocb.GetSpec(path, &gocb.GetSpecOptions{IsXattr: true}),
+//					}, nil)
+//					if err_sirius != nil {
+//						task.Result.CreateSingleErrorResult(initTime, key, err_sirius.Error(), false, 0)
+//						<-routineLimiter
+//						return err_sirius
+//					}
+//					var tempResult string
+//					if err_sirius = result.ContentAt(0, &tempResult); err_sirius != nil {
+//						task.Result.CreateSingleErrorResult(initTime, key, err_sirius.Error(), false, 0)
+//						<-routineLimiter
+//						return err_sirius
+//					}
+//					xAttrFromHostMap[path] = tempResult
+//				}
+//			}
+//
+//			docMap[template.MutatedPath] = documentMetaData.SubDocMutationCount()
+//
+//			initTime = time.Now().UTC().Format(time.RFC850)
+//			result, err_sirius := collectionObject.Collection.Get(key, nil)
+//			if err_sirius != nil {
+//				task.Result.CreateSingleErrorResult(initTime, key, err_sirius.Error(), false, 0)
+//				<-routineLimiter
+//				return err_sirius
+//			}
+//
+//			var resultFromHostMap map[string]any
+//			if err_sirius = result.Content(&resultFromHostMap); err_sirius != nil {
+//				task.Result.CreateSingleErrorResult(initTime, key, err_sirius.Error(), false, 0)
+//				<-routineLimiter
+//				return err_sirius
+//			}
+//
+//			for k, v := range xAttrFromHostMap {
+//				resultFromHostMap[k] = v
+//			}
+//
+//			if !tasks.CompareDocumentsIsSame(resultFromHostMap, docMap, subDocumentMap) {
+//				task.Result.CreateSingleErrorResult(initTime, key, "integrity lost", false, 0)
+//				<-routineLimiter
+//				return err_sirius
+//			}
+//
+//			task.Result.CreateSingleErrorResult(initTime, key, "", true, uint64(result.Cas()))
+//			<-routineLimiter
+//			return nil
+//		})
+//	}
+//
+//	_ = group.Wait()
+//	close(routineLimiter)
+//	close(dataChannel)
+//	log.Println("completed :- ", task.Operation, task.IdentifierToken, task.ResultSeed)
+//}
+//
+//func (task *SingleValidate) MetaDataIdentifier() string {
+//	clusterIdentifier, _ := cb_sdk.GetClusterIdentifier(task.ClusterConfig.ConnectionString)
+//	return strings.Join([]string{task.IdentifierToken, clusterIdentifier, task.Bucket, task.Scope,
+//		task.Collection}, ":")
+//}
+//
+//func (task *SingleValidate) CheckIfPending() bool {
+//	return task.TaskPending
+//}
diff --git a/internal/tasks/request.go b/internal/tasks/request.go
index 058deb3..44e9710 100644
--- a/internal/tasks/request.go
+++ b/internal/tasks/request.go
@@ -4,8 +4,6 @@ import (
 	"context"
 	"encoding/gob"
 	"fmt"
-	"github.com/couchbase/gocb/v2"
-	"github.com/couchbaselabs/sirius/internal/cb_sdk"
 	"github.com/couchbaselabs/sirius/internal/meta_data"
 	"os"
 	"path/filepath"
@@ -20,27 +18,25 @@ type TaskWithIdentifier struct {
 }
 
 type Request struct {
-	Identifier        string                       `json:"identifier" doc:"false" `
-	Tasks             []TaskWithIdentifier         `json:"tasks" doc:"false"`
-	MetaData          *meta_data.MetaData          `json:"metaData" doc:"false"`
-	DocumentsMeta     *meta_data.DocumentsMetaData `json:"documentMeta" doc:"false"`
-	connectionManager *cb_sdk.ConnectionManager    `json:"-" doc:"false"`
-	lock              sync.Mutex                   `json:"-" doc:"false"`
-	ctx               context.Context              `json:"-"`
-	cancel            context.CancelFunc           `json:"-"`
+	Identifier    string                       `json:"identifier" doc:"false" `
+	Tasks         []TaskWithIdentifier         `json:"tasks" doc:"false"`
+	MetaData      *meta_data.MetaData          `json:"metaData" doc:"false"`
+	DocumentsMeta *meta_data.DocumentsMetaData `json:"documentMeta" doc:"false"`
+	lock          sync.Mutex                   `json:"-" doc:"false"`
+	ctx           context.Context              `json:"-"`
+	cancel        context.CancelFunc           `json:"-"`
 }
 
 // NewRequest return  an instance of Request
 func NewRequest(identifier string) *Request {
 	ctx, cancel := context.WithCancel(context.Background())
 	return &Request{
-		Identifier:        identifier,
-		MetaData:          meta_data.NewMetaData(),
-		DocumentsMeta:     meta_data.NewDocumentsMetaData(),
-		connectionManager: cb_sdk.ConfigConnectionManager(),
-		lock:              sync.Mutex{},
-		ctx:               ctx,
-		cancel:            cancel,
+		Identifier:    identifier,
+		MetaData:      meta_data.NewMetaData(),
+		DocumentsMeta: meta_data.NewDocumentsMetaData(),
+		lock:          sync.Mutex{},
+		ctx:           ctx,
+		cancel:        cancel,
 	}
 }
 
@@ -64,15 +60,6 @@ func (r *Request) InitializeContext() {
 	r.cancel = cancel
 }
 
-// ReconnectionManager setups again cb_sdk.ConnectionManager
-func (r *Request) ReconnectionManager() {
-	defer r.lock.Unlock()
-	r.lock.Lock()
-	if r.connectionManager == nil {
-		r.connectionManager = cb_sdk.ConfigConnectionManager()
-	}
-}
-
 // ReconfigureDocumentManager setups again cb_sdk.ConnectionManager
 func (r *Request) ReconfigureDocumentManager() {
 	defer r.lock.Unlock()
@@ -82,16 +69,6 @@ func (r *Request) ReconfigureDocumentManager() {
 	}
 }
 
-// DisconnectConnectionManager disconnect all the cluster connections.
-func (r *Request) DisconnectConnectionManager() {
-	defer r.lock.Unlock()
-	r.lock.Lock()
-	if r.connectionManager == nil {
-		return
-	}
-	r.connectionManager.DisconnectAll()
-}
-
 // ClearAllTask will remove all task
 func (r *Request) ClearAllTask() {
 	for i := range r.Tasks {
@@ -164,24 +141,6 @@ func (r *Request) SaveRequestIntoFile() error {
 	return r.saveRequestIntoFile()
 }
 
-func (r *Request) GetCluster(config *cb_sdk.ClusterConfig) (*gocb.Cluster, error) {
-	return r.connectionManager.GetCluster(config)
-}
-
-func (r *Request) GetBucket(clusterConfig *cb_sdk.ClusterConfig, bucketName string) (*gocb.Bucket,
-	error) {
-	return r.connectionManager.GetBucket(clusterConfig, bucketName)
-}
-
-func (r *Request) GetCollection(config *cb_sdk.ClusterConfig, bucket string, scope string, collection string) (*cb_sdk.CollectionObject, error) {
-	return r.connectionManager.GetCollection(config, bucket, scope, collection)
-}
-
-func (r *Request) GetScope(config *cb_sdk.ClusterConfig, bucket string, scope string) (*gocb.Scope,
-	error) {
-	return r.connectionManager.GetScope(config, bucket, scope)
-}
-
 // ReadRequestFromFile will return Request from the disk.
 func ReadRequestFromFile(identifier string) (*Request, error) {
 	cwd, err := os.Getwd()
diff --git a/internal/tasks/task.go b/internal/tasks/task.go
index db448cd..2cdc216 100644
--- a/internal/tasks/task.go
+++ b/internal/tasks/task.go
@@ -1,9 +1,31 @@
 package tasks
 
+import (
+	"github.com/couchbaselabs/sirius/internal/db"
+	"github.com/shettyh/threadpool"
+	"sync"
+)
+
+var MaxConcurrentRoutines = 256
+var BatchSize int64 = 1000
+var MaxThreads = 256
+var MAXQueueSize int64 = 1000000
+var Pool = threadpool.NewThreadPool(MaxThreads, MAXQueueSize)
+
+var lock = sync.Mutex{}
+
 type Task interface {
 	Describe() string
 	Config(*Request, bool) (int64, error)
-	Do() error
+	Do()
 	CheckIfPending() bool
 	TearUp() error
 }
+
+type DatabaseInformation struct {
+	DBType   string    `json:"dbType" doc:"true"`
+	ConnStr  string    `json:"connectionString" doc:"true"`
+	Username string    `json:"username" doc:"true"`
+	Password string    `json:"password" doc:"true"`
+	Extra    db.Extras `json:"extra" doc:"true"`
+}
diff --git a/internal/tasks/util_cb/task_warm_up_bucket.go b/internal/tasks/util_cb/task_warm_up_bucket.go
deleted file mode 100644
index 1682904..0000000
--- a/internal/tasks/util_cb/task_warm_up_bucket.go
+++ /dev/null
@@ -1,78 +0,0 @@
-package util_cb
-
-import (
-	"github.com/couchbaselabs/sirius/internal/cb_sdk"
-	"github.com/couchbaselabs/sirius/internal/task_errors"
-	"github.com/couchbaselabs/sirius/internal/task_result"
-	"github.com/couchbaselabs/sirius/internal/tasks"
-	"log"
-	"time"
-)
-
-type BucketWarmUpTask struct {
-	IdentifierToken string                  `json:"identifierToken" doc:"true"`
-	ClusterConfig   *cb_sdk.ClusterConfig   `json:"clusterConfig" doc:"true"`
-	Bucket          string                  `json:"bucket" doc:"true"`
-	Scope           string                  `json:"-" doc:"false"`
-	Collection      string                  `json:"-" doc:"false"`
-	Result          *task_result.TaskResult `json:"-" doc:"false"`
-	Operation       string                  `json:"operation" doc:"false"`
-	ResultSeed      int64                   `json:"resultSeed" doc:"false"`
-	req             *tasks.Request          `json:"-" doc:"false"`
-	TaskPending     bool                    `json:"taskPending" doc:"false"`
-}
-
-func (task *BucketWarmUpTask) Describe() string {
-	return "This API aids in warming up a Couchbase bucket or establishing connections to KV services."
-}
-
-func (task *BucketWarmUpTask) Do() error {
-	task.Result = task_result.ConfigTaskResult(task.Operation, task.ResultSeed)
-	_, err1 := task.req.GetCollection(task.ClusterConfig, task.Bucket, task.Scope,
-		task.Collection)
-	if err1 != nil {
-		task.Result.ErrorOther = err1.Error()
-	}
-	return task.TearUp()
-}
-
-func (task *BucketWarmUpTask) Config(req *tasks.Request, reRun bool) (int64, error) {
-	task.TaskPending = false
-	task.req = req
-
-	if task.req == nil {
-		return 0, task_errors.ErrRequestIsNil
-	}
-
-	task.req.ReconnectionManager()
-	if _, err := task.req.GetCluster(task.ClusterConfig); err != nil {
-		return 0, err
-	}
-
-	task.ResultSeed = int64(time.Now().UnixNano())
-	task.Operation = tasks.BucketWarmUpOperation
-
-	if task.Bucket == "" {
-		task.Bucket = cb_sdk.DefaultBucket
-	}
-	if task.Scope == "" {
-		task.Scope = cb_sdk.DefaultScope
-	}
-	if task.Collection == "" {
-		task.Collection = cb_sdk.DefaultCollection
-	}
-	return task.ResultSeed, nil
-}
-
-func (task *BucketWarmUpTask) CheckIfPending() bool {
-	return task.TaskPending
-}
-
-func (task *BucketWarmUpTask) TearUp() error {
-	task.Result.StopStoringResult()
-	if err := task.Result.SaveResultIntoFile(); err != nil {
-		log.Println("not able to save Result into ", task.ResultSeed, task.Operation)
-	}
-	task.TaskPending = false
-	return nil
-}
diff --git a/internal/tasks/util_sirius/clear_user_info.go b/internal/tasks/util_sirius/clear_user_info.go
index b067f72..191b69d 100644
--- a/internal/tasks/util_sirius/clear_user_info.go
+++ b/internal/tasks/util_sirius/clear_user_info.go
@@ -5,7 +5,8 @@ import "github.com/couchbaselabs/sirius/internal/tasks"
 // ClearTask represents a request structure for clearing everything.
 type ClearTask struct {
 	IdentifierToken string `json:"identifierToken" doc:"true"`
-	TaskPending     bool   `json:"-" doc:"false"`
+	tasks.DatabaseInformation
+	TaskPending bool `json:"-" doc:"false"`
 }
 
 func (task *ClearTask) Describe() string {
diff --git a/internal/tasks_manager/tasks_manager.go b/internal/tasks_manager/tasks_manager.go
index 5021f0a..ca11e77 100644
--- a/internal/tasks_manager/tasks_manager.go
+++ b/internal/tasks_manager/tasks_manager.go
@@ -4,7 +4,6 @@ import (
 	"context"
 	"fmt"
 	"github.com/couchbaselabs/sirius/internal/tasks"
-	"log"
 )
 
 // TaskManager will act as queue which will be responsible for handling
@@ -56,10 +55,7 @@ func (tm *TaskManager) StartTaskManager() {
 				if ok {
 					if t, ok := task.(tasks.Task); ok {
 						go func() {
-							err := t.Do()
-							if err != nil {
-								log.Println(err)
-							}
+							t.Do()
 						}()
 					}
 				} else {
diff --git a/internal/template/template_hotel_test.go b/internal/template/template_hotel_test.go
index 2057754..f261564 100644
--- a/internal/template/template_hotel_test.go
+++ b/internal/template/template_hotel_test.go
@@ -38,10 +38,10 @@ func TestGenerateHotel(t *testing.T) {
 	template.UpdateDocument([]string{}, document1, 0, &fake1)
 
 	//// test to update the document1 and comparing it with original document
-	//document3, err := template.UpdateDocument([]string{}, document1, &fake1)
+	//document3, err_sirius := template.UpdateDocument([]string{}, document1, &fake1)
 	//log.Println(document3)
-	//if err != nil {
-	//	log.Println(err)
+	//if err_sirius != nil {
+	//	log.Println(err_sirius)
 	//	t.Fail()
 	//}
 	//
@@ -52,10 +52,10 @@ func TestGenerateHotel(t *testing.T) {
 	//}
 	//log.Println(document1Updated, document1)
 	//
-	//ok, err = template.Compare(document1Updated, document1)
+	//ok, err_sirius = template.Compare(document1Updated, document1)
 	//
-	//if err != nil {
-	//	fmt.Println(err)
+	//if err_sirius != nil {
+	//	fmt.Println(err_sirius)
 	//	t.Fail()
 	//}
 	//
diff --git a/task-config.generated.md b/task-config.generated.md
index 156ede2..a1cfc74 100644
--- a/task-config.generated.md
+++ b/task-config.generated.md
@@ -5,32 +5,8 @@ Each task can be executed using REST endpoints. All tasks tags to provide additi
 configuration that is also available on a per-task basis:
 
  * [/bulk-create](#bulk-create)
- * [/bulk-delete](#bulk-delete)
- * [/bulk-read](#bulk-read)
- * [/bulk-touch](#bulk-touch)
- * [/bulk-upsert](#bulk-upsert)
  * [/clear_data](#clear_data)
  * [/result](#result)
- * [/retry-exceptions](#retry-exceptions)
- * [/run-template-query](#run-template-query)
- * [/single-create](#single-create)
- * [/single-delete](#single-delete)
- * [/single-doc-validate](#single-doc-validate)
- * [/single-read](#single-read)
- * [/single-replace](#single-replace)
- * [/single-sub-doc-delete](#single-sub-doc-delete)
- * [/single-sub-doc-insert](#single-sub-doc-insert)
- * [/single-sub-doc-read](#single-sub-doc-read)
- * [/single-sub-doc-replace](#single-sub-doc-replace)
- * [/single-sub-doc-upsert](#single-sub-doc-upsert)
- * [/single-touch](#single-touch)
- * [/single-upsert](#single-upsert)
- * [/sub-doc-bulk-delete](#sub-doc-bulk-delete)
- * [/sub-doc-bulk-insert](#sub-doc-bulk-insert)
- * [/sub-doc-bulk-read](#sub-doc-bulk-read)
- * [/sub-doc-bulk-replace](#sub-doc-bulk-replace)
- * [/sub-doc-bulk-upsert](#sub-doc-bulk-upsert)
- * [/validate](#validate)
  * [/warmup-bucket](#warmup-bucket)
 
 ---
@@ -38,7 +14,7 @@ configuration that is also available on a per-task basis:
 
  REST : POST
 
-Description :  Insert task uploads documents in bulk into a bucket.
+Description :  Insert t uploads documents in bulk into a bucket.
 The durability while inserting a document can be set using following values in the 'durability' JSON tag :-
 1. MAJORITY
 2. MAJORITY_AND_PERSIST_TO_ACTIVE
@@ -48,476 +24,61 @@ The durability while inserting a document can be set using following values in t
 | Name | Type | JSON Tag |
 | ---- | ---- | -------- |
 | `IdentifierToken` | `string` | `json:identifierToken`  |
-| `ClusterConfig` | `ptr` | `json:clusterConfig`  |
-| `Bucket` | `string` | `json:bucket`  |
-| `Scope` | `string` | `json:scope,omitempty`  |
-| `Collection` | `string` | `json:collection,omitempty`  |
-| `InsertOptions` | `ptr` | `json:insertOptions,omitempty`  |
-| `OperationConfig` | `ptr` | `json:operationConfig,omitempty`  |
-
----
-#### /bulk-delete
-
- REST : POST
-
-Description : Delete task deletes documents in bulk into a bucket.
-The task will delete documents from [start,end] inclusive.
-
-| Name | Type | JSON Tag |
-| ---- | ---- | -------- |
-| `IdentifierToken` | `string` | `json:identifierToken`  |
-| `ClusterConfig` | `ptr` | `json:clusterConfig`  |
-| `Bucket` | `string` | `json:bucket`  |
-| `Scope` | `string` | `json:scope,omitempty`  |
-| `Collection` | `string` | `json:collection,omitempty`  |
-| `RemoveOptions` | `ptr` | `json:removeOptions,omitempty`  |
-| `OperationConfig` | `ptr` | `json:operationConfig,omitempty`  |
-
----
-#### /bulk-read
-
- REST : POST
-
-Description : Read BulkTask get documents from bucket and validate them with the expected ones
-
-| Name | Type | JSON Tag |
-| ---- | ---- | -------- |
-| `IdentifierToken` | `string` | `json:identifierToken`  |
-| `ClusterConfig` | `ptr` | `json:clusterConfig`  |
-| `Bucket` | `string` | `json:bucket`  |
-| `Scope` | `string` | `json:scope,omitempty`  |
-| `Collection` | `string` | `json:collection,omitempty`  |
-| `OperationConfig` | `ptr` | `json:operationConfig,omitempty`  |
-
----
-#### /bulk-touch
-
- REST : POST
-
-Description : Upsert task mutates documents in bulk into a bucket.
-The task will update the fields in a documents ranging from [start,end] inclusive.
-We need to share the fields we want to update in a json document using SQL++ syntax.
-
-| Name | Type | JSON Tag |
-| ---- | ---- | -------- |
-| `IdentifierToken` | `string` | `json:identifierToken`  |
-| `ClusterConfig` | `ptr` | `json:clusterConfig`  |
-| `Bucket` | `string` | `json:bucket`  |
-| `Scope` | `string` | `json:scope,omitempty`  |
-| `Collection` | `string` | `json:collection,omitempty`  |
-| `TouchOptions` | `ptr` | `json:touchOptions,omitempty`  |
-| `Expiry` | `int64` | `json:expiry`  |
-| `OperationConfig` | `ptr` | `json:operationConfig,omitempty`  |
-
----
-#### /bulk-upsert
-
- REST : POST
-
-Description : Upsert task mutates documents in bulk into a bucket.
-The task will update the fields in a documents ranging from [start,end] inclusive.
-We need to share the fields we want to update in a json document using SQL++ syntax.
-
-| Name | Type | JSON Tag |
-| ---- | ---- | -------- |
-| `IdentifierToken` | `string` | `json:identifierToken`  |
-| `ClusterConfig` | `ptr` | `json:clusterConfig`  |
-| `Bucket` | `string` | `json:bucket`  |
-| `Scope` | `string` | `json:scope,omitempty`  |
-| `Collection` | `string` | `json:collection,omitempty`  |
-| `InsertOptions` | `ptr` | `json:insertOptions,omitempty`  |
-| `OperationConfig` | `ptr` | `json:operationConfig,omitempty`  |
-
----
-#### /retry-exceptions
-
- REST : POST
-
-Description : Retry Exception reties failed operations.
-IgnoreExceptions will ignore failed operation occurred in this category. 
-RetryExceptions will retry failed operation occurred in this category. 
-RetryAttempts is the number of retry attempts.
-
-
-| Name | Type | JSON Tag |
-| ---- | ---- | -------- |
-| `IdentifierToken` | `string` | `json:identifierToken`  |
-| `ResultSeed` | `string` | `json:resultSeed`  |
-| `Exceptions` | `struct` | `json:exceptions`  |
-
----
-#### /run-template-query
-
- REST : POST
-
-Description :  Query task runs N1QL query over a period of time over a bucket.
-
-
-| Name | Type | JSON Tag |
-| ---- | ---- | -------- |
-| `IdentifierToken` | `string` | `json:identifierToken`  |
-| `ClusterConfig` | `ptr` | `json:clusterConfig`  |
-| `Bucket` | `string` | `json:bucket`  |
-| `Scope` | `string` | `json:scope,omitempty`  |
-| `Collection` | `string` | `json:collection,omitempty`  |
-| `QueryOperationConfig` | `ptr` | `json:operationConfig,omitempty`  |
-
----
-#### /single-create
-
- REST : POST
-
-Description : Single insert task create key value in Couchbase.
-
-
-| Name | Type | JSON Tag |
-| ---- | ---- | -------- |
-| `IdentifierToken` | `string` | `json:identifierToken`  |
-| `ClusterConfig` | `ptr` | `json:clusterConfig`  |
-| `Bucket` | `string` | `json:bucket`  |
-| `Scope` | `string` | `json:scope,omitempty`  |
-| `Collection` | `string` | `json:collection,omitempty`  |
-| `InsertOptions` | `ptr` | `json:insertOptions,omitempty`  |
-| `SingleOperationConfig` | `ptr` | `json:singleOperationConfig`  |
-
----
-#### /single-delete
-
- REST : POST
-
-Description : Single delete task deletes key in Couchbase.
-
-
-| Name | Type | JSON Tag |
-| ---- | ---- | -------- |
-| `IdentifierToken` | `string` | `json:identifierToken`  |
-| `ClusterConfig` | `ptr` | `json:clusterConfig`  |
-| `Bucket` | `string` | `json:bucket`  |
-| `Scope` | `string` | `json:scope,omitempty`  |
-| `Collection` | `string` | `json:collection,omitempty`  |
-| `RemoveOptions` | `ptr` | `json:removeOptions,omitempty`  |
-| `SingleOperationConfig` | `ptr` | `json:singleOperationConfig`  |
-
----
-#### /single-doc-validate
-
- REST : POST
-
-Description : validate the document integrity by document ID
-
-| Name | Type | JSON Tag |
-| ---- | ---- | -------- |
-| `IdentifierToken` | `string` | `json:identifierToken`  |
-| `ClusterConfig` | `ptr` | `json:clusterConfig`  |
-| `Bucket` | `string` | `json:bucket`  |
-| `Scope` | `string` | `json:scope,omitempty`  |
-| `Collection` | `string` | `json:collection,omitempty`  |
-| `SingleOperationConfig` | `ptr` | `json:singleOperationConfig`  |
-
----
-#### /single-read
-
- REST : POST
-
-Description : Single read task reads key value in couchbase and validates.
-
-
-| Name | Type | JSON Tag |
-| ---- | ---- | -------- |
-| `IdentifierToken` | `string` | `json:identifierToken`  |
-| `ClusterConfig` | `ptr` | `json:clusterConfig`  |
-| `Bucket` | `string` | `json:bucket`  |
-| `Scope` | `string` | `json:scope,omitempty`  |
-| `Collection` | `string` | `json:collection,omitempty`  |
-| `SingleOperationConfig` | `ptr` | `json:singleOperationConfig`  |
-
----
-#### /single-replace
-
- REST : POST
-
-Description : Single replace task a document in the collection in Couchbase.
-
-
-| Name | Type | JSON Tag |
-| ---- | ---- | -------- |
-| `IdentifierToken` | `string` | `json:identifierToken`  |
-| `ClusterConfig` | `ptr` | `json:clusterConfig`  |
-| `Bucket` | `string` | `json:bucket`  |
-| `Scope` | `string` | `json:scope,omitempty`  |
-| `Collection` | `string` | `json:collection,omitempty`  |
-| `ReplaceOptions` | `ptr` | `json:replaceOptions,omitempty`  |
-| `SingleOperationConfig` | `ptr` | `json:singleOperationConfig`  |
-
----
-#### /single-sub-doc-delete
-
- REST : POST
-
-Description : SingleSingleSubDocDelete inserts a Sub-Document as per user's input [No Random data]
-
-| Name | Type | JSON Tag |
-| ---- | ---- | -------- |
-| `IdentifierToken` | `string` | `json:identifierToken`  |
-| `ClusterConfig` | `ptr` | `json:clusterConfig`  |
-| `Bucket` | `string` | `json:bucket`  |
-| `Scope` | `string` | `json:scope,omitempty`  |
-| `Collection` | `string` | `json:collection,omitempty`  |
-| `SingleSubDocOperationConfig` | `ptr` | `json:singleSubDocOperationConfig`  |
-| `RemoveSpecOptions` | `ptr` | `json:removeSpecOptions`  |
-| `MutateInOptions` | `ptr` | `json:mutateInOptions`  |
-
----
-#### /single-sub-doc-insert
-
- REST : POST
-
-Description : SingleSingleSubDocInsert inserts a Sub-Document as per user's input [No Random data]
-
-| Name | Type | JSON Tag |
-| ---- | ---- | -------- |
-| `IdentifierToken` | `string` | `json:identifierToken`  |
-| `ClusterConfig` | `ptr` | `json:clusterConfig`  |
-| `Bucket` | `string` | `json:bucket`  |
-| `Scope` | `string` | `json:scope,omitempty`  |
-| `Collection` | `string` | `json:collection,omitempty`  |
-| `SingleSubDocOperationConfig` | `ptr` | `json:singleSubDocOperationConfig`  |
-| `InsertSpecOptions` | `ptr` | `json:insertSpecOptions`  |
-| `MutateInOptions` | `ptr` | `json:mutateInOptions`  |
-
----
-#### /single-sub-doc-read
-
- REST : POST
-
-Description : SingleSingleSubDocRead inserts a Sub-Document as per user's input [No Random data]
-
-| Name | Type | JSON Tag |
-| ---- | ---- | -------- |
-| `IdentifierToken` | `string` | `json:identifierToken`  |
-| `ClusterConfig` | `ptr` | `json:clusterConfig`  |
-| `Bucket` | `string` | `json:bucket`  |
-| `Scope` | `string` | `json:scope,omitempty`  |
-| `Collection` | `string` | `json:collection,omitempty`  |
-| `SingleSubDocOperationConfig` | `ptr` | `json:singleSubDocOperationConfig`  |
-| `LookupInOptions` | `ptr` | `json:lookupInOptions`  |
-| `GetSpecOptions` | `ptr` | `json:getSpecOptions`  |
-
----
-#### /single-sub-doc-replace
-
- REST : POST
-
-Description : SingleSingleSubDocReplace inserts a Sub-Document as per user's input [No Random data]
-
-| Name | Type | JSON Tag |
-| ---- | ---- | -------- |
-| `IdentifierToken` | `string` | `json:identifierToken`  |
-| `ClusterConfig` | `ptr` | `json:clusterConfig`  |
-| `Bucket` | `string` | `json:bucket`  |
-| `Scope` | `string` | `json:scope,omitempty`  |
-| `Collection` | `string` | `json:collection,omitempty`  |
-| `SingleSubDocOperationConfig` | `ptr` | `json:singleSubDocOperationConfig`  |
-| `ReplaceSpecOptions` | `ptr` | `json:replaceSpecOptions`  |
-| `MutateInOptions` | `ptr` | `json:mutateInOptions`  |
-
----
-#### /single-sub-doc-upsert
-
- REST : POST
-
-Description : SingleSingleSubDocUpsert inserts a Sub-Document as per user's input [No Random data]
-
-| Name | Type | JSON Tag |
-| ---- | ---- | -------- |
-| `IdentifierToken` | `string` | `json:identifierToken`  |
-| `ClusterConfig` | `ptr` | `json:clusterConfig`  |
-| `Bucket` | `string` | `json:bucket`  |
-| `Scope` | `string` | `json:scope,omitempty`  |
-| `Collection` | `string` | `json:collection,omitempty`  |
-| `SingleSubDocOperationConfig` | `ptr` | `json:singleSubDocOperationConfig`  |
-| `InsertSpecOptions` | `ptr` | `json:insertSpecOptions`  |
-| `MutateInOptions` | `ptr` | `json:mutateInOptions`  |
-
----
-#### /single-touch
-
- REST : POST
-
-Description : Single touch task specifies a new expiry time for a document in Couchbase.
-
-
-| Name | Type | JSON Tag |
-| ---- | ---- | -------- |
-| `IdentifierToken` | `string` | `json:identifierToken`  |
-| `ClusterConfig` | `ptr` | `json:clusterConfig`  |
-| `Bucket` | `string` | `json:bucket`  |
-| `Scope` | `string` | `json:scope,omitempty`  |
-| `Collection` | `string` | `json:collection,omitempty`  |
-| `InsertOptions` | `ptr` | `json:insertOptions,omitempty`  |
-| `SingleOperationConfig` | `ptr` | `json:singleOperationConfig`  |
-
----
-#### /single-upsert
-
- REST : POST
-
-Description : Single insert task updates key value in Couchbase.
-
-
-| Name | Type | JSON Tag |
-| ---- | ---- | -------- |
-| `IdentifierToken` | `string` | `json:identifierToken`  |
-| `ClusterConfig` | `ptr` | `json:clusterConfig`  |
-| `Bucket` | `string` | `json:bucket`  |
-| `Scope` | `string` | `json:scope,omitempty`  |
-| `Collection` | `string` | `json:collection,omitempty`  |
-| `InsertOptions` | `ptr` | `json:insertOptions,omitempty`  |
-| `SingleOperationConfig` | `ptr` | `json:singleOperationConfig`  |
-
----
-#### /sub-doc-bulk-delete
-
- REST : POST
-
-Description :  SubDocDelete deletes sub-documents in bulk
-
-| Name | Type | JSON Tag |
-| ---- | ---- | -------- |
-| `IdentifierToken` | `string` | `json:identifierToken`  |
-| `ClusterConfig` | `ptr` | `json:clusterConfig`  |
-| `Bucket` | `string` | `json:bucket`  |
-| `Scope` | `string` | `json:scope,omitempty`  |
-| `Collection` | `string` | `json:collection,omitempty`  |
-| `OperationConfig` | `ptr` | `json:operationConfig`  |
-| `RemoveSpecOptions` | `ptr` | `json:removeSpecOptions`  |
-| `MutateInOptions` | `ptr` | `json:mutateInOptions`  |
-
----
-#### /sub-doc-bulk-insert
-
- REST : POST
-
-Description :  SubDocInsert inserts a Sub-Document
-
-| Name | Type | JSON Tag |
-| ---- | ---- | -------- |
-| `IdentifierToken` | `string` | `json:identifierToken`  |
-| `ClusterConfig` | `ptr` | `json:clusterConfig`  |
-| `Bucket` | `string` | `json:bucket`  |
-| `Scope` | `string` | `json:scope,omitempty`  |
-| `Collection` | `string` | `json:collection,omitempty`  |
 | `OperationConfig` | `ptr` | `json:operationConfig`  |
-| `InsertSpecOptions` | `ptr` | `json:insertSpecOptions`  |
-| `MutateInOptions` | `ptr` | `json:mutateInOptions`  |
-
----
-#### /sub-doc-bulk-read
-
- REST : POST
-
-Description :  SubDocRead reads sub-document in bulk
-
-| Name | Type | JSON Tag |
-| ---- | ---- | -------- |
-| `IdentifierToken` | `string` | `json:identifierToken`  |
-| `ClusterConfig` | `ptr` | `json:clusterConfig`  |
-| `Bucket` | `string` | `json:bucket`  |
-| `Scope` | `string` | `json:scope,omitempty`  |
-| `Collection` | `string` | `json:collection,omitempty`  |
-| `OperationConfig` | `ptr` | `json:operationConfig`  |
-| `GetSpecOptions` | `ptr` | `json:getSpecOptions`  |
-| `LookupInOptions` | `ptr` | `json:lookupInOptions`  |
-
----
-#### /sub-doc-bulk-replace
-
- REST : POST
-
-Description :  SubDocReplace upserts a Sub-Document
-
-| Name | Type | JSON Tag |
-| ---- | ---- | -------- |
-| `IdentifierToken` | `string` | `json:identifierToken`  |
-| `ClusterConfig` | `ptr` | `json:clusterConfig`  |
-| `Bucket` | `string` | `json:bucket`  |
-| `Scope` | `string` | `json:scope,omitempty`  |
-| `Collection` | `string` | `json:collection,omitempty`  |
-| `OperationConfig` | `ptr` | `json:operationConfig`  |
-| `ReplaceSpecOptions` | `ptr` | `json:replaceSpecOptions`  |
-| `MutateInOptions` | `ptr` | `json:mutateInOptions`  |
+| `DBType` | `string` | `json:dbType`  |
+| `ConnStr` | `string` | `json:connectionString`  |
+| `Username` | `string` | `json:username`  |
+| `Password` | `string` | `json:password`  |
+| `Extra` | `struct` | `json:extra`  |
 
 ---
-#### /sub-doc-bulk-upsert
+#### /warmup-bucket
 
  REST : POST
 
-Description :  SubDocUpsert upserts a Sub-Document
+Description : This API aids in warming up a Couchbase bucket or establishing connections to KV services.
 
 | Name | Type | JSON Tag |
 | ---- | ---- | -------- |
 | `IdentifierToken` | `string` | `json:identifierToken`  |
-| `ClusterConfig` | `ptr` | `json:clusterConfig`  |
-| `Bucket` | `string` | `json:bucket`  |
-| `Scope` | `string` | `json:scope,omitempty`  |
-| `Collection` | `string` | `json:collection,omitempty`  |
-| `OperationConfig` | `ptr` | `json:operationConfig`  |
-| `InsertSpecOptions` | `ptr` | `json:insertSpecOptions`  |
-| `MutateInOptions` | `ptr` | `json:mutateInOptions`  |
+| `DBType` | `string` | `json:dbType`  |
+| `ConnStr` | `string` | `json:connectionString`  |
+| `Username` | `string` | `json:username`  |
+| `Password` | `string` | `json:password`  |
+| `Extra` | `struct` | `json:extra`  |
 
 ---
-#### /validate
-
- REST : POST
-
-Description : Validates every document in the cluster's bucket
+**Description of Extra Parameters**.
 
 | Name | Type | JSON Tag |
 | ---- | ---- | -------- |
-| `IdentifierToken` | `string` | `json:identifierToken`  |
-| `ClusterConfig` | `ptr` | `json:clusterConfig`  |
-| `Bucket` | `string` | `json:bucket`  |
+| `CompressionDisabled` | `bool` | `json:compressionDisabled,omitempty`  |
+| `CompressionMinSize` | `uint32` | `json:compressionMinSize,omitempty`  |
+| `CompressionMinRatio` | `float64` | `json:compressionMinRatio,omitempty`  |
+| `ConnectionTimeout` | `int` | `json:connectionTimeout,omitempty`  |
+| `KVTimeout` | `int` | `json:KVTimeout,omitempty`  |
+| `KVDurableTimeout` | `int` | `json:KVDurableTimeout,omitempty`  |
+| `Bucket` | `string` | `json:bucket,omitempty`  |
 | `Scope` | `string` | `json:scope,omitempty`  |
 | `Collection` | `string` | `json:collection,omitempty`  |
+| `Expiry` | `int` | `json:expiry,omitempty`  |
+| `PersistTo` | `uint` | `json:persistTo,omitempty`  |
+| `ReplicateTo` | `uint` | `json:replicateTo,omitempty`  |
+| `Durability` | `string` | `json:durability,omitempty`  |
+| `OperationTimeout` | `int` | `json:operationTimeout,omitempty`  |
+| `Cas` | `uint64` | `json:cas,omitempty`  |
+| `IsXattr` | `bool` | `json:isXattr,omitempty`  |
+| `StoreSemantic` | `int` | `json:storeSemantic,omitempty`  |
+| `PreserveExpiry` | `bool` | `json:preserveExpiry,omitempty`  |
+| `CreatePath` | `bool` | `json:createPath,omitempty`  |
 
 ---
-#### /warmup-bucket
-
- REST : POST
-
-Description : This API aids in warming up a Couchbase bucket or establishing connections to KV services.
-
-| Name | Type | JSON Tag |
-| ---- | ---- | -------- |
-| `IdentifierToken` | `string` | `json:identifierToken`  |
-| `ClusterConfig` | `ptr` | `json:clusterConfig`  |
-| `Bucket` | `string` | `json:bucket`  |
-
----
-**Description of JSON tags used in routes**.
-
  * [bulkError](#bulkerror)
- * [clusterConfig](#clusterconfig)
- * [compressionConfig](#compressionconfig)
  * [exceptions](#exceptions)
- * [getSpecOptions](#getspecoptions)
- * [insertOptions](#insertoptions)
- * [insertSpecOptions](#insertspecoptions)
- * [lookupInOptions](#lookupinoptions)
- * [mutateInOptions](#mutateinoptions)
  * [operationConfig](#operationconfig)
- * [queryOperationConfig](#queryoperationconfig)
- * [removeOptions](#removeoptions)
- * [removeSpecOptions](#removespecoptions)
- * [replaceOption](#replaceoption)
- * [replaceSpecOptions](#replacespecoptions)
  * [retriedError](#retriederror)
  * [sdkTimings](#sdktimings)
- * [singleOperationConfig](#singleoperationconfig)
  * [singleResult](#singleresult)
- * [singleSubDocOperationConfig](#singlesubdocoperationconfig)
- * [timeoutsConfig](#timeoutsconfig)
- * [touchOptions](#touchoptions)
 
 ---
 #### bulkError
@@ -529,22 +90,6 @@ Description : This API aids in warming up a Couchbase bucket or establishing con
 | `Status` | `bool` | `json:status`  |
 | `Cas` | `uint64` | `json:cas`  |
 | `ErrorString` | `string` | `json:errorString`  |
-#### clusterConfig
-
-| Name | Type | JSON Tag |
-| ---- | ---- | -------- |
-| `Username` | `string` | `json:username`  |
-| `Password` | `string` | `json:password`  |
-| `ConnectionString` | `string` | `json:connectionString`  |
-| `CompressionConfig` | `struct` | `json:compressionConfig,omitempty`  |
-| `TimeoutsConfig` | `struct` | `json:timeoutsConfig,omitempty`  |
-#### compressionConfig
-
-| Name | Type | JSON Tag |
-| ---- | ---- | -------- |
-| `Disabled` | `bool` | `json:disabled,omitempty`  |
-| `MinSize` | `uint32` | `json:minSize,omitempty`  |
-| `MinRatio` | `float64` | `json:minRatio,omitempty`  |
 #### exceptions
 
 | Name | Type | JSON Tag |
@@ -552,96 +97,18 @@ Description : This API aids in warming up a Couchbase bucket or establishing con
 | `IgnoreExceptions` | `slice` | `json:ignoreExceptions,omitempty`  |
 | `RetryExceptions` | `slice` | `json:retryExceptions,omitempty`  |
 | `RetryAttempts` | `int` | `json:retryAttempts,omitempty`  |
-#### getSpecOptions
-
-| Name | Type | JSON Tag |
-| ---- | ---- | -------- |
-| `IsXattr` | `bool` | `json:isXattr,omitempty`  |
-#### insertOptions
-
-| Name | Type | JSON Tag |
-| ---- | ---- | -------- |
-| `Expiry` | `int64` | `json:expiry,omitempty`  |
-| `PersistTo` | `uint` | `json:persistTo,omitempty`  |
-| `ReplicateTo` | `uint` | `json:replicateTo,omitempty`  |
-| `Durability` | `string` | `json:durability,omitempty`  |
-| `Timeout` | `int` | `json:timeout,omitempty`  |
-#### insertSpecOptions
-
-| Name | Type | JSON Tag |
-| ---- | ---- | -------- |
-| `CreatePath` | `bool` | `json:createPath,omitempty`  |
-| `IsXattr` | `bool` | `json:isXattr,omitempty`  |
-#### lookupInOptions
-
-| Name | Type | JSON Tag |
-| ---- | ---- | -------- |
-| `Timeout` | `int` | `json:timeout,omitempty`  |
-#### mutateInOptions
-
-| Name | Type | JSON Tag |
-| ---- | ---- | -------- |
-| `Expiry` | `int` | `json:expiry,omitempty`  |
-| `Cas` | `uint64` | `json:cas,omitempty`  |
-| `PersistTo` | `uint` | `json:persistTo,omitempty`  |
-| `ReplicateTo` | `uint` | `json:replicateTo,omitempty`  |
-| `Durability` | `string` | `json:durability,omitempty`  |
-| `StoreSemantic` | `int` | `json:storeSemantic,omitempty`  |
-| `Timeout` | `int` | `json:timeout,omitempty`  |
-| `PreserveExpiry` | `bool` | `json:preserveExpiry,omitempty`  |
 #### operationConfig
 
 | Name | Type | JSON Tag |
 | ---- | ---- | -------- |
-| `Count` | `int64` | `json:count,omitempty`  |
 | `DocSize` | `int` | `json:docSize`  |
 | `DocType` | `string` | `json:docType,omitempty`  |
 | `KeySize` | `int` | `json:keySize,omitempty`  |
-| `KeyPrefix` | `string` | `json:keyPrefix`  |
-| `KeySuffix` | `string` | `json:keySuffix`  |
-| `ReadYourOwnWrite` | `bool` | `json:readYourOwnWrite,omitempty`  |
 | `TemplateName` | `string` | `json:template`  |
 | `Start` | `int64` | `json:start`  |
 | `End` | `int64` | `json:end`  |
 | `FieldsToChange` | `slice` | `json:fieldsToChange`  |
 | `Exceptions` | `struct` | `json:exceptions,omitempty`  |
-#### queryOperationConfig
-
-| Name | Type | JSON Tag |
-| ---- | ---- | -------- |
-| `Template` | `string` | `json:template,omitempty`  |
-| `Duration` | `int` | `json:duration,omitempty`  |
-| `BuildIndex` | `bool` | `json:buildIndex`  |
-| `BuildIndexViaSDK` | `bool` | `json:buildIndexViaSDK`  |
-#### removeOptions
-
-| Name | Type | JSON Tag |
-| ---- | ---- | -------- |
-| `Cas` | `uint64` | `json:cas,omitempty`  |
-| `PersistTo` | `uint` | `json:persistTo,omitempty`  |
-| `ReplicateTo` | `uint` | `json:replicateTo,omitempty`  |
-| `Durability` | `string` | `json:durability,omitempty`  |
-| `Timeout` | `int` | `json:timeout,omitempty`  |
-#### removeSpecOptions
-
-| Name | Type | JSON Tag |
-| ---- | ---- | -------- |
-| `IsXattr` | `bool` | `json:isXattr,omitempty`  |
-#### replaceOption
-
-| Name | Type | JSON Tag |
-| ---- | ---- | -------- |
-| `Expiry` | `int64` | `json:expiry,omitempty`  |
-| `Cas` | `uint64` | `json:cas,omitempty`  |
-| `PersistTo` | `uint` | `json:persistTo,omitempty`  |
-| `ReplicateTo` | `uint` | `json:replicateTo,omitempty`  |
-| `Durability` | `string` | `json:durability,omitempty`  |
-| `Timeout` | `int` | `json:timeout,omitempty`  |
-#### replaceSpecOptions
-
-| Name | Type | JSON Tag |
-| ---- | ---- | -------- |
-| `IsXattr` | `bool` | `json:isXattr,omitempty`  |
 #### retriedError
 
 | Name | Type | JSON Tag |
@@ -657,13 +124,6 @@ Description : This API aids in warming up a Couchbase bucket or establishing con
 | ---- | ---- | -------- |
 | `SendTime` | `string` | `json:sendTime`  |
 | `AckTime` | `string` | `json:ackTime`  |
-#### singleOperationConfig
-
-| Name | Type | JSON Tag |
-| ---- | ---- | -------- |
-| `Keys` | `slice` | `json:keys`  |
-| `Template` | `string` | `json:template`  |
-| `DocSize` | `int` | `json:docSize`  |
 #### singleResult
 
 | Name | Type | JSON Tag |
@@ -672,25 +132,6 @@ Description : This API aids in warming up a Couchbase bucket or establishing con
 | `ErrorString` | `string` | `json:errorString`  |
 | `Status` | `bool` | `json:status`  |
 | `Cas` | `uint64` | `json:cas`  |
-#### singleSubDocOperationConfig
-
-| Name | Type | JSON Tag |
-| ---- | ---- | -------- |
-| `Key` | `string` | `json:key`  |
-| `Paths` | `slice` | `json:paths`  |
-| `DocSize` | `int` | `json:docSize`  |
-#### timeoutsConfig
-
-| Name | Type | JSON Tag |
-| ---- | ---- | -------- |
-| `ConnectTimeout` | `int` | `json:connectTimeout,omitempty`  |
-| `KVTimeout` | `int` | `json:KVTimeout,omitempty`  |
-| `KVDurableTimeout` | `int` | `json:KVDurableTimeout,omitempty`  |
-#### touchOptions
-
-| Name | Type | JSON Tag |
-| ---- | ---- | -------- |
-| `Timeout` | `int` | `json:timeout,omitempty`  |
 
 ---
 **APIs Response Description**.
